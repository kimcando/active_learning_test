

Fri Jan 15 16:37:47 2021
imbal_test_imbal12_rand_cifar10_vgg_Adam's set level: 10
imbal_test_imbal12_rand_cifar10_vgg_Adam logger is ready
imbal_test with imbalance_ratio: [0.5 0.5 1.  1.  1.  1.  1.  1.  1.  1. ]
parallel mode is False
initial count: cifar10: [0 1 2 3 4 5 6 7 8 9] class, each has [5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]
cifar10: [0 1 2 3 4 5 6 7 8 9] class, each has [2500 2500 5000 5000 5000 5000 5000 5000 5000 5000]
Epoch@@0
Files already downloaded and verified
TRAINING class wise acc@0 :: {0: [0.0004], 1: [0.0028], 2: [0.047], 3: [0.059], 4: [0.1106], 5: [0.087], 6: [0.1642], 7: [0.1344], 8: [0.265], 9: [0.3558]}
TRAIINING epoch@0 ::  acc@0.136  loss@2.431
[[   1    1  104  120   63  197   82  303  698  931]
 [   1    7   80  138   45  226   80  327  684  912]
 [   6   13  235  271  404  422  530  619 1112 1388]
 [   2    8  156  295  191  423  296  664 1296 1669]
 [   4    3  294  265  553  378  753  566 1024 1160]
 [   0    5  176  291  181  435  285  611 1336 1680]
 [   0    6  282  300  569  320  821  603  987 1112]
 [   0    6  209  284  234  428  340  672 1215 1612]
 [   2    8  143  277   80  439  165  696 1325 1865]
 [   0    6  162  277   90  450  180  666 1390 1779]]
TESTING class wise acc@0 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.082], 6: [0.708], 7: [0.0], 8: [0.0], 9: [0.894]}
TESTING epoch@0 ::  acc@0.168  loss@2.208
Epoch@@1
[[  0   0   0   0   0  21  53   0   0 926]
 [  0   0   0   0   0  36  77   0   0 887]
 [  0   0   0   0   0  74 443   0   0 483]
 [  0   0   0   0   0 104 272   0   0 624]
 [  0   0   0   0   0  73 624   0   0 303]
 [  0   0   0   0   0  82 254   0   0 664]
 [  0   0   0   0   0  53 708   0   0 239]
 [  0   0   0   0   0 116 257   0   0 627]
 [  0   0   0   0   0  24  25   0   0 951]
 [  0   0   0   0   0  44  62   0   0 894]]
TRAINING class wise acc@1 :: {0: [0.0], 1: [0.0], 2: [0.0496], 3: [0.1142], 4: [0.2422], 5: [0.1296], 6: [0.266], 7: [0.164], 8: [0.5972], 9: [0.3134]}
TRAIINING epoch@1 ::  acc@0.208  loss@1.998
[[   0    0   39   57   25   81   33  147 1331  787]
 [   0    0   26   47   19   39   26   93 1467  783]
 [   0    0  248  445  843  495  932  601  796  640]
 [   0    0  297  571  624  666  730  771  701  640]
 [   0    0  272  398 1211  507 1266  503  473  370]
 [   0    0  305  628  635  648  747  756  648  633]
 [   0    0  319  466 1179  535 1330  565  323  283]
 [   0    0  280  639  488  668  642  820  728  735]
 [   0    0   42   88   15   81   31  186 2986 1571]
 [   0    0   80  144   28  127   33  289 2732 1567]]
TESTING class wise acc@1 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.806], 5: [0.212], 6: [0.0], 7: [0.069], 8: [0.213], 9: [0.334]}
TESTING epoch@1 ::  acc@0.163  loss@2.162
Epoch@@2
[[  0   0   0   0  88 118   0 288  95 411]
 [  0   0   0   0  62 126   0 265 143 404]
 [  0   0   0   0 663 188   0 107   0  42]
 [  0   0   0   0 714 202   0  70   0  14]
 [  0   0   0   0 806 101   0  61   1  31]
 [  0   0   0   0 734 212   0  47   1   6]
 [  0   0   0   0 895  84   0  18   0   3]
 [  0   0   0   0 697 224   0  69   0  10]
 [  0   0   0   0  42  79   0 199 213 467]
 [  0   0   0   0  78 156   0 333  99 334]]
TRAINING class wise acc@2 :: {0: [0.0008], 1: [0.0], 2: [0.034], 3: [0.1474], 4: [0.1332], 5: [0.3038], 6: [0.4332], 7: [0.3176], 8: [0.5954], 9: [0.3714]}
TRAIINING epoch@2 ::  acc@0.260  loss@1.836
[[   2    0   29   57    8  102   23  335  923 1021]
 [   1    0   17   30    7   56    3  154 1361  871]
 [   2    0  170  562  428 1021  975 1136  158  548]
 [   1    0  184  737  369 1295  623 1330   78  383]
 [   3    0  209  562  666  938 1319  827  105  371]
 [   0    0  186  838  352 1519  499 1308   33  265]
 [   0    0  167  452  686  696 2166  604   45  184]
 [   1    0  153  744  259 1340  307 1588   98  510]
 [   6    0   32   38    8   80    9  269 2977 1581]
 [   4    0   44   91    6  152    7  437 2402 1857]]
TESTING class wise acc@2 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.121], 4: [0.103], 5: [0.0], 6: [0.584], 7: [0.686], 8: [0.923], 9: [0.042]}
TESTING epoch@2 ::  acc@0.246  loss@1.844
Epoch@@3
[[  0   0   0   4   4   0   7 136 734 115]
 [  0   0   0   2   0   0   1  36 925  36]
 [  0   0   0 114  80   0 132 487  93  94]
 [  0   0   0 121  67   0 113 559  73  67]
 [  0   0   0 146 103   0 218 402  70  61]
 [  0   0   0 127  63   0  62 649  53  46]
 [  0   0   0  93  78   0 584 196  24  25]
 [  0   0   0  63  22   0  39 686 107  83]
 [  0   0   0   1   1   0   3  32 923  40]
 [  0   0   0   4   1   0   1  50 902  42]]
TRAINING class wise acc@3 :: {0: [0.002], 1: [0.0], 2: [0.0426], 3: [0.1022], 4: [0.167], 5: [0.3728], 6: [0.6114], 7: [0.4266], 8: [0.5686], 9: [0.5532]}
TRAIINING epoch@3 ::  acc@0.316  loss@1.728
[[   5    0   18   24   25   95    9  474 1249  601]
 [   1    0    8    9    6   45    5  106  918 1402]
 [   7    0  213  426  579 1118  777 1351  337  192]
 [   4    0  195  511  554 1379  582 1429  235  111]
 [   4    0  282  458  835 1048 1064  995  203  111]
 [   3    0  201  513  435 1864  249 1551  124   60]
 [   0    0  145  254  588  420 3057  389   99   48]
 [   6    0  113  341  232 1582  117 2133  326  150]
 [   4    0   11   20   10   82   11  343 2843 1676]
 [   3    2   19   20   11   84    4  339 1752 2766]]
TESTING class wise acc@3 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.198], 5: [0.206], 6: [0.511], 7: [0.747], 8: [0.864], 9: [0.681]}
TESTING epoch@3 ::  acc@0.321  loss@1.688
Epoch@@4
[[  0   0   0   0   4  17   0 148 800  31]
 [  0   0   0   0   0   6   0  35 363 596]
 [  0   0   0   0 148 221  86 417 126   2]
 [  0   0   0   0  94 188  38 576 100   4]
 [  0   0   0   0 198 299  91 331  79   2]
 [  0   0   0   0  26 206  10 698  60   0]
 [  0   0   0   0 191 140 511 132  26   0]
 [  0   0   0   0  11 104   5 747 132   1]
 [  0   0   0   0   2   4   0  61 864  69]
 [  0   0   0   0   0   3   0  31 285 681]]
TRAINING class wise acc@4 :: {0: [0.0272], 1: [0.0028], 2: [0.104], 3: [0.1434], 4: [0.2208], 5: [0.3738], 6: [0.664], 7: [0.5456], 8: [0.798], 9: [0.7648]}
TRAIINING epoch@4 ::  acc@0.403  loss@1.553
[[  68    0  109  119   22   85   13  225 1677  182]
 [  12    7   28   29    4   13   14   39  536 1818]
 [  65    0  520  731  689  983  734  801  452   25]
 [  39    0  518  717  638 1306  587  877  267   51]
 [  21    0  419  471 1104 1060  757  921  230   17]
 [  17    0  375  527  601 1869  196 1277  117   21]
 [   5    0  333  333  503  267 3320  126   84   29]
 [  15    0  161  238  268 1205   62 2728  290   33]
 [  49    0   76   80   11   46   16  189 3990  543]
 [  17   18   42   61   14   30   17  108  869 3824]]
TESTING class wise acc@4 :: {0: [0.087], 1: [0.0], 2: [0.037], 3: [0.458], 4: [0.094], 5: [0.151], 6: [0.784], 7: [0.657], 8: [0.928], 9: [0.847]}
TESTING epoch@4 ::  acc@0.404  loss@1.540
Epoch@@5
[[ 87   0  33 114   0   2   5  25 653  81]
 [  1   0   1   9   0   0   3   3 181 802]
 [ 33   0  37 419  23  42 279  47  71  49]
 [ 28   0  36 458  14  55 178  68  57 106]
 [ 23   0  32 238  94  61 297 201  40  14]
 [ 21   0  60 459  13 151  74 149  22  51]
 [  3   0   7 138   6   9 784   2  10  41]
 [ 27   0  46 121   9  43  13 657  54  30]
 [ 21   0   6   8   0   0   1   1 928  35]
 [  2   0   2   8   0   0   1   0 140 847]]
TRAINING class wise acc@5 :: {0: [0.1372], 1: [0.0124], 2: [0.089], 3: [0.3358], 4: [0.4612], 5: [0.5194], 6: [0.7242], 7: [0.6656], 8: [0.827], 9: [0.8526]}
TRAIINING epoch@5 ::  acc@0.506  loss@1.304
[[ 343    2  134  248    8   92   17  106 1351  199]
 [  30   31   21   53    1    5   25    8  217 2109]
 [ 204    1  445 1357  670 1206  603  262  203   49]
 [ 110    1  401 1679  252 1630  598  161   79   89]
 [  69    0  211  449 2306  867  375  645   57   21]
 [  55    0  238 1097  404 2597  154  406   19   30]
 [  15    0  183  617  277  194 3621   13   20   60]
 [  64    0   88  227  371  784   26 3328   61   51]
 [ 200    4   60  119    2   39   10   43 4135  388]
 [  71   56   46  110    4   37   28   34  351 4263]]
TESTING class wise acc@5 :: {0: [0.264], 1: [0.0], 2: [0.0], 3: [0.393], 4: [0.621], 5: [0.75], 6: [0.588], 7: [0.622], 8: [0.804], 9: [0.791]}
TESTING epoch@5 ::  acc@0.483  loss@1.351
Epoch@@6
[[264   0   1 256   6  88   1  22 327  35]
 [ 21   0   0  42   1  13   8   3 123 789]
 [ 48   0   0 374 172 341  41  10  11   3]
 [ 15   0   0 393  65 453  52  10   6   6]
 [ 11   0   0  66 621 216  19  59   8   0]
 [  2   0   0 152  70 750   6  17   0   3]
 [  3   0   0 223 116  66 588   0   1   3]
 [ 12   0   0  24  87 252   1 622   2   0]
 [109   0   0  49   2  19   0   6 804  11]
 [ 28   0   0  59   1  13   4   4 100 791]]
TRAINING class wise acc@6 :: {0: [0.32], 1: [0.1964], 2: [0.2518], 3: [0.3228], 4: [0.5608], 5: [0.6058], 6: [0.7424], 7: [0.7054], 8: [0.8422], 9: [0.8358]}
TRAIINING epoch@6 ::  acc@0.569  loss@1.157
[[ 800   27  283  146   11   61   13   60  938  161]
 [  46  491   27   27    1    7   19    4  200 1678]
 [ 361    1 1259 1153  673  745  534  143   93   38]
 [ 135    3  602 1614  228 1736  451  114   42   75]
 [  65    0  423  355 2804  581  308  417   27   20]
 [  53    1  286  906  282 3029  101  314   13   15]
 [  39    1  352  488  211  114 3712    6   15   62]
 [ 107    1  118  184  287  706   16 3527   32   22]
 [ 333   88  107   49    3   16    8   22 4211  163]
 [ 121  303   73   73    2   32   21   13  183 4179]]
TESTING class wise acc@6 :: {0: [0.089], 1: [0.531], 2: [0.514], 3: [0.343], 4: [0.307], 5: [0.454], 6: [0.836], 7: [0.507], 8: [0.902], 9: [0.73]}
TESTING epoch@6 ::  acc@0.521  loss@1.287
Epoch@@7
[[ 89  19 101   6   2   8   6   5 716  48]
 [  7 531   2   4   0   1  11   0 163 281]
 [ 33   0 514  98  14  51 141  12 131   6]
 [ 50   2 169 343   7 176 174  17  50  12]
 [  5   1 248  85 307  35 266  26  25   2]
 [ 29   1  99 264  22 454  40  62  24   5]
 [  3   1  95  46   1   2 836   1   8   7]
 [ 37   0 127  56  87 129  22 507  31   4]
 [ 25  22  24   5   0   2   4   2 902  14]
 [ 21 112   8  29   1   1  32   0  66 730]]
TRAINING class wise acc@7 :: {0: [0.3872], 1: [0.7164], 2: [0.5428], 3: [0.375], 4: [0.6548], 5: [0.6144], 6: [0.7572], 7: [0.7508], 8: [0.8436], 9: [0.8384]}
TRAIINING epoch@7 ::  acc@0.659  loss@0.955
[[ 968   48  497   80   11   21    7   70  666  132]
 [  37 1791    8   28    1    3   10    4  149  469]
 [ 276    1 2714  556  581  263  363  141   66   39]
 [ 117    5  443 1875  242 1689  393  126   27   83]
 [  42    0  632  216 3274  278  194  338   12   14]
 [  43    1  205 1017  256 3072   81  289    5   31]
 [  24    5  414  440  160   82 3786    8   12   69]
 [ 105    2  186  175  247  486    6 3754   13   26]
 [ 327  139  118   35    2    9    3   18 4218  131]
 [ 101  385   31   95    0   25   18    9  144 4192]]
TESTING class wise acc@7 :: {0: [0.525], 1: [0.719], 2: [0.442], 3: [0.617], 4: [0.255], 5: [0.686], 6: [0.574], 7: [0.661], 8: [0.861], 9: [0.688]}
TESTING epoch@7 ::  acc@0.603  loss@1.158
Epoch@@8
[[525   5  66 130   1   5   0  14 245   9]
 [ 36 719   0  36   0   7   1   4 112  85]
 [128   0 442 328   7  50  19  13  10   3]
 [ 20   0  36 617   2 301   7   6   4   7]
 [ 19   0 185 334 255 131  22  52   2   0]
 [ 10   0  12 275   4 686   0  12   0   1]
 [  7   0  55 347   0   9 574   0   3   5]
 [ 17   0  13  85   6 218   0 661   0   0]
 [ 72   6   4  41   0   2   0   0 861  14]
 [ 40 124   6  68   0   5   4   6  59 688]]
TRAINING class wise acc@8 :: {0: [0.4996], 1: [0.7956], 2: [0.627], 3: [0.4676], 4: [0.7052], 5: [0.6262], 6: [0.7824], 7: [0.7822], 8: [0.8514], 9: [0.8674]}
TRAIINING epoch@8 ::  acc@0.706  loss@0.839
[[1249   40  452   88   17   15    7   67  447  118]
 [  50 1989    7   28    0    1    5    2  102  316]
 [ 295    4 3135  458  484  149  296  121   32   26]
 [ 105    6  380 2338  230 1404  335  102   21   79]
 [  62    0  558  220 3526  187  153  282    6    6]
 [  36    3  159 1045  228 3131   73  293    6   26]
 [  22    2  317  480  143   35 3912    9   11   69]
 [ 116    2  130  193  230  380    8 3911    7   23]
 [ 386   94   76   34    0    1    6   17 4257  129]
 [  89  264   38  110    2    7   16   11  126 4337]]
TESTING class wise acc@8 :: {0: [0.454], 1: [0.853], 2: [0.686], 3: [0.4], 4: [0.772], 5: [0.306], 6: [0.892], 7: [0.585], 8: [0.728], 9: [0.784]}
TESTING epoch@8 ::  acc@0.646  loss@1.268
Epoch@@9
[[454  17 293  13  30   1  23  37 106  26]
 [  6 853  16   9   1   0  32   0  17  66]
 [ 22   1 686  36  91   3 154   1   2   4]
 [  9   3 186 400  81  49 245  12   4  11]
 [  8   1  85  18 772   1 104  11   0   0]
 [  4   0 123 380  91 306  77  16   0   3]
 [  0   0  61  23  22   0 892   0   1   1]
 [ 12   0 107  57 190  27  16 585   2   4]
 [ 63  26 109   6   6   0  36   2 728  24]
 [  9 115  19  11   1   0  33   1  27 784]]
TRAINING class wise acc@9 :: {0: [0.6224], 1: [0.8368], 2: [0.6792], 3: [0.5572], 4: [0.7584], 5: [0.6286], 6: [0.8068], 7: [0.8166], 8: [0.8688], 9: [0.8908]}
TRAIINING epoch@9 ::  acc@0.748  loss@0.731
[[1556   30  359   92   13   14    7   64  288   77]
 [  29 2092    6   22    0    1    2    1   84  263]
 [ 257    1 3396  441  422  109  230   91   35   18]
 [  79    7  307 2786  201 1143  293  106   17   61]
 [  46    1  439  202 3792  162  114  238    4    2]
 [  27    4  117 1167  208 3143   62  256    5   11]
 [   8    3  296  440  111   40 4034    3   15   50]
 [  96    3  117  202  217  270    1 4083    4    7]
 [ 316   93   68   39    6    1    6    8 4344  119]
 [  60  215   26  105    0    4   18    7  111 4454]]
TESTING class wise acc@9 :: {0: [0.632], 1: [0.873], 2: [0.791], 3: [0.621], 4: [0.66], 5: [0.363], 6: [0.681], 7: [0.614], 8: [0.899], 9: [0.908]}
TESTING epoch@9 ::  acc@0.704  loss@0.957
Epoch@@10
[[632  20 105  17   4   0   0   2 179  41]
 [  6 873   1   4   0   0   0   0  28  88]
 [ 59   1 791  51  38   7  12   1  16  24]
 [ 27   9 174 621  20  57  21  10  13  48]
 [ 17   1 198  72 660  12   7  19   6   8]
 [ 17   1 143 397  37 363   3  11   7  21]
 [  4   1 138 119  16   4 681   1   6  30]
 [ 68   3 110 109  52  17   0 614   2  25]
 [ 40  16   2   4   0   0   2   0 899  37]
 [  3  52   3   4   0   0   0   0  30 908]]
TRAINING class wise acc@10 :: {0: [0.6884], 1: [0.86], 2: [0.711], 3: [0.6562], 4: [0.7818], 5: [0.6314], 6: [0.8208], 7: [0.8488], 8: [0.8852], 9: [0.8974]}
TRAIINING epoch@10 ::  acc@0.779  loss@0.650
[[1721   21  301  105   15    1    4   44  235   53]
 [  35 2150    6   21    0    1    2    4   64  217]
 [ 245    0 3555  451  350   70  218   68   25   18]
 [  65    6  285 3281  181  764  232  119   13   54]
 [  39    0  400  225 3909  132   94  194    5    2]
 [  17    3  107 1228  198 3157   34  244    3    9]
 [  12    1  280  419  111   22 4104    4    9   38]
 [  77    2   85  215  170  184    3 4244    7   13]
 [ 267   75   61   52    2    0    3    6 4426  108]
 [  52  196   16  114    1    3    9    6  116 4487]]
TESTING class wise acc@10 :: {0: [0.57], 1: [0.817], 2: [0.853], 3: [0.479], 4: [0.722], 5: [0.437], 6: [0.824], 7: [0.755], 8: [0.884], 9: [0.765]}
TESTING epoch@10 ::  acc@0.711  loss@0.901
Epoch@@11
[[570   3 234   8  12   0   3  10 156   4]
 [ 44 817   5  10   0   0   2   3  75  44]
 [ 32   0 853  27  40   5  29   9   5   0]
 [ 19   2 262 479  65  43 103  19   4   4]
 [  4   0 241   5 722   2  18   6   2   0]
 [  6   0 123 268 101 437  26  37   2   0]
 [  1   0 147  16   9   1 824   0   2   0]
 [ 22   0  49  22 130  19   1 755   1   1]
 [ 46   6  46   4   1   0   3   2 884   8]
 [ 51  62  15  13   1   0   4   7  82 765]]
TRAINING class wise acc@11 :: {0: [0.7324], 1: [0.8832], 2: [0.7462], 3: [0.7024], 4: [0.8152], 5: [0.6636], 6: [0.8488], 7: [0.869], 8: [0.908], 9: [0.9124]}
TRAIINING epoch@11 ::  acc@0.808  loss@0.577
[[1831   16  237   85   20    2    1   49  210   49]
 [  28 2208    4   25    1    0    1    0   48  185]
 [ 222    2 3731  391  312   50  193   66   23   10]
 [  60    6  238 3512  171  645  204  104   13   47]
 [  32    0  326  179 4076  113   84  186    2    2]
 [   9    0  100 1113  192 3318   50  206    2   10]
 [   5    0  219  363   90   30 4244    7    8   34]
 [  63    3   68  203  147  156    1 4345    6    8]
 [ 202   61   64   35    0    2    6    5 4540   85]
 [  53  158   14  100    1    0   11    7   94 4562]]
TESTING class wise acc@11 :: {0: [0.69], 1: [0.86], 2: [0.718], 3: [0.519], 4: [0.636], 5: [0.598], 6: [0.772], 7: [0.908], 8: [0.893], 9: [0.918]}
TESTING epoch@11 ::  acc@0.751  loss@0.815
Epoch@@12
[[690   9  86  17   3   0   3  29 124  39]
 [  6 860   4   5   0   0   0   0  27  98]
 [ 61   0 718  32  60  13  43  46  20   7]
 [ 29   5 132 519  28 113  60  69  13  32]
 [ 16   0 126  26 636  11   6 171   2   6]
 [ 10   0  57 179  26 598  18  97   3  12]
 [  1   0  94  58  33  18 772  12   6   6]
 [ 11   0  21  33   4  13   0 908   3   7]
 [ 30  14   7   6   0   0   1   5 893  44]
 [  6  33   4   7   0   2   3   4  23 918]]
TRAINING class wise acc@12 :: {0: [0.7652], 1: [0.8924], 2: [0.7746], 3: [0.7362], 4: [0.8234], 5: [0.6996], 6: [0.8706], 7: [0.8878], 8: [0.9222], 9: [0.916]}
TRAIINING epoch@12 ::  acc@0.829  loss@0.512
[[1913   19  240   69    8    3    2   56  150   40]
 [  26 2231    0   26    0    0    0    0   45  172]
 [ 218    1 3873  331  272   29  175   56   29   16]
 [  62    3  206 3681  132  584  172   97   18   45]
 [  33    0  296  173 4117  109   74  189    5    4]
 [  11    2   67 1007  185 3498   37  186    1    6]
 [   6    2  225  296   71   16 4353    3    2   26]
 [  59    1   54  157  152  126    1 4439    5    6]
 [ 173   46   51   26    2    2    6    7 4611   76]
 [  44  155   18   91    2    1   15    6   88 4580]]
TESTING class wise acc@12 :: {0: [0.662], 1: [0.931], 2: [0.726], 3: [0.521], 4: [0.878], 5: [0.525], 6: [0.725], 7: [0.903], 8: [0.929], 9: [0.78]}
TESTING epoch@12 ::  acc@0.758  loss@0.794
Epoch@@13
[[662  18 138  36  17   0   2  23  96   8]
 [ 16 931   1   7   0   0   0   2  15  28]
 [ 29   0 726  45 139   8  19  24   8   2]
 [  9   2  72 521 157 111  37  69  13   9]
 [  2   2  45  14 878   7   3  47   1   1]
 [  3   1  45 160 136 525   8 119   3   0]
 [  0   1  92  38 114   2 725   6  13   9]
 [  3   0  18  22  40   9   1 903   3   1]
 [ 20  11  11   5   0   1   0   6 929  17]
 [ 11 121   7  33   0   2   1  15  30 780]]
TRAINING class wise acc@13 :: {0: [0.8012], 1: [0.9092], 2: [0.7986], 3: [0.7626], 4: [0.8576], 5: [0.725], 6: [0.8736], 7: [0.8986], 8: [0.9216], 9: [0.9274]}
TRAIINING epoch@13 ::  acc@0.847  loss@0.465
[[2003   27  153   75   18    3    1   41  155   24]
 [  27 2273    0   20    0    0    0    1   34  145]
 [ 180    2 3993  269  258   28  166   58   35   11]
 [  33    5  190 3813  119  521  161   98   16   44]
 [  29    0  236  138 4288  104   57  141    5    2]
 [   8    0   62  900  162 3625   42  191    4    6]
 [   3    4  218  258   77   26 4368    3   12   31]
 [  47    2   37  145  123  132    3 4493   12    6]
 [ 151   43   68   31    1    0    2   16 4608   80]
 [  33  134   19   78    0    1   10    3   85 4637]]
TESTING class wise acc@13 :: {0: [0.616], 1: [0.806], 2: [0.545], 3: [0.745], 4: [0.798], 5: [0.72], 6: [0.681], 7: [0.867], 8: [0.917], 9: [0.951]}
TESTING epoch@13 ::  acc@0.765  loss@0.819
Epoch@@14
[[616  10  19  55  11   1   1  18 190  79]
 [  7 806   0  20   0   0   2   2  21 142]
 [ 92   0 545 173  73  23  37  28  15  14]
 [ 10   2  21 745  38 112  14  30   6  22]
 [  7   1  37  82 798  27  10  35   2   1]
 [  1   0   5 208  32 720   0  29   1   4]
 [  3   0  40 207  32  18 681   6   3  10]
 [  3   0   2  43  30  39   0 867   7   9]
 [ 10   5   1  11   1   2   1   4 917  48]
 [  0  10   2  12   2   0   2   0  21 951]]
TRAINING class wise acc@14 :: {0: [0.83], 1: [0.9228], 2: [0.829], 3: [0.784], 4: [0.8766], 5: [0.7576], 6: [0.896], 7: [0.9116], 8: [0.9372], 9: [0.9398]}
TRAIINING epoch@14 ::  acc@0.868  loss@0.407
[[2075   23  159   53   12    0    1   45  110   22]
 [  20 2307    2   14    0    0    1    2   43  111]
 [ 176    0 4145  209  197   34  147   55   21   16]
 [  39    7  148 3920  118  487  162   73    9   37]
 [  21    0  202  122 4383   87   66  118    1    0]
 [   9    0   38  816  145 3788   43  157    2    2]
 [   5    2  173  224   54   30 4480    7    3   22]
 [  30    0   42  135  119  105    0 4558    9    2]
 [ 117   44   34   23    1    2    7    9 4686   77]
 [  19  110   15   55    0    3   10    4   85 4699]]
TESTING class wise acc@14 :: {0: [0.825], 1: [0.963], 2: [0.732], 3: [0.719], 4: [0.763], 5: [0.55], 6: [0.718], 7: [0.878], 8: [0.867], 9: [0.848]}
TESTING epoch@14 ::  acc@0.786  loss@0.715
Epoch@@15
[[825  51  35  12   6   0   0   3  54  14]
 [  7 963   0   4   0   0   0   0   3  23]
 [ 86   6 732  78  38   7  14  19  11   9]
 [ 49  28  64 719  33  35  22  26   6  18]
 [ 18   7  71  86 763   5   8  40   2   0]
 [ 20   7  31 312  29 550   7  36   2   6]
 [  9  14  63 132  19   1 718   2  10  32]
 [ 24   4   8  46  17  16   0 878   3   4]
 [ 59  42   8   9   0   0   0   2 867  13]
 [  5 120   2   7   0   0   0   1  17 848]]
TRAINING class wise acc@15 :: {0: [0.85], 1: [0.9384], 2: [0.8476], 3: [0.7926], 4: [0.8958], 5: [0.7866], 6: [0.9086], 7: [0.9314], 8: [0.9474], 9: [0.9482]}
TRAIINING epoch@15 ::  acc@0.884  loss@0.349
[[2125   14  125   48   17    0    0   42  110   19]
 [  16 2346    1   12    0    0    0    0   28   97]
 [ 143    1 4238  195  182   21  140   40   28   12]
 [  43    4  154 3963   96  498  139   70   10   23]
 [  21    0  188  108 4479   82   46   75    1    0]
 [   5    1   37  720  114 3933   33  150    1    6]
 [   5    0  161  187   52   30 4543    2    7   13]
 [  27    1   32   83   80  107    1 4657    8    4]
 [  91   30   38   21    1    1    3   11 4737   67]
 [  19  105    8   57    0    1    6    4   59 4741]]
TESTING class wise acc@15 :: {0: [0.784], 1: [0.915], 2: [0.639], 3: [0.65], 4: [0.911], 5: [0.717], 6: [0.85], 7: [0.91], 8: [0.889], 9: [0.868]}
TESTING epoch@15 ::  acc@0.813  loss@0.658
Epoch@@16
[[784  24  36  36  19   1   1  13  63  23]
 [  9 915   0   9   1   0   0   1  12  53]
 [ 59   2 639  54 154  33  32  20   4   3]
 [ 22   4  28 650  92 118  44  28  11   3]
 [  5   1  10  24 911   8  13  27   1   0]
 [  5   0   5 135  59 717  10  66   2   1]
 [  2   0  27  49  57   5 850   4   2   4]
 [ 11   0   4   8  44  18   2 910   2   1]
 [ 41  17  10   8   2   1   2   6 889  24]
 [ 11  63   2  31   2   0   4   1  18 868]]
TRAINING class wise acc@16 :: {0: [0.8616], 1: [0.9404], 2: [0.872], 3: [0.8308], 4: [0.9016], 5: [0.8118], 6: [0.9248], 7: [0.9352], 8: [0.9522], 9: [0.9492]}
TRAIINING epoch@16 ::  acc@0.898  loss@0.310
[[2154   17  123   47   10    2    0   23  104   20]
 [  23 2351    2   13    0    0    0    0   19   92]
 [ 145    2 4360  147  160   19  114   27   18    8]
 [  30    6  116 4154   85  411  111   49    7   31]
 [  17    0  162   93 4508   80   42   96    2    0]
 [   5    0   32  624  119 4059   28  127    1    5]
 [   2    0  131  146   40   30 4624    1    5   21]
 [  26    0   29   86   83   87    0 4676    9    4]
 [  91   27   25   19    0    2    5   11 4761   59]
 [  14  103   14   56    1    1    6    4   55 4746]]
TESTING class wise acc@16 :: {0: [0.855], 1: [0.883], 2: [0.588], 3: [0.571], 4: [0.723], 5: [0.795], 6: [0.68], 7: [0.937], 8: [0.83], 9: [0.876]}
TESTING epoch@16 ::  acc@0.774  loss@0.808
Epoch@@17
[[855  11  24  31  12   3   1  17  31  15]
 [ 26 883   0  21   1   2   0   3   9  55]
 [100   1 588  52  90  49  11 105   2   2]
 [ 22   1  34 571  43 232   6  81   2   8]
 [ 18   0  17  11 723  33   5 193   0   0]
 [  3   0   8  69  24 795   1  96   2   2]
 [  5   0  59  73  80  91 680   9   2   1]
 [  9   0   1  12  11  26   0 937   2   2]
 [ 97  13   6   5   1   3   0  10 830  35]
 [ 24  48   1  19   2   4   2   7  17 876]]
TRAINING class wise acc@17 :: {0: [0.8696], 1: [0.9464], 2: [0.8778], 3: [0.8446], 4: [0.9114], 5: [0.831], 6: [0.926], 7: [0.9462], 8: [0.9556], 9: [0.9624]}
TRAIINING epoch@17 ::  acc@0.907  loss@0.278
[[2174   18  107   51   17    1    0   25   91   16]
 [  18 2366    0   11    0    0    0    0   17   88]
 [ 119    0 4389  145  152   26  111   30   21    7]
 [  29    6  114 4223   70  374   91   59    6   28]
 [  19    0  142   90 4557   72   37   81    2    0]
 [   1    0   33  578   83 4155   27  116    1    6]
 [   3    0  152  134   44   27 4630    0    1    9]
 [  16    0   27   84   56   76    0 4731    7    3]
 [  85   22   33    8    0    3    1   12 4778   58]
 [  11   72    7   32    0    0    7    7   52 4812]]
TESTING class wise acc@17 :: {0: [0.828], 1: [0.937], 2: [0.736], 3: [0.801], 4: [0.755], 5: [0.613], 6: [0.691], 7: [0.906], 8: [0.927], 9: [0.827]}
TESTING epoch@17 ::  acc@0.802  loss@0.685
Epoch@@18
[[828  25  22  21   3   0   0   5  74  22]
 [  8 937   1   7   1   0   0   0  14  32]
 [ 92   0 736  81  30   5   9  24  20   3]
 [ 25   6  44 801  17  48   6  34  10   9]
 [ 20   2  55  85 755  20   8  52   1   2]
 [ 12   2  25 283   8 613   2  52   1   2]
 [  6   2  81 188  11   5 691   6   3   7]
 [ 12   0   8  35  24   9   0 906   4   2]
 [ 26  22   1   5   0   1   0   3 927  15]
 [  7 113   4  21   1   0   2   3  22 827]]
TRAINING class wise acc@18 :: {0: [0.8996], 1: [0.958], 2: [0.901], 3: [0.8662], 4: [0.9288], 5: [0.861], 6: [0.935], 7: [0.9538], 8: [0.9608], 9: [0.9658]}
TRAIINING epoch@18 ::  acc@0.922  loss@0.238
[[2249   17   77   36    8    0    0   17   87    9]
 [  13 2395    2   10    0    0    1    0   17   62]
 [ 106    1 4505  102  115   27   98   25   17    4]
 [  23    2   89 4331   55  333   89   42    7   29]
 [  15    0   99   83 4644   63   24   72    0    0]
 [   4    0   39  447   73 4305   28  101    1    2]
 [   2    1  121  120   35   23 4675    0    6   17]
 [  16    0   25   56   56   70    0 4769    6    2]
 [  82   19   25   11    0    0    2    6 4804   51]
 [   9   67    5   39    0    1    9    4   37 4829]]
TESTING class wise acc@18 :: {0: [0.772], 1: [0.853], 2: [0.842], 3: [0.751], 4: [0.613], 5: [0.688], 6: [0.873], 7: [0.823], 8: [0.856], 9: [0.82]}
TESTING epoch@18 ::  acc@0.789  loss@0.788
Epoch@@19
[[772   3 134  45   8   1   1   8  21   7]
 [ 31 853   4  13   0   1   1   1  37  59]
 [ 30   0 842  42  14  16  35  14   6   1]
 [ 10   1  90 751  10  63  40  17   7  11]
 [  5   1 209  85 613  19  42  21   5   0]
 [  1   0  47 201  20 688  16  23   3   1]
 [  2   0  70  39   1   7 873   3   1   4]
 [ 16   0  36  47  30  42   4 823   0   2]
 [ 90   7  20   9   0   3   3   0 856  12]
 [ 28  60  12  27   1   4  11   5  32 820]]
TRAINING class wise acc@19 :: {0: [0.908], 1: [0.9556], 2: [0.9116], 3: [0.8828], 4: [0.934], 5: [0.8706], 6: [0.944], 7: [0.954], 8: [0.9672], 9: [0.9624]}
TRAIINING epoch@19 ::  acc@0.929  loss@0.220
[[2270   15   83   34    5    1    0   19   68    5]
 [  15 2389    1    4    0    0    1    0   13   77]
 [  98    0 4558   82  106   22   83   24   18    9]
 [  17    6   65 4414   62  277   89   34    8   28]
 [  11    0   95   88 4670   55   32   49    0    0]
 [   1    0   24  430   65 4353   23   96    1    7]
 [   0    0   94  110   33   24 4720    0    0   19]
 [  17    0   22   44   53   80    1 4770    6    7]
 [  63   15   23    7    0    2    0    4 4836   50]
 [   7   79   10   34    1    2   10    7   38 4812]]
TESTING class wise acc@19 :: {0: [0.762], 1: [0.89], 2: [0.767], 3: [0.696], 4: [0.713], 5: [0.697], 6: [0.755], 7: [0.93], 8: [0.853], 9: [0.951]}
TESTING epoch@19 ::  acc@0.801  loss@0.777
Epoch@@20
[[762  22  59  36   4   0   1  24  44  48]
 [  3 890   0   3   0   1   0   1   4  98]
 [ 39   0 767  45  51  25  17  42   4  10]
 [ 15   6  61 696  38  91  14  57   5  17]
 [ 18   0  63  59 713  14   8 118   4   3]
 [  1   1  31 150  26 697   2  86   1   5]
 [  5   1  93  66  30  15 755  12   3  20]
 [ 12   0   3  25  11  13   0 930   0   6]
 [ 36  31   9   6   0   3   1  10 853  51]
 [  5  24   2   4   1   1   1   3   8 951]]
TRAINING class wise acc@20 :: {0: [0.9108], 1: [0.956], 2: [0.9186], 3: [0.894], 4: [0.9376], 5: [0.8826], 6: [0.9508], 7: [0.955], 8: [0.966], 9: [0.9682]}
TRAIINING epoch@20 ::  acc@0.934  loss@0.200
[[2277   18   85   29    7    1    0   14   63    6]
 [  15 2390    1    3    0    0    2    0   21   68]
 [  91    0 4593   84   91   17   79   19   21    5]
 [  19    7   62 4470   53  271   64   31    8   15]
 [  14    0   90   63 4688   63   19   63    0    0]
 [   0    0   30  362   63 4413   20  108    2    2]
 [   1    2   89   90   19   28 4754    1    3   13]
 [  11    0   24   51   61   72    0 4775    5    1]
 [  56   24   29    6    0    1    2    3 4830   49]
 [   4   63    6   34    0    0    8    2   42 4841]]
TESTING class wise acc@20 :: {0: [0.652], 1: [0.887], 2: [0.742], 3: [0.716], 4: [0.744], 5: [0.713], 6: [0.933], 7: [0.85], 8: [0.894], 9: [0.943]}
TESTING epoch@20 ::  acc@0.807  loss@0.750
Epoch@@21
[[652  29  87  51  12   1   6  12  97  53]
 [  2 887   1   5   0   0   0   1   5  99]
 [ 26   0 742  56  34  24  72  15  17  14]
 [  3   6  42 716  26  80  81  14   5  27]
 [  3   0  68  47 744  31  44  48  11   4]
 [  2   0  30 186  13 713  28  17   6   5]
 [  0   0  26  26   4   5 933   2   2   2]
 [  7   0  10  35  22  64   0 850   5   7]
 [ 17  27   6   8   0   2   1   3 894  42]
 [  2  25   2   8   0   2   2   0  16 943]]
TRAINING class wise acc@21 :: {0: [0.9312], 1: [0.9664], 2: [0.93], 3: [0.912], 4: [0.9508], 5: [0.8992], 6: [0.9586], 7: [0.9664], 8: [0.971], 9: [0.9722]}
TRAIINING epoch@21 ::  acc@0.945  loss@0.170
[[2328   16   60   17    9    1    0    8   56    5]
 [  12 2416    0    5    0    0    0    0   15   52]
 [  71    0 4650   72   78   20   66   24   15    4]
 [  15    1   50 4560   43  222   58   27    4   20]
 [   6    0   81   54 4754   42   15   48    0    0]
 [   3    0   29  341   52 4496   20   56    0    3]
 [   1    1   78   75   17   19 4793    0    2   14]
 [  10    0   17   32   42   56    0 4832    9    2]
 [  50   17   23    6    0    1    1    8 4855   39]
 [   9   49    5   23    0    0    8    5   40 4861]]
TESTING class wise acc@21 :: {0: [0.762], 1: [0.866], 2: [0.674], 3: [0.797], 4: [0.888], 5: [0.573], 6: [0.859], 7: [0.842], 8: [0.908], 9: [0.926]}
TESTING epoch@21 ::  acc@0.809  loss@0.756
Epoch@@22
[[762  11  44  47  36   0   6   7  59  28]
 [  5 866   1  10   1   1   3   1  23  89]
 [ 44   1 674  92 102  14  51  12   6   4]
 [  6   2  25 797  58  35  43  20   4  10]
 [  1   0  25  54 888   4  16  10   1   1]
 [  3   0  17 310  48 573  10  36   1   2]
 [  4   0  30  60  29   5 859   4   6   3]
 [  8   0   9  55  55  26   1 842   0   4]
 [ 26  15  10  11   4   0   3   2 908  21]
 [  4  28   4  16   0   0   3   1  18 926]]
TRAINING class wise acc@22 :: {0: [0.9332], 1: [0.9636], 2: [0.9366], 3: [0.9152], 4: [0.9532], 5: [0.9108], 6: [0.9578], 7: [0.9668], 8: [0.9746], 9: [0.9638]}
TRAIINING epoch@22 ::  acc@0.947  loss@0.162
[[2333   11   68   19    6    0    0   12   37   14]
 [  12 2409    0    5    0    0    0    0   14   60]
 [  83    0 4683   50   60   25   64   15   15    5]
 [  16    2   47 4576   40  196   71   26    3   23]
 [   7    0   72   47 4766   38   14   55    1    0]
 [   2    0   20  269   55 4554   24   73    0    3]
 [   1    0   64   96   16   23 4789    0    1   10]
 [  16    0   12   31   48   51    0 4834    7    1]
 [  37   21   18    3    0    1    0    7 4873   40]
 [  15   75    2   36    0    5    9    3   36 4819]]
TESTING class wise acc@22 :: {0: [0.677], 1: [0.848], 2: [0.744], 3: [0.591], 4: [0.819], 5: [0.842], 6: [0.919], 7: [0.846], 8: [0.868], 9: [0.793]}
TESTING epoch@22 ::  acc@0.795  loss@0.946
Epoch@@23
[[677   3  98  61  21  10   7  72  41  10]
 [ 27 848   2  26   0   2   2  11  31  51]
 [ 34   1 744  28  54  56  64  18   1   0]
 [  2   1  41 591  41 219  84  17   3   1]
 [  3   0  34  30 819  44  27  42   0   1]
 [  0   0  13  78  28 842  27  12   0   0]
 [  2   0  21  21  19  17 919   1   0   0]
 [  1   0   9  13  23 108   0 846   0   0]
 [ 33   8  22  17   1  10   5  14 868  22]
 [  9  38   5  51   3  16  20  42  23 793]]
TRAINING class wise acc@23 :: {0: [0.9408], 1: [0.9668], 2: [0.9386], 3: [0.925], 4: [0.957], 5: [0.919], 6: [0.9626], 7: [0.9728], 8: [0.9776], 9: [0.972]}
TRAIINING epoch@23 ::  acc@0.953  loss@0.144
[[2352    8   52   21    5    0    0   13   45    4]
 [  17 2417    1    6    0    0    1    0   10   48]
 [  62    0 4693   57   78   17   56    9   16   12]
 [  19    2   36 4625   31  189   51   24    0   23]
 [   7    0   69   46 4785   34   13   45    0    1]
 [   1    0   23  256   40 4595   23   55    0    7]
 [   0    1   66   73   21   16 4813    0    0   10]
 [  14    0   11   30   31   47    0 4864    2    1]
 [  41   14   16    1    0    0    0    7 4888   33]
 [   4   53    7   26    1    3    7    2   37 4860]]
TESTING class wise acc@23 :: {0: [0.589], 1: [0.911], 2: [0.693], 3: [0.656], 4: [0.788], 5: [0.81], 6: [0.914], 7: [0.87], 8: [0.909], 9: [0.938]}
TESTING epoch@23 ::  acc@0.808  loss@0.945
Epoch@@24
[[589  41  59  63  19   4  12  19 124  70]
 [  1 911   0   3   0   1   0   0   6  78]
 [ 26   3 693  66  49  39  79  23   9  13]
 [  5   3  27 656  25 163  59  29   7  26]
 [  2   2  37  58 788  24  30  50   6   3]
 [  0   0  14 100  13 810  16  34   1  12]
 [  0   1  14  39   7  16 914   4   1   4]
 [  4   0   6  30  20  54   3 870   4   9]
 [  7  21   3   5   0   3   0   4 909  48]
 [  1  40   0   7   1   1   4   0   8 938]]
TRAINING class wise acc@24 :: {0: [0.938], 1: [0.9724], 2: [0.9438], 3: [0.9326], 4: [0.9574], 5: [0.9344], 6: [0.965], 7: [0.9722], 8: [0.9758], 9: [0.9768]}
TRAIINING epoch@24 ::  acc@0.957  loss@0.135
[[2345   12   49   23    7    0    1    8   47    8]
 [   9 2431    0    5    0    0    0    0   14   41]
 [  64    0 4719   52   64   16   51   14   14    6]
 [  24    2   44 4663   32  140   56   25    2   12]
 [  10    0   61   44 4787   32   16   50    0    0]
 [   0    0   17  193   39 4672   20   53    3    3]
 [   1    1   61   57   18   19 4825    0    1   17]
 [  10    0   12   25   37   48    0 4861    5    2]
 [  34   12   23    9    0    3    1    8 4879   31]
 [   6   38    6   22    0    2    7    3   32 4884]]
TESTING class wise acc@24 :: {0: [0.825], 1: [0.916], 2: [0.652], 3: [0.662], 4: [0.846], 5: [0.743], 6: [0.913], 7: [0.919], 8: [0.898], 9: [0.806]}
TESTING epoch@24 ::  acc@0.818  loss@0.824
Epoch@@25
[[825  10  49  20   7   0   2  21  60   6]
 [ 16 916   2   7   0   0   4   0  30  25]
 [ 56   0 652  45 103  34  68  33   8   1]
 [ 25   6  34 662  59  97  51  49  15   2]
 [ 11   0  19  29 846  18  25  49   3   0]
 [  5   0   6 150  28 743  16  48   4   0]
 [  4   0  12  35  17  10 913   1   8   0]
 [  8   0   2  14  27  26   3 919   0   1]
 [ 44  15   8   7   4   2   2  10 898  10]
 [ 17  74   5  24   3   3  12   9  47 806]]
TRAINING class wise acc@25 :: {0: [0.9452], 1: [0.9728], 2: [0.9496], 3: [0.9298], 4: [0.9632], 5: [0.9384], 6: [0.9654], 7: [0.9754], 8: [0.9798], 9: [0.9748]}
TRAIINING epoch@25 ::  acc@0.959  loss@0.127
[[2363   10   45   20    9    0    0   10   37    6]
 [  11 2432    0    7    0    0    0    0   11   39]
 [  57    0 4748   42   60   18   44   10   15    6]
 [  13    5   29 4649   43  166   57   16    2   20]
 [   8    0   45   43 4816   37   14   37    0    0]
 [   0    0   15  198   28 4692   20   41    4    2]
 [   0    0   54   77   10   21 4827    0    3    8]
 [   5    0   14   25   37   37    0 4877    3    2]
 [  29   11   16    0    0    3    1    7 4899   34]
 [   7   46    4   23    0    5    3    4   34 4874]]
TESTING class wise acc@25 :: {0: [0.858], 1: [0.912], 2: [0.753], 3: [0.812], 4: [0.855], 5: [0.546], 6: [0.859], 7: [0.838], 8: [0.915], 9: [0.849]}
TESTING epoch@25 ::  acc@0.820  loss@0.744
Epoch@@26
[[858   9  46  27   8   0   1   1  41   9]
 [ 11 912   2  11   0   0   2   0  18  44]
 [ 58   0 753  72  51   8  41   9   7   1]
 [ 24   5  40 812  38  27  32  10   6   6]
 [ 12   1  37  63 855   2  16  13   1   0]
 [  8   0  45 320  36 546   9  32   2   2]
 [  7   0  32  80  14   3 859   2   1   2]
 [ 23   0  34  38  49  11   0 838   4   3]
 [ 50  11   6   6   0   1   1   2 915   8]
 [ 21  67   3  20   2   1   4   0  33 849]]
TRAINING class wise acc@26 :: {0: [0.9536], 1: [0.972], 2: [0.9564], 3: [0.9446], 4: [0.9656], 5: [0.945], 6: [0.9726], 7: [0.9786], 8: [0.9804], 9: [0.981]}
TRAIINING epoch@26 ::  acc@0.965  loss@0.111
[[2384   14   35   15    5    0    0    8   33    6]
 [  14 2430    0    4    0    0    0    0    8   44]
 [  53    0 4782   33   59    9   37   10   12    5]
 [  16    4   23 4723   29  135   40   13    4   13]
 [   8    0   57   32 4828   29   21   25    0    0]
 [   2    0   14  176   29 4725   12   39    0    3]
 [   1    1   53   36   21   19 4863    0    0    6]
 [   9    0    8   21   27   34    0 4893    5    3]
 [  42   15   16    4    0    0    0    2 4902   19]
 [   6   37    5   15    0    5    6    1   20 4905]]
TESTING class wise acc@26 :: {0: [0.726], 1: [0.734], 2: [0.738], 3: [0.79], 4: [0.855], 5: [0.634], 6: [0.828], 7: [0.855], 8: [0.948], 9: [0.933]}
TESTING epoch@26 ::  acc@0.804  loss@0.918
Epoch@@27
[[726   2  38  33  12   0   1   9 142  37]
 [ 11 734   2  12   0   0   1   1  79 160]
 [ 57   0 738  61  70  14  20  19  16   5]
 [ 12   2  46 790  37  48  19  14  18  14]
 [  4   0  29  66 855  11   5  21   7   2]
 [  3   1  28 218  59 634  11  36   5   5]
 [  4   0  47  76  35   5 828   0   2   3]
 [  6   0  15  42  52  21   0 855   7   2]
 [ 20   3   4   7   0   1   0   2 948  15]
 [  3   6   4  15   1   0   0   1  37 933]]
TRAINING class wise acc@27 :: {0: [0.9536], 1: [0.9768], 2: [0.9558], 3: [0.9458], 4: [0.9672], 5: [0.945], 6: [0.9706], 7: [0.976], 8: [0.981], 9: [0.9828]}
TRAIINING epoch@27 ::  acc@0.965  loss@0.109
[[2384   12   37   20    5    0    0    7   30    5]
 [   9 2442    0    1    0    0    1    1   11   35]
 [  46    0 4779   35   47   13   54    9   12    5]
 [  16    0   26 4729   30  124   37   20    4   14]
 [   7    0   45   33 4836   38   11   30    0    0]
 [   0    0   14  156   44 4725   13   42    2    4]
 [   1    1   55   53   18   16 4853    1    0    2]
 [   8    0   13   23   35   34    0 4880    6    1]
 [  35   10   11    4    0    0    1    4 4905   30]
 [   2   31    2   21    0    2    6    1   21 4914]]
TESTING class wise acc@27 :: {0: [0.753], 1: [0.809], 2: [0.779], 3: [0.756], 4: [0.795], 5: [0.649], 6: [0.863], 7: [0.93], 8: [0.914], 9: [0.716]}
TESTING epoch@27 ::  acc@0.796  loss@0.930
Epoch@@28
[[753   0 104  29  13   0   2  39  60   0]
 [ 29 809   9  18   0   3   4   9  86  33]
 [ 34   0 779  52  38  19  36  33   9   0]
 [ 16   1  44 756  39  56  42  41   5   0]
 [  9   0  47  58 795   7  23  60   1   0]
 [  5   0  35 231  25 649   6  48   1   0]
 [  2   0  41  70   9   7 863   6   2   0]
 [  6   0   8  25  14  14   2 930   1   0]
 [ 35   2  15   9   0   3   1  17 914   4]
 [ 29  42   9  34   1   4   9  44 112 716]]
TRAINING class wise acc@28 :: {0: [0.958], 1: [0.9788], 2: [0.961], 3: [0.9498], 4: [0.9696], 5: [0.9512], 6: [0.9698], 7: [0.9808], 8: [0.9834], 9: [0.9824]}
TRAIINING epoch@28 ::  acc@0.968  loss@0.100
[[2395    8   35   14    5    0    0   10   31    2]
 [   5 2447    0    1    0    0    1    0   14   32]
 [  41    0 4805   31   32   16   47   12    8    8]
 [  17    1   23 4749   26  114   38   17    3   12]
 [   5    0   39   30 4848   33   13   32    0    0]
 [   0    0   16  137   35 4756   17   35    1    3]
 [   0    0   57   54   10   19 4849    0    0   11]
 [   8    0    8   24   26   26    0 4904    3    1]
 [  23   10   16    1    0    2    0    4 4917   27]
 [   5   33    3   20    0    2    6    1   18 4912]]
TESTING class wise acc@28 :: {0: [0.826], 1: [0.812], 2: [0.81], 3: [0.694], 4: [0.834], 5: [0.684], 6: [0.909], 7: [0.837], 8: [0.902], 9: [0.956]}
TESTING epoch@28 ::  acc@0.826  loss@0.849
Epoch@@29
[[826   8  39  21   9   1   6   5  38  47]
 [ 12 812   8   5   0   0   3   1  15 144]
 [ 58   0 810  26  36  12  37  11   7   3]
 [ 26   1  80 694  43  72  61  11   2  10]
 [ 11   0  71  33 834  11  25  14   0   1]
 [  6   0  48 168  41 684  27  19   2   5]
 [  2   0  38  35   8   3 909   4   1   0]
 [ 17   0  23  43  54  23   0 837   0   3]
 [ 35   8  11   2   2   1   3   1 902  35]
 [  3  10   4  12   2   1   2   2   8 956]]
TRAINING class wise acc@29 :: {0: [0.9548], 1: [0.9768], 2: [0.9606], 3: [0.9524], 4: [0.9682], 5: [0.9598], 6: [0.9794], 7: [0.9788], 8: [0.9846], 9: [0.9824]}
TRAIINING epoch@29 ::  acc@0.970  loss@0.095
[[2387   10   48   13    2    0    0    8   26    6]
 [  10 2442    0    4    0    0    1    0    9   34]
 [  42    1 4803   31   45    6   44    8   14    6]
 [  10    3   23 4762   36  112   28   15    2    9]
 [   8    0   42   37 4841   26   13   33    0    0]
 [   0    0   14  123   22 4799   10   28    1    3]
 [   0    0   38   37   13    9 4897    1    1    4]
 [   7    0    6   27   32   26    0 4894    4    4]
 [  26   10    9    3    0    1    1    4 4923   23]
 [   6   30    5   16    0    3    6    1   21 4912]]
TESTING class wise acc@29 :: {0: [0.842], 1: [0.898], 2: [0.776], 3: [0.629], 4: [0.828], 5: [0.566], 6: [0.952], 7: [0.825], 8: [0.873], 9: [0.9]}
TESTING epoch@29 ::  acc@0.809  loss@0.931
Epoch@@30
[[842   5  72  17  16   0   6   3  23  16]
 [ 17 898   1   3   0   1   2   2   7  69]
 [ 37   0 776  19  56   7  91   7   6   1]
 [ 15   1  86 629  67  29 137  12   4  20]
 [  4   0  53  33 828   0  68  13   0   1]
 [  3   0  68 168  59 566  91  32   0  13]
 [  2   0  20  12   9   0 952   1   2   2]
 [ 19   0  36  41  59  10   8 825   0   2]
 [ 54  15  19   4   0   2   3   2 873  28]
 [ 16  49   4   6   1   0   9   1  14 900]]
TRAINING class wise acc@30 :: {0: [0.966], 1: [0.986], 2: [0.9672], 3: [0.9594], 4: [0.9732], 5: [0.9584], 6: [0.9758], 7: [0.9836], 8: [0.9852], 9: [0.987]}
TRAIINING epoch@30 ::  acc@0.974  loss@0.085
[[2415    3   21   11    2    0    0    9   33    6]
 [   3 2465    0    2    0    0    0    0    5   25]
 [  30    0 4836   24   39   11   40    9    4    7]
 [  15    1   23 4797   19  104   18   13    2    8]
 [   5    0   40   27 4866   21   19   22    0    0]
 [   0    0   13  123   17 4792   18   34    0    3]
 [   1    0   44   35   21   14 4879    0    1    5]
 [   9    0    5   14   16   30    0 4918    4    4]
 [  24   13    7    4    0    1    0    6 4926   19]
 [   5   24    2    9    0    2    3    3   17 4935]]
TESTING class wise acc@30 :: {0: [0.858], 1: [0.917], 2: [0.745], 3: [0.647], 4: [0.813], 5: [0.796], 6: [0.889], 7: [0.854], 8: [0.894], 9: [0.843]}
TESTING epoch@30 ::  acc@0.826  loss@0.835
Epoch@@31
[[858  12  37  23   7   2   4   5  39  13]
 [ 13 917   2  10   0   2   1   1  22  32]
 [ 77   1 745  43  44  27  42   9   7   5]
 [ 24   1  46 647  33 178  40  12   8  11]
 [ 15   0  52  46 813  38  20  14   2   0]
 [  5   2  35  99  21 796  12  27   2   1]
 [  6   0  22  43  14  12 889   4   5   5]
 [ 13   0  11  32  33  50   1 854   3   3]
 [ 51  13   9   4   1   2   2   1 894  23]
 [ 12  80   2  18   2   4  11   5  23 843]]
TRAINING class wise acc@31 :: {0: [0.968], 1: [0.984], 2: [0.9706], 3: [0.9614], 4: [0.974], 5: [0.9646], 6: [0.9784], 7: [0.9856], 8: [0.9852], 9: [0.986]}
TRAIINING epoch@31 ::  acc@0.976  loss@0.080
[[2420   10   25   12    3    0    0    1   27    2]
 [   8 2460    0    2    0    0    1    0    6   23]
 [  28    1 4853   23   28   13   36    6   11    1]
 [  17    0   21 4807   28   80   28    9    0   10]
 [   4    0   35   29 4870   25   10   27    0    0]
 [   0    0   12  101   24 4823   12   26    1    1]
 [   0    0   40   44    7   10 4892    0    0    7]
 [   2    0    8   11   25   18    0 4928    8    0]
 [  23   10    7    4    0    1    0    5 4926   24]
 [   4   18    1   16    0    0    6    1   24 4930]]
TESTING class wise acc@31 :: {0: [0.868], 1: [0.898], 2: [0.825], 3: [0.661], 4: [0.848], 5: [0.694], 6: [0.789], 7: [0.832], 8: [0.913], 9: [0.884]}
TESTING epoch@31 ::  acc@0.821  loss@0.845
Epoch@@32
[[868   8  43  12  12   0   0   5  43   9]
 [ 22 898   0   3   0   0   0   0  22  55]
 [ 49   0 825  23  57  15   9  12   8   2]
 [ 33   4 103 661  59  77  22  15  13  13]
 [ 19   1  53  37 848  11   6  22   3   0]
 [  9   2  90 112  46 694   3  33   2   9]
 [  3   3  89  56  33  11 789   3   2  11]
 [ 23   0  42  15  60  15   1 832   9   3]
 [ 54   8   6   0   0   1   0   1 913  17]
 [ 19  49   6   8   0   0   1   1  32 884]]
TRAINING class wise acc@32 :: {0: [0.9676], 1: [0.9772], 2: [0.9692], 3: [0.9616], 4: [0.9792], 5: [0.961], 6: [0.9784], 7: [0.9834], 8: [0.9884], 9: [0.9822]}
TRAIINING epoch@32 ::  acc@0.975  loss@0.080
[[2419   11   32   13    3    0    0    6   14    2]
 [  12 2443    0    2    0    0    0    0    7   36]
 [  35    0 4846   20   33   15   29    7   11    4]
 [  14    4   22 4808   18   82   27   17    2    6]
 [   5    0   32   22 4896   16    8   21    0    0]
 [   0    0   13  111   20 4805   16   26    2    7]
 [   1    0   26   32   15   21 4892    0    0   13]
 [   5    0   12   17   22   20    0 4917    5    2]
 [  16    4   13    4    0    1    0    6 4942   14]
 [   2   39    6   16    0    5    6    2   13 4911]]
TESTING class wise acc@32 :: {0: [0.731], 1: [0.777], 2: [0.671], 3: [0.759], 4: [0.791], 5: [0.645], 6: [0.956], 7: [0.786], 8: [0.894], 9: [0.931]}
TESTING epoch@32 ::  acc@0.794  loss@1.104
Epoch@@33
[[731   2  60  59  14   1  15   7  78  33]
 [ 15 777   4  14   0   2  17   0  29 142]
 [ 42   0 671  75  41  27 123  11   6   4]
 [  8   1  25 759  30  51 115   2   2   7]
 [  7   0  48  54 791  12  71  15   0   2]
 [  1   0  13 222  23 645  85   8   0   3]
 [  1   0   7  26   7   2 956   1   0   0]
 [  5   0  19  70  34  56  25 786   2   3]
 [ 23   1  21  13   1   1  16   6 894  24]
 [  3   7   4  14   1   1   8   1  30 931]]
TRAINING class wise acc@33 :: {0: [0.9716], 1: [0.9844], 2: [0.9724], 3: [0.9636], 4: [0.9722], 5: [0.9658], 6: [0.9812], 7: [0.9828], 8: [0.988], 9: [0.99]}
TRAIINING epoch@33 ::  acc@0.977  loss@0.075
[[2429    5   25   10    4    0    0    4   22    1]
 [   7 2461    0    2    0    0    0    0   11   19]
 [  27    0 4862   26   37    7   21    7    8    5]
 [   7    3   17 4818   21   81   27   15    2    9]
 [  10    0   35   31 4861   20   13   30    0    0]
 [   1    0    8   91   26 4829   12   28    1    4]
 [   0    0   36   25   12   12 4906    0    1    8]
 [   3    0    8   15   31   21    0 4914    5    3]
 [  20    8   11    3    0    2    0    4 4940   12]
 [   4   15    5   10    0    4    2    0   10 4950]]
TESTING class wise acc@33 :: {0: [0.757], 1: [0.879], 2: [0.75], 3: [0.763], 4: [0.807], 5: [0.684], 6: [0.824], 7: [0.929], 8: [0.943], 9: [0.915]}
TESTING epoch@33 ::  acc@0.825  loss@0.751
Epoch@@34
[[757  10  55  27   8   1   0  21  96  25]
 [  8 879   0   7   0   0   0   3  27  76]
 [ 37   2 750  54  57  29  32  25  11   3]
 [  8   3  39 763  37  74  14  42   7  13]
 [  8   0  31  59 807  21  20  52   1   1]
 [  4   0  21 196  23 684   5  60   3   4]
 [  5   0  21 108  17  15 824   4   4   2]
 [  7   0   7  18  20  14   1 929   3   1]
 [ 19   5   4   6   0   1   0   5 943  17]
 [  4  24   2   9   0   2   3   6  35 915]]
TRAINING class wise acc@34 :: {0: [0.9652], 1: [0.9836], 2: [0.9698], 3: [0.9674], 4: [0.974], 5: [0.969], 6: [0.98], 7: [0.9854], 8: [0.9856], 9: [0.9848]}
TRAIINING epoch@34 ::  acc@0.977  loss@0.075
[[2413    7   21   14    7    0    1    5   26    6]
 [   4 2459    0    2    0    0    0    0   10   25]
 [  34    0 4849   17   46   12   23    6    8    5]
 [   8    2   17 4837   25   60   30   14    0    7]
 [   5    0   46   24 4870   22    8   25    0    0]
 [   0    0   11   90   20 4845   16   18    0    0]
 [   0    0   32   37   10   14 4900    0    1    6]
 [   7    0    7   12   21   22    0 4927    3    1]
 [  28    7   10    1    0    2    0    3 4928   21]
 [   4   25    4   11    0    0    4    1   27 4924]]
TESTING class wise acc@34 :: {0: [0.808], 1: [0.873], 2: [0.701], 3: [0.672], 4: [0.687], 5: [0.767], 6: [0.936], 7: [0.898], 8: [0.961], 9: [0.838]}
TESTING epoch@34 ::  acc@0.814  loss@0.979
Epoch@@35
[[808   2  52  16   2   1   7   7  97   8]
 [ 17 873   2   8   0   1   2   2  51  44]
 [ 41   0 701  48  20  33 112  21  22   2]
 [ 19   2  35 672  10 121  85  20  28   8]
 [  9   0  44  94 687  36  82  38  10   0]
 [  7   0  19 131   9 767  28  27  10   2]
 [  4   1  14  20   2  14 936   2   7   0]
 [ 14   0  19  17  10  32   4 898   5   1]
 [ 21   2   1   1   0   4   2   1 961   7]
 [ 20  53   1   6   1   2   8   3  68 838]]
TRAINING class wise acc@35 :: {0: [0.9652], 1: [0.9772], 2: [0.9688], 3: [0.9618], 4: [0.9726], 5: [0.962], 6: [0.9812], 7: [0.983], 8: [0.985], 9: [0.9834]}
TRAIINING epoch@35 ::  acc@0.974  loss@0.082
[[2413    4   31   14    6    0    0    8   21    3]
 [   8 2443    0    7    0    0    2    0    9   31]
 [  32    3 4844   20   40   14   30    5   10    2]
 [  13    3   15 4809   25   82   28   13    3    9]
 [   5    0   47   25 4863   26    5   29    0    0]
 [   0    0   23  107   24 4810    7   24    2    3]
 [   1    1   35   29    8   11 4906    0    2    7]
 [   6    0    4   20   26   25    0 4915    4    0]
 [  31   11   10    3    0    0    1    3 4925   16]
 [   1   38    5   16    0    2    3    0   18 4917]]
TESTING class wise acc@35 :: {0: [0.805], 1: [0.88], 2: [0.743], 3: [0.722], 4: [0.794], 5: [0.811], 6: [0.876], 7: [0.818], 8: [0.94], 9: [0.9]}
TESTING epoch@35 ::  acc@0.829  loss@0.883
Epoch@@36
[[805   4  36  24   5   3   2   9  93  19]
 [  9 880   2  12   0   0   4   0  43  50]
 [ 45   0 743  56  46  51  35  11  12   1]
 [ 11   1  41 722  17 150  28  12  12   6]
 [  9   1  34  65 794  53  21  19   4   0]
 [  4   0  21 128  16 811   3  13   0   4]
 [  3   0  23  42   9  43 876   1   2   1]
 [ 13   0  14  22  39  83   3 818   6   2]
 [ 20   4   5   4   0   3   0   4 940  20]
 [  6  38   3  13   1   5   5   3  26 900]]
TRAINING class wise acc@36 :: {0: [0.97], 1: [0.9856], 2: [0.9706], 3: [0.9676], 4: [0.9792], 5: [0.9698], 6: [0.9828], 7: [0.9866], 8: [0.9878], 9: [0.9886]}
TRAIINING epoch@36 ::  acc@0.979  loss@0.068
[[2425    7   23   11    7    0    0    5   21    1]
 [   7 2464    0    2    0    0    0    0    6   21]
 [  30    0 4853   13   34   18   28    7   13    4]
 [   8    0   19 4838   15   76   24   10    1    9]
 [   7    0   32   23 4896   21    7   13    1    0]
 [   0    0   16   81   17 4849   10   24    0    3]
 [   0    0   32   26   10   13 4914    0    0    5]
 [   3    0    9   18   15   17    0 4933    4    1]
 [  18    6   13    0    0    1    1    6 4939   16]
 [   1   20    1   11    0    3    3    2   16 4943]]
TESTING class wise acc@36 :: {0: [0.729], 1: [0.903], 2: [0.789], 3: [0.667], 4: [0.887], 5: [0.796], 6: [0.855], 7: [0.82], 8: [0.902], 9: [0.913]}
TESTING epoch@36 ::  acc@0.826  loss@0.881
Epoch@@37
[[729   5 113  38  38   0   2   8  45  22]
 [  7 903   1   7   0   1   0   1  14  66]
 [ 16   0 789  49  74  31  19  10  10   2]
 [  6   1  55 667  56 146  36  17   6  10]
 [  3   0  34  39 887  13  13  10   0   1]
 [  2   0  28 102  46 796   5  18   0   3]
 [  5   0  42  53  16  14 855   3   3   9]
 [  5   0  11  27  84  50   1 820   1   1]
 [ 39  10   6   6   1   2   1   5 902  28]
 [ 10  38   6  12   2   1   0   4  14 913]]
TRAINING class wise acc@37 :: {0: [0.9688], 1: [0.9848], 2: [0.9768], 3: [0.9668], 4: [0.983], 5: [0.9712], 6: [0.9824], 7: [0.9858], 8: [0.9868], 9: [0.9864]}
TRAIINING epoch@37 ::  acc@0.980  loss@0.065
[[2422    7   28    6    2    0    1    8   23    3]
 [   5 2462    0    3    0    0    0    0   10   20]
 [  32    0 4884   10   18   12   21    6   12    5]
 [   5    1   10 4834   22   76   28   16    1    7]
 [   5    0   18   25 4915   14    6   17    0    0]
 [   0    0   10   85   17 4856   11   20    0    1]
 [   0    1   28   34    6   14 4912    0    0    5]
 [   8    0    4   16   16   24    0 4929    2    1]
 [  21    9   11    0    1    0    0    5 4934   19]
 [   2   27    5    7    0    2    5    2   18 4932]]
TESTING class wise acc@37 :: {0: [0.833], 1: [0.821], 2: [0.757], 3: [0.672], 4: [0.857], 5: [0.742], 6: [0.932], 7: [0.814], 8: [0.92], 9: [0.929]}
TESTING epoch@37 ::  acc@0.828  loss@0.948
Epoch@@38
[[833   3  59  10  19   1   7   4  40  24]
 [ 16 821   5   3   0   1   4   0  44 106]
 [ 43   0 757  28  60  29  65   9   6   3]
 [ 15   1  57 672  34  86  89  15  12  19]
 [  7   0  35  28 857  19  35  16   2   1]
 [  4   0  35 141  22 742  32  18   1   5]
 [  5   0  28  17   9   7 932   0   2   0]
 [ 18   0  27  23  55  45  10 814   4   4]
 [ 50   1   7   2   1   1   3   2 920  13]
 [ 12  16   5   3   1   1   5   0  28 929]]
TRAINING class wise acc@38 :: {0: [0.9768], 1: [0.9876], 2: [0.975], 3: [0.9718], 4: [0.9806], 5: [0.977], 6: [0.983], 7: [0.989], 8: [0.9926], 9: [0.9886]}
TRAIINING epoch@38 ::  acc@0.982  loss@0.056
[[2442    3   20   12    7    0    0    2   13    1]
 [   3 2469    0    1    0    0    1    0    2   24]
 [  29    0 4875   16   31    8   24   10    5    2]
 [  12    4   17 4859   16   47   24   12    1    8]
 [   8    0   31   22 4903   18    7   11    0    0]
 [   0    0    6   60   13 4885    7   27    0    2]
 [   0    0   33   33    8    6 4915    0    0    5]
 [   0    0    7   13   13   19    0 4945    2    1]
 [  11    4    9    0    0    0    0    2 4963   11]
 [   1   27    1   11    0    2    3    0   12 4943]]
TESTING class wise acc@38 :: {0: [0.712], 1: [0.911], 2: [0.814], 3: [0.651], 4: [0.792], 5: [0.839], 6: [0.857], 7: [0.796], 8: [0.935], 9: [0.893]}
TESTING epoch@38 ::  acc@0.820  loss@0.953
Epoch@@39
[[712  16  88  28   7   5   1   3 119  21]
 [  5 911   3   7   1   1   0   0  29  43]
 [ 20   0 814  35  38  37  30   3  15   8]
 [  6   2  71 651  25 185  26   8  14  12]
 [  6   0  65  50 792  46  20  11   7   3]
 [  0   1  36  84  16 839   3  10   4   7]
 [  2   0  37  41  10  44 857   3   4   2]
 [ 11   0  15  24  40 104   0 796   7   3]
 [ 14   8   8   5   1   5   0   2 935  22]
 [  2  51   3  13   1   3   3   0  31 893]]
TRAINING class wise acc@39 :: {0: [0.9704], 1: [0.9868], 2: [0.9722], 3: [0.9758], 4: [0.9802], 5: [0.9762], 6: [0.9848], 7: [0.9842], 8: [0.9864], 9: [0.9884]}
TRAIINING epoch@39 ::  acc@0.981  loss@0.063
[[2426    6   30   10    3    0    0    4   20    1]
 [  11 2467    0    0    0    0    1    0    9   12]
 [  35    0 4861   16   29   14   23    8   10    4]
 [   6    0   15 4879    9   53   19    6    4    9]
 [   5    0   32   19 4901   15    8   20    0    0]
 [   0    0   10   63   20 4881    7   15    0    4]
 [   0    1   29   25    7    8 4924    0    0    6]
 [   6    0   10   15   25   18    0 4921    4    1]
 [  17    8   14    2    0    0    0    6 4932   21]
 [   1   18    6   10    0    2    5    0   16 4942]]
TESTING class wise acc@39 :: {0: [0.734], 1: [0.897], 2: [0.821], 3: [0.727], 4: [0.849], 5: [0.739], 6: [0.921], 7: [0.835], 8: [0.88], 9: [0.91]}
TESTING epoch@39 ::  acc@0.831  loss@0.832
Epoch@@40
[[734   3 117  41  13   0   7   4  59  22]
 [  7 897   6  11   0   0   2   0  12  65]
 [ 16   0 821  36  46  24  46   7   4   0]
 [  7   2  55 727  40  84  68  11   1   5]
 [  3   1  56  40 849  10  32   8   1   0]
 [  2   0  50 139  28 739  24  17   0   1]
 [  2   0  27  26  14   9 921   0   1   0]
 [  5   0  24  43  48  39   3 835   1   2]
 [ 23  19  21  13   0   2   4   6 880  32]
 [  3  41   6  15   2   3   4   1  15 910]]
TRAINING class wise acc@40 :: {0: [0.9772], 1: [0.9904], 2: [0.9818], 3: [0.9708], 4: [0.9828], 5: [0.9726], 6: [0.9872], 7: [0.9886], 8: [0.992], 9: [0.9908]}
TRAIINING epoch@40 ::  acc@0.983  loss@0.055
[[2443    6   17    8    3    0    1    2   16    4]
 [   3 2476    0    2    0    0    0    0    2   17]
 [  18    0 4909   15   20    6   21    4    4    3]
 [   4    2   11 4854   24   69   17    8    2    9]
 [   0    0   20   23 4914   17    9   17    0    0]
 [   0    0   13   71   14 4863   13   22    0    4]
 [   0    1   22   20   10    9 4936    0    0    2]
 [   2    0    4   11   18   19    0 4943    2    1]
 [   9    6    6    2    1    0    0    0 4960   16]
 [   6   13    1    6    0    2    3    0   15 4954]]
TESTING class wise acc@40 :: {0: [0.676], 1: [0.81], 2: [0.712], 3: [0.533], 4: [0.827], 5: [0.849], 6: [0.927], 7: [0.881], 8: [0.922], 9: [0.921]}
TESTING epoch@40 ::  acc@0.806  loss@1.054
Epoch@@41
[[676   3  84  57  12   8   7  16  92  45]
 [  2 810   2   9   0   1   7   2  23 144]
 [ 15   0 712  29  56  71  83  22   8   4]
 [  3   1  31 533  57 257  75  23   5  15]
 [  4   0  35  29 827  33  31  40   0   1]
 [  1   0  15  51  33 849  25  25   0   1]
 [  2   0   8   9  27  19 927   3   2   3]
 [  1   0  11  15  23  57   6 881   5   1]
 [  9   9   9   4   1   5   3   8 922  30]
 [  3  20   4   8   0   8  12   2  22 921]]
TRAINING class wise acc@41 :: {0: [0.9752], 1: [0.9856], 2: [0.9776], 3: [0.9722], 4: [0.9834], 5: [0.9708], 6: [0.983], 7: [0.9898], 8: [0.9902], 9: [0.9904]}
TRAIINING epoch@41 ::  acc@0.982  loss@0.059
[[2438    7   16   10    6    0    0    4   17    2]
 [   4 2464    0    2    0    0    1    0    5   24]
 [  16    0 4888   12   17   14   34   11    8    0]
 [  10    1    9 4861   14   65   22    6    0   12]
 [   4    0   23   16 4917   20    7   13    0    0]
 [   1    0   14   78   25 4854    7   20    1    0]
 [   2    0   31   25    9   11 4915    0    2    5]
 [   3    0    5   10   14   17    0 4949    1    1]
 [  18    5    9    0    0    0    3    3 4951   11]
 [   1   18    1   11    0    3    4    0   10 4952]]
TESTING class wise acc@41 :: {0: [0.857], 1: [0.863], 2: [0.772], 3: [0.692], 4: [0.895], 5: [0.319], 6: [0.766], 7: [0.893], 8: [0.913], 9: [0.852]}
TESTING epoch@41 ::  acc@0.782  loss@1.188
Epoch@@42
[[857   0  54  18  10   0   0   4  54   3]
 [ 49 863   4   4   2   0   0   2  34  42]
 [ 45   0 772  32  93   2  11  24  19   2]
 [ 34   4  50 692  89  15  22  59  25  10]
 [  7   0  23  28 895   1   6  33   7   0]
 [ 10   1  44 211 198 319   5 195  10   7]
 [ 14   2  61  61  68   4 766   4   8  12]
 [ 24   0  11  16  46   0   1 893   7   2]
 [ 54  13   1   3   2   0   0   2 913  12]
 [ 44  48   3   6   2   0   0   3  42 852]]
TRAINING class wise acc@42 :: {0: [0.9808], 1: [0.99], 2: [0.983], 3: [0.9762], 4: [0.985], 5: [0.981], 6: [0.9862], 7: [0.9896], 8: [0.9902], 9: [0.9892]}
TRAIINING epoch@42 ::  acc@0.985  loss@0.051
[[2452    3   11    8    1    1    0    4   18    2]
 [   4 2475    0    0    0    0    0    0    5   16]
 [  17    0 4915    9   22    6   18    5    6    2]
 [   5    1    8 4881   13   48   22   13    2    7]
 [   6    0   22   17 4925   14    3   13    0    0]
 [   0    0    8   55   10 4905    6   14    0    2]
 [   1    0   23   22    6    8 4931    0    2    7]
 [   4    0    7    8   16   10    0 4948    7    0]
 [  13    4    9    3    1    1    1    2 4951   15]
 [   5   19    1    9    0    2    0    1   17 4946]]
TESTING class wise acc@42 :: {0: [0.669], 1: [0.85], 2: [0.749], 3: [0.823], 4: [0.767], 5: [0.711], 6: [0.91], 7: [0.838], 8: [0.863], 9: [0.932]}
TESTING epoch@42 ::  acc@0.811  loss@1.120
Epoch@@43
[[669  10  94 115  10   2  15   7  54  24]
 [  2 850   3  25   0   0  11   0  14  95]
 [ 13   1 749  86  35  27  59  14   8   8]
 [  2   1  39 823  21  44  54   5   2   9]
 [  4   1  61  79 767  17  36  31   4   0]
 [  0   0  23 220  13 711  18  11   0   4]
 [  1   0  18  51  10   6 910   2   1   1]
 [  2   0  12  72  21  42   5 838   5   3]
 [ 15   7  19  22   0   3  11   2 863  58]
 [  7  18   1  19   1   0  11   1  10 932]]
TRAINING class wise acc@43 :: {0: [0.9728], 1: [0.9904], 2: [0.9758], 3: [0.973], 4: [0.9828], 5: [0.975], 6: [0.9878], 7: [0.9872], 8: [0.99], 9: [0.9912]}
TRAIINING epoch@43 ::  acc@0.983  loss@0.057
[[2432    6   22   13    4    0    0    5   17    1]
 [   5 2476    0    2    0    0    1    1    1   14]
 [  24    0 4879   18   26   16   21    6    8    2]
 [   7    1   21 4865   11   63   17    8    2    5]
 [   6    0   35   17 4914    8    8   12    0    0]
 [   0    0   11   66   12 4875    8   23    0    5]
 [   0    2   15   21    7   11 4939    0    2    3]
 [   2    0    5   15   15   22    0 4936    5    0]
 [  16    3    7    2    0    0    1    2 4950   19]
 [   2   17    2    9    0    1    1    0   12 4956]]
TESTING class wise acc@43 :: {0: [0.792], 1: [0.894], 2: [0.758], 3: [0.765], 4: [0.809], 5: [0.482], 6: [0.948], 7: [0.845], 8: [0.942], 9: [0.924]}
TESTING epoch@43 ::  acc@0.816  loss@1.053
Epoch@@44
[[792  16  46  33  10   1   2   2  67  31]
 [  7 894   2   5   0   0   4   0  18  70]
 [ 43   0 758  53  47   4  68  13  12   2]
 [ 14   4  42 765  34  11  84  18  13  15]
 [ 11   1  42  56 809  10  44  20   5   2]
 [  4   2  55 330  22 482  68  27   4   6]
 [  3   0  12  24   6   1 948   0   4   2]
 [ 15   0  22  63  25   8   6 845  10   6]
 [ 24   9   2   3   0   0   2   2 942  16]
 [  4  33   2   8   0   0   5   0  24 924]]
TRAINING class wise acc@44 :: {0: [0.9796], 1: [0.9856], 2: [0.9808], 3: [0.9774], 4: [0.9848], 5: [0.9776], 6: [0.9856], 7: [0.9908], 8: [0.9904], 9: [0.9892]}
TRAIINING epoch@44 ::  acc@0.984  loss@0.053
[[2449    5   20    6    1    0    0    2   16    1]
 [   3 2464    0    1    0    0    0    0    8   24]
 [  16    0 4904   12   25   10   20    6    5    2]
 [   4    1    7 4887    8   48   27    7    4    7]
 [   3    0   27   12 4924   10   10   14    0    0]
 [   0    0   10   62   15 4888   10   13    0    2]
 [   0    0   25   25    7   11 4928    0    0    4]
 [   1    0    5   10   11   12    0 4954    5    2]
 [  15    9    3    4    0    1    0    1 4952   15]
 [   1   22    4   14    1    0    0    3    9 4946]]
TESTING class wise acc@44 :: {0: [0.77], 1: [0.858], 2: [0.709], 3: [0.738], 4: [0.882], 5: [0.733], 6: [0.911], 7: [0.835], 8: [0.887], 9: [0.942]}
TESTING epoch@44 ::  acc@0.826  loss@0.892
Epoch@@45
[[770  11  39  62  18   2   5   6  54  33]
 [  9 858   1  12   1   1   2   2  20  94]
 [ 44   0 709  43 105  24  53  18   4   0]
 [  7   1  31 738  61  81  59  11   2   9]
 [  6   1  11  41 882   8  23  26   2   0]
 [  0   1  34 134  51 733  28  17   0   2]
 [  4   0  16  31  23   7 911   2   4   2]
 [  5   0  16  38  60  38   3 835   2   3]
 [ 34  13   9  15   0   2   2   3 887  35]
 [  3  20   0  12   1   3   6   1  12 942]]
TRAINING class wise acc@45 :: {0: [0.9792], 1: [0.99], 2: [0.9822], 3: [0.9764], 4: [0.987], 5: [0.9798], 6: [0.9918], 7: [0.9892], 8: [0.9932], 9: [0.9924]}
TRAIINING epoch@45 ::  acc@0.986  loss@0.045
[[2448    4   16    5    4    0    0    3   19    1]
 [   5 2475    0    0    0    1    0    0    7   12]
 [  18    0 4911   13   25    8   15    5    3    2]
 [   8    0   11 4882   14   50   17   10    0    8]
 [   4    0   23   13 4935    7    5   13    0    0]
 [   0    0   10   56    7 4899    7   21    0    0]
 [   0    1   12   15    4    9 4959    0    0    0]
 [   5    0    9    7   12   20    0 4946    1    0]
 [  14    4    5    1    0    0    0    1 4966    9]
 [   2   15    2    8    0    0    2    1    8 4962]]
TESTING class wise acc@45 :: {0: [0.848], 1: [0.883], 2: [0.768], 3: [0.731], 4: [0.756], 5: [0.796], 6: [0.875], 7: [0.903], 8: [0.92], 9: [0.915]}
TESTING epoch@45 ::  acc@0.839  loss@0.903
Epoch@@46
[[848   8  44  25   5   0   1  12  37  20]
 [ 25 883   0   4   1   0   0   1  13  73]
 [ 38   1 768  49  24  35  33  31   8  13]
 [ 17   4  35 731  20 113  32  27   6  15]
 [ 14   3  71  60 756  21  26  43   6   0]
 [  3   1  25 109  11 796   6  39   0  10]
 [  6   0  44  34   7  19 875   4   2   9]
 [  7   0  13  22  17  30   0 903   4   4]
 [ 37  14   2   4   0   2   1   3 920  17]
 [ 18  31   2  12   0   0   0   3  19 915]]
TRAINING class wise acc@46 :: {0: [0.982], 1: [0.9932], 2: [0.981], 3: [0.976], 4: [0.9872], 5: [0.978], 6: [0.984], 7: [0.9904], 8: [0.9922], 9: [0.9898]}
TRAIINING epoch@46 ::  acc@0.985  loss@0.050
[[2455    6    9    7    2    0    0    1   17    3]
 [   3 2483    0    0    0    0    0    0    3   11]
 [  11    0 4905   14   13   18   26    5    4    4]
 [   5    0   15 4880   14   53   16    7    3    7]
 [   3    0   23   13 4936    6    8   11    0    0]
 [   0    0   18   61    9 4890    9   13    0    0]
 [   0    0   30   27    4    9 4920    0    2    8]
 [   3    0    2   10   12   18    0 4952    2    1]
 [  15    4    5    2    0    0    1    0 4961   12]
 [   4   13    4   10    0    0    5    1   14 4949]]
TESTING class wise acc@46 :: {0: [0.888], 1: [0.934], 2: [0.762], 3: [0.688], 4: [0.781], 5: [0.766], 6: [0.886], 7: [0.9], 8: [0.863], 9: [0.865]}
TESTING epoch@46 ::  acc@0.833  loss@0.925
Epoch@@47
[[888  12  29  24   7   2   2   6  22   8]
 [  8 934   1   4   1   0   3   2   6  41]
 [ 79   0 762  24  36  24  39  28   4   4]
 [ 18   6  59 688  27 119  40  28   5  10]
 [ 15   2  65  42 781  25  25  40   4   1]
 [  6   0  40 109  20 766   7  43   0   9]
 [  8   0  30  31   8  27 886   5   3   2]
 [ 13   0  18  16  17  30   1 900   2   3]
 [ 70  24  13   5   0   4   1   6 863  14]
 [ 24  79   0   8   0   4   3   4  13 865]]
TRAINING class wise acc@47 :: {0: [0.976], 1: [0.988], 2: [0.9788], 3: [0.9738], 4: [0.9824], 5: [0.9764], 6: [0.9846], 7: [0.9886], 8: [0.9874], 9: [0.9898]}
TRAIINING epoch@47 ::  acc@0.983  loss@0.059
[[2440    5   16   13    6    0    1    2   15    2]
 [   2 2470    0    1    0    0    1    0    6   20]
 [  19    0 4894   11   27   12   17    6   11    3]
 [  10    2   12 4869   16   55   23    4    1    8]
 [   5    0   22   20 4912   13    5   23    0    0]
 [   0    0   10   60   14 4882    8   23    1    2]
 [   1    1   24   27    6    7 4923    0    4    7]
 [   2    0    6    9   21   18    0 4943    1    0]
 [  25    5   14    2    0    0    5    3 4937    9]
 [   5   17    0    8    0    3    5    0   13 4949]]
TESTING class wise acc@47 :: {0: [0.8], 1: [0.873], 2: [0.752], 3: [0.743], 4: [0.772], 5: [0.829], 6: [0.916], 7: [0.89], 8: [0.91], 9: [0.892]}
TESTING epoch@47 ::  acc@0.838  loss@0.868
Epoch@@48
[[800   3  52  41  12   5  10   8  59  10]
 [ 22 873   0  10   1   1   3   2  26  62]
 [ 34   2 752  57  31  50  54  14   3   3]
 [ 19   1  29 743  14 127  44  14   5   4]
 [ 10   0  35  53 772  66  33  28   2   1]
 [  5   0  10 108  16 829   9  22   1   0]
 [  4   0  17  34   7  16 916   3   2   1]
 [ 10   0   8  25  20  41   3 890   3   0]
 [ 31   8   4  16   0   5   3   4 910  19]
 [ 10  37   4  21   2   1   6   8  19 892]]
TRAINING class wise acc@48 :: {0: [0.9844], 1: [0.99], 2: [0.9818], 3: [0.9796], 4: [0.9878], 5: [0.9816], 6: [0.9888], 7: [0.9924], 8: [0.9932], 9: [0.9944]}
TRAIINING epoch@48 ::  acc@0.987  loss@0.044
[[2461    3   15    5    2    1    0    3    9    1]
 [   2 2475    0    1    0    0    0    0    6   16]
 [  13    0 4909   15   15    8   25    7    3    5]
 [   6    2   12 4898   15   48   10    6    1    2]
 [   2    0   14   11 4939   13    7   14    0    0]
 [   0    0   13   50    8 4908    8   11    0    2]
 [   1    0   21   20    4    8 4944    0    0    2]
 [   3    0    9    6   11    6    0 4962    3    0]
 [  14    6    5    2    0    0    0    2 4966    5]
 [   0   12    5    4    0    0    0    0    7 4972]]
TESTING class wise acc@48 :: {0: [0.861], 1: [0.924], 2: [0.795], 3: [0.67], 4: [0.83], 5: [0.811], 6: [0.889], 7: [0.833], 8: [0.926], 9: [0.894]}
TESTING epoch@48 ::  acc@0.843  loss@0.865
Epoch@@49
[[861   5  34  25   8   2   1   3  48  13]
 [  9 924   2   5   1   2   0   0  18  39]
 [ 49   1 795  35  38  35  30   7   8   2]
 [ 19   2  51 670  33 143  44  12  11  15]
 [ 14   1  64  35 830  26  14  13   3   0]
 [  5   0  37 102  18 811  12  11   0   4]
 [  7   1  45  28  12  15 889   0   2   1]
 [ 12   0  22  26  44  52   2 833   8   1]
 [ 35  11   6   2   0   4   0   3 926  13]
 [ 10  60   3   8   2   0   1   2  20 894]]
TRAINING class wise acc@49 :: {0: [0.9828], 1: [0.99], 2: [0.9832], 3: [0.9816], 4: [0.986], 5: [0.9818], 6: [0.987], 7: [0.9928], 8: [0.993], 9: [0.9898]}
TRAIINING epoch@49 ::  acc@0.987  loss@0.045
[[2457    2   15    8    1    0    0    4   10    3]
 [   3 2475    0    0    0    0    2    0    4   16]
 [  16    0 4916    6   19   10   22    4    4    3]
 [   5    0    3 4908   16   36   17    8    1    6]
 [   2    0   19   17 4930   14    9    8    1    0]
 [   0    0    9   50   11 4909    7   13    0    1]
 [   0    2   25   19    9    8 4935    0    1    1]
 [   1    0    2   10    9   13    0 4964    0    1]
 [  14    5    2    2    0    0    0    1 4965   11]
 [   1   16    2   14    0    0    3    0   15 4949]]
TESTING class wise acc@49 :: {0: [0.74], 1: [0.938], 2: [0.813], 3: [0.627], 4: [0.815], 5: [0.739], 6: [0.93], 7: [0.816], 8: [0.927], 9: [0.906]}
TESTING epoch@49 ::  acc@0.825  loss@1.103
[[740  25 101  22  13   1  11   1  63  23]
 [  3 938   1   3   1   1   0   0  10  43]
 [ 25   1 813  17  37  21  71   5   6   4]
 [ 16  10  75 627  49  86  85  13  12  27]
 [ 10   3  68  25 815  12  49  12   3   3]
 [  4   3  42  95  38 739  53  14   1  11]
 [  3   4  21  15  13   4 930   2   5   3]
 [ 21   0  37  26  37  43  10 816   4   6]
 [ 24  17   4   1   0   1   2   2 927  22]
 [ 10  53   3   6   0   1   4   1  16 906]]
