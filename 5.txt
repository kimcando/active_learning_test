

Fri Jan 15 16:59:29 2021
imbal_test_imbal123_rand_cifar10_vgg_Adam's set level: 10
imbal_test_imbal123_rand_cifar10_vgg_Adam logger is ready
imbal_test with imbalance_ratio: [0.5 0.5 0.5 1.  1.  1.  1.  1.  1.  1. ]
parallel mode is False
initial count: cifar10: [0 1 2 3 4 5 6 7 8 9] class, each has [5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]
cifar10: [0 1 2 3 4 5 6 7 8 9] class, each has [2500 2500 2500 5000 5000 5000 5000 5000 5000 5000]
Epoch@@0
Files already downloaded and verified
TRAINING class wise acc@0 :: {0: [0.0212], 1: [0.002], 2: [0.0004], 3: [0.1654], 4: [0.1588], 5: [0.0846], 6: [0.2586], 7: [0.182], 8: [0.124], 9: [0.111]}
TRAIINING epoch@0 ::  acc@0.129  loss@2.468
[[  53    5   14  369  340  198  507  394  375  245]
 [   4    5    0  433  376  199  645  421  150  267]
 [  23    1    1  401  363  204  640  438  172  257]
 [  14   10    0  827  812  444 1218  898  242  535]
 [  11    6    1  817  794  396 1241  928  244  562]
 [  11    7    1  841  747  423 1291  871  245  563]
 [   7    0    0  873  772  425 1293  841  241  548]
 [   5    6    2  782  762  423 1310  910  242  558]
 [  76    2    9  731  726  400 1113  788  620  535]
 [  16    6    1  804  788  425 1241  877  287  555]]
TESTING class wise acc@0 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.986], 5: [0.0], 6: [0.0], 7: [0.0], 8: [0.133], 9: [0.0]}
TESTING epoch@0 ::  acc@0.112  loss@2.330
Epoch@@1
[[  0   0   0   0 835   0   0   0 165   0]
 [  0   0   0   0 989   0   0   0  11   0]
 [  0   0   0   0 972   0   0   0  28   0]
 [  0   0   0   0 999   0   0   0   1   0]
 [  0   0   0   0 986   0   0   0  14   0]
 [  0   0   0   0 998   0   0   0   2   0]
 [  0   0   0   0 999   0   0   0   1   0]
 [  0   0   0   0 998   0   0   0   2   0]
 [  0   0   0   0 867   0   0   0 133   0]
 [  0   0   0   0 992   0   0   0   8   0]]
TRAINING class wise acc@1 :: {0: [0.0064], 1: [0.0], 2: [0.0], 3: [0.1262], 4: [0.0574], 5: [0.1358], 6: [0.405], 7: [0.2216], 8: [0.5874], 9: [0.2568]}
TRAIINING epoch@1 ::  acc@0.211  loss@2.024
[[  16    0    0   98   95  105  157  272 1234  523]
 [   3    0    0  119  149  164  202  429  809  625]
 [   1    0    0  291  151  342  843  439  176  257]
 [   1    0    0  631  343  707 1770  975  142  431]
 [   0    0    0  579  287  655 1864  923  225  467]
 [   0    0    0  623  322  679 1947  963  103  363]
 [   0    0    0  648  323  758 2025  880   61  305]
 [   0    0    0  647  361  668 1481 1108  180  555]
 [  43    0    0   87  158  165  189  467 2937  954]
 [   5    0    0  266  248  385  465  872 1475 1284]]
TESTING class wise acc@1 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.375], 5: [0.0], 6: [0.356], 7: [0.0], 8: [0.622], 9: [0.256]}
TESTING epoch@1 ::  acc@0.161  loss@2.135
Epoch@@2
[[  0   0   0   0 308   0 153   0 357 182]
 [  0   0   0   0  95   0   5   0 694 206]
 [  0   0   0   0 349   0 448   0  64 139]
 [  0   0   0   0 389   0 407   0  47 157]
 [  0   0   0   0 375   0 415   1  70 139]
 [  0   0   0   0 352   0 542   0  24  82]
 [  0   0   0   0 431   0 356   0  46 167]
 [  0   0   0   0 489   0 274   0  70 167]
 [  0   0   0   0 132   0  25   0 622 221]
 [  0   0   0   0 117   0  11   0 616 256]]
TRAINING class wise acc@2 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0898], 4: [0.0998], 5: [0.2328], 6: [0.411], 7: [0.2102], 8: [0.6596], 9: [0.51]}
TRAIINING epoch@2 ::  acc@0.260  loss@1.856
[[   0    0    0   66   51   39   80  239 1036  989]
 [   0    0    0   52   28    9   28  170 1140 1073]
 [   0    0    0  218  256  429  816  348   58  375]
 [   0    0    0  449  572  998 1665  786   44  486]
 [   0    0    0  355  499 1097 1861  632   79  477]
 [   0    0    0  387  564 1164 1918  629   18  320]
 [   0    0    0  372  554 1168 2055  555   24  272]
 [   0    0    0  578  591  751 1362 1051   53  614]
 [   0    0    0   70   41   23   34  173 3298 1361]
 [   0    0    0  148   82   43   69  455 1653 2550]]
TESTING class wise acc@2 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.0], 6: [0.847], 7: [0.21], 8: [0.143], 9: [0.649]}
TESTING epoch@2 ::  acc@0.185  loss@2.198
Epoch@@3
[[  0   0   0   0   0   0 308 230  62 400]
 [  0   0   0   0   0   0  86 188  64 662]
 [  0   0   0   0   0   0 819 120   0  61]
 [  0   0   0   0   0   0 818 136   0  46]
 [  0   0   0   0   0   0 827 123   0  50]
 [  0   0   0   0   0   0 914  70   1  15]
 [  0   0   0   0   0   0 847 108   0  45]
 [  0   0   0   0   0   0 720 210   1  69]
 [  0   0   0   0   0   0 203 203 143 451]
 [  0   0   0   0   0   0 117 213  21 649]]
TRAINING class wise acc@3 :: {0: [0.0], 1: [0.0004], 2: [0.0], 3: [0.0798], 4: [0.0576], 5: [0.3], 6: [0.3782], 7: [0.3078], 8: [0.6046], 9: [0.6456]}
TRAIINING epoch@3 ::  acc@0.279  loss@1.816
[[   0    1    0   54   22   39   38  302  911 1133]
 [   0    1    0   24   14   10   16  179  730 1526]
 [   0    0    0  186  137  597  670  533   38  339]
 [   0    0    0  399  300 1390 1510  942   17  442]
 [   0    0    0  304  288 1347 1703  830   31  497]
 [   0    0    0  405  303 1500 1792  747   11  242]
 [   0    0    0  383  248 1537 1891  681   13  247]
 [   0    0    0  599  344  910  926 1539   26  656]
 [   0    1    0   37   20   24   21  243 3023 1631]
 [   0    1    0   74   34   37   39  562 1025 3228]]
TESTING class wise acc@3 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.288], 5: [0.0], 6: [0.446], 7: [0.517], 8: [0.555], 9: [0.848]}
TESTING epoch@3 ::  acc@0.265  loss@1.843
Epoch@@4
[[  0   0   0   0  41   0   7 176 273 503]
 [  0   0   0   0   5   0   0  47 123 825]
 [  0   0   0   0 264   0 351 271   6 108]
 [  0   0   0   0 274   0 330 304   1  91]
 [  0   0   0   0 288   0 398 247   3  64]
 [  0   0   0   0 282   0 425 247   0  46]
 [  0   0   0   0 269   0 446 223   0  62]
 [  0   0   0   0 214   0  94 517   1 174]
 [  0   0   0   0  12   0   2  52 555 379]
 [  0   0   0   0   7   0   1  70  74 848]]
TRAINING class wise acc@4 :: {0: [0.0328], 1: [0.008], 2: [0.0224], 3: [0.0982], 4: [0.0702], 5: [0.1688], 6: [0.6298], 7: [0.4626], 8: [0.6732], 9: [0.7706]}
TRAIINING epoch@4 ::  acc@0.342  loss@1.701
[[  82    4   35   47   54   55   37  299 1115  772]
 [  14   20    0   13    8   12   10  165  401 1857]
 [  24    0   56  252  245  297  822  535   81  188]
 [  19    1   39  491  378  707 1925 1136   33  271]
 [  27    0   46  423  351  660 2118 1068   49  258]
 [   7    0   37  498  449  844 2003 1010   16  136]
 [   8    0   12  331  282  499 3149  583   11  125]
 [  11    0    9  496  342  525  787 2313   18  499]
 [  89   10   17   36   34   36   15  233 3366 1164]
 [  26   22    2   31   25   28   16  529  468 3853]]
TESTING class wise acc@4 :: {0: [0.0], 1: [0.0], 2: [0.001], 3: [0.0], 4: [0.65], 5: [0.0], 6: [0.0], 7: [0.338], 8: [0.819], 9: [0.847]}
TESTING epoch@4 ::  acc@0.265  loss@2.010
Epoch@@5
[[  0   0   0   0 167   0   0  14 668 151]
 [  0   0   0   0  47   0   0  16 150 787]
 [  0   0   1   0 725   0   0 104 114  56]
 [  0   0   0   0 676   0   0 167  39 118]
 [  0   0   0   0 650   0   0 229  33  88]
 [  0   0   0   0 720   0   0 204  18  58]
 [  0   0   0   0 436   0   0 277  31 256]
 [  0   0   0   0 291   0   0 338   8 363]
 [  0   0   0   0  72   0   0   2 819 107]
 [  0   0   0   0  36   0   0  19  98 847]]
TRAINING class wise acc@5 :: {0: [0.1048], 1: [0.136], 2: [0.1292], 3: [0.1104], 4: [0.2194], 5: [0.2932], 6: [0.657], 7: [0.607], 8: [0.7912], 9: [0.7558]}
TRAIINING epoch@5 ::  acc@0.426  loss@1.490
[[ 262   39  161  115  161   54   30   91 1355  232]
 [  26  340    4   36   24   12    7  100  388 1563]
 [  85    0  323  236  660  298  504  176  189   29]
 [  76    5  102  552  921 1175  918 1036  121   94]
 [  76    2  205  514 1097  887 1260  775  119   65]
 [  30    0   75  508  729 1466  672 1421   44   55]
 [  23    2  134  206  579  433 3285  264   48   26]
 [  30    3   27  355  245  722  264 3035   51  268]
 [ 279   97   96   82  113   22   16   67 3956  272]
 [  37  265    7   63   24   43    8  458  316 3779]]
TESTING class wise acc@5 :: {0: [0.668], 1: [0.0], 2: [0.328], 3: [0.0], 4: [0.0], 5: [0.447], 6: [0.736], 7: [0.808], 8: [0.0], 9: [0.87]}
TESTING epoch@5 ::  acc@0.386  loss@1.967
Epoch@@6
[[668   0 129   0   0  14  12  66   0 111]
 [135   0   8   0   0   2   2  37   0 816]
 [ 97   0 328   0   0 160 284 123   0   8]
 [ 36   0 141   0   0 307 245 263   0   8]
 [ 43   0 208   0   0 223 304 219   0   3]
 [ 16   0  89   0   0 447 121 325   0   2]
 [ 17   0  64   0   0 104 736  79   0   0]
 [ 12   0  27   0   0  97  25 808   0  31]
 [857   0  32   0   0   2   1  24   0  84]
 [ 44   0   4   0   0   2   1  79   0 870]]
TRAINING class wise acc@6 :: {0: [0.1268], 1: [0.3464], 2: [0.0548], 3: [0.1408], 4: [0.4546], 5: [0.6054], 6: [0.7062], 7: [0.6954], 8: [0.7884], 9: [0.7566]}
TRAIINING epoch@6 ::  acc@0.519  loss@1.298
[[ 317   59   67   35  445   36   26   77 1218  220]
 [  91  866    2    6   65    5    8   34  326 1097]
 [  97    1  137  230 1087  304  351  137  130   26]
 [  66    4   57  704 1066 1917  589  498   56   43]
 [ 105    3  111  604 2273  625  573  582   80   44]
 [  22    1   30  527  542 3027  216  601   18   16]
 [  28    1   77  310  629  309 3531   47   48   20]
 [  22    1    9  182  413  725   34 3477   14  123]
 [ 347  187   45    9  253   10   22   33 3942  152]
 [  83  396    1   22  141   22    7  342  203 3783]]
TESTING class wise acc@6 :: {0: [0.118], 1: [0.751], 2: [0.0], 3: [0.005], 4: [0.564], 5: [0.319], 6: [0.088], 7: [0.865], 8: [0.729], 9: [0.825]}
TESTING epoch@6 ::  acc@0.426  loss@1.802
Epoch@@7
[[118  52   0   0 245   0   0  55 355 175]
 [  4 751   0   0   7   0   0   2  30 206]
 [ 33   1   0   0 599  25   1 234  35  72]
 [ 11   8   0   5 317 151   1 388  13 106]
 [ 11   3   0   0 564   6   1 335  12  68]
 [  6   1   0   3 153 319   0 473   6  39]
 [ 31   3   0   4 662  10  88 105  25  72]
 [  3   1   0   0  42  13   0 865   1  75]
 [ 19 156   0   0  23   1   0  11 729  61]
 [  3 128   0   0   5   0   0  27  12 825]]
TRAINING class wise acc@7 :: {0: [0.2376], 1: [0.5448], 2: [0.0296], 3: [0.2024], 4: [0.6288], 5: [0.6256], 6: [0.759], 7: [0.7462], 8: [0.8102], 9: [0.791]}
TRAIINING epoch@7 ::  acc@0.585  loss@1.150
[[ 594   43   31   28  552   12   14   83  966  177]
 [ 100 1362    4    3   51    2   10   18  273  677]
 [ 112    0   74  219 1359  176  313  127  108   12]
 [  71    3   30 1012 1157 1771  540  354   27   35]
 [ 117    3   64  334 3144  257  384  611   54   32]
 [  13    1   15  584  637 3128  155  445    9   13]
 [  28    3   52  381  530  130 3795   29   40   12]
 [  32    0    4  112  535  473   19 3731    6   88]
 [ 338  169   17   11  241    5   35   20 4051  113]
 [ 127  378    2    5  156   15    7  257   98 3955]]
TESTING class wise acc@7 :: {0: [0.353], 1: [0.76], 2: [0.0], 3: [0.106], 4: [0.529], 5: [0.409], 6: [0.849], 7: [0.755], 8: [0.897], 9: [0.77]}
TESTING epoch@7 ::  acc@0.543  loss@1.373
Epoch@@8
[[353  15   0   0  28   0   8   6 543  47]
 [ 30 760   0   0   4   0   2   0  92 112]
 [196   1   0  13 395  21 172  49 151   2]
 [128   1   0 106 296 137 184  80  59   9]
 [165   0   0   9 529   4 154 100  37   2]
 [ 53   0   0  69 226 409  67 158  15   3]
 [ 25   0   0   9  55   5 849   0  57   0]
 [ 76   1   0   9 100  13   4 755  12  30]
 [ 55  31   0   1   6   0   0   3 897   7]
 [ 53 104   0   0  10   0   0  28  35 770]]
TRAINING class wise acc@8 :: {0: [0.4764], 1: [0.696], 2: [0.0088], 3: [0.3018], 4: [0.7282], 5: [0.6352], 6: [0.7868], 7: [0.785], 8: [0.8358], 9: [0.8376]}
TRAIINING epoch@8 ::  acc@0.647  loss@1.003
[[1191   29   11   32  426    9   12   77  553  160]
 [  90 1740    1   13   16    2   13   13  170  442]
 [ 224    0   22  285 1338  161  301  104   53   12]
 [  92    4   10 1509  869 1709  474  270   20   43]
 [ 160    0   32  267 3641  161  263  447   18   11]
 [  25    1    4  821  476 3176  101  387    2    7]
 [  30   11   25  432  409   88 3934   19   44    8]
 [  57    1    1  133  468  340   12 3925    3   60]
 [ 391  116    9   26  131    3   42   18 4179   85]
 [ 174  263    1   32   64    7   14  181   76 4188]]
TESTING class wise acc@8 :: {0: [0.559], 1: [0.771], 2: [0.074], 3: [0.059], 4: [0.44], 5: [0.845], 6: [0.571], 7: [0.79], 8: [0.807], 9: [0.735]}
TESTING epoch@8 ::  acc@0.565  loss@1.263
Epoch@@9
[[559  17  49   8  50  12   6  44 222  33]
 [ 10 771   5  10   0   5   8   1  99  91]
 [125   0  74  24 246 263 111 143  14   0]
 [  8   1  59  59  63 664  55  80  10   1]
 [ 23   0  30   9 440 319  52 121   6   0]
 [  3   0  17  14  32 845   3  82   2   2]
 [  1   0  22  45  41 299 571  15   6   0]
 [ 13   0   4   1  32 153   0 790   2   5]
 [ 70  31  36   7  12   5  16   6 807  10]
 [ 25  92  16  13   0  15   6  72  26 735]]
TRAINING class wise acc@9 :: {0: [0.6208], 1: [0.7236], 2: [0.1004], 3: [0.438], 4: [0.754], 5: [0.6412], 6: [0.7976], 7: [0.7986], 8: [0.842], 9: [0.8522]}
TRAIINING epoch@9 ::  acc@0.688  loss@0.894
[[1552   22   78   67  198    6   19   76  335  147]
 [  54 1809   15   25    1    4   18    7  194  373]
 [ 242    2  251  448  905  133  317  124   65   13]
 [  61    6  127 2190  473 1464  380  219   33   47]
 [ 133    0  111  281 3770  133  193  345   23   11]
 [  11    0   37 1049  291 3206   77  307   11   11]
 [  24   15   71  485  249   78 3988   23   53   14]
 [  51    1   30  233  299  285   22 3993    6   80]
 [ 332  138   68   55   35    3   65   13 4210   81]
 [ 162  229   24   61   17    8   15  141   82 4261]]
TESTING class wise acc@9 :: {0: [0.702], 1: [0.635], 2: [0.036], 3: [0.542], 4: [0.829], 5: [0.513], 6: [0.711], 7: [0.89], 8: [0.71], 9: [0.815]}
TESTING epoch@9 ::  acc@0.638  loss@1.016
Epoch@@10
[[702   2   8  19 125   0   2  64  40  38]
 [ 18 635   5  35   4   0  14  11  79 199]
 [117   0  36 127 529  31  65  85   8   2]
 [ 10   0  20 542 179 122  34  85   6   2]
 [ 24   0   3  26 829   5  27  85   1   0]
 [  6   0   2 226 112 513   4 135   0   2]
 [  6   0  21 115 128   4 711   6   9   0]
 [  9   0   1  23  65  11   0 890   1   0]
 [180   4  15  12  22   0   1  12 710  44]
 [ 26  15   2  18   8   0   0 105  11 815]]
TRAINING class wise acc@10 :: {0: [0.6984], 1: [0.7968], 2: [0.21], 3: [0.5588], 4: [0.78], 5: [0.6544], 6: [0.8386], 7: [0.8224], 8: [0.8776], 9: [0.8786]}
TRAIINING epoch@10 ::  acc@0.737  loss@0.771
[[1746   11  120   67  110    1    9   64  251  121]
 [  29 1992    7   32    0    1   12    5  138  284]
 [ 257    0  525  476  707  110  267   95   57    6]
 [  37    6  170 2794  321 1109  284  184   31   64]
 [  98    0  184  248 3900  126  167  261   11    5]
 [  14    2   42 1078  247 3272   61  268    5   11]
 [  17    5  105  412  185   43 4193    8   24    8]
 [  42    0   43  233  278  228    8 4112    5   51]
 [ 259   79   86   49   10    1   33    7 4388   88]
 [ 113  219   19   86    7    3    8   82   70 4393]]
TESTING class wise acc@10 :: {0: [0.316], 1: [0.763], 2: [0.261], 3: [0.433], 4: [0.74], 5: [0.757], 6: [0.682], 7: [0.883], 8: [0.636], 9: [0.72]}
TESTING epoch@10 ::  acc@0.619  loss@1.245
Epoch@@11
[[316  17 168 220  44  15   2  91  33  94]
 [  0 763   0 139   1   9   0  10   1  77]
 [ 12   0 261 282 182 110  45 105   1   2]
 [  0   0   8 433  53 410  13  81   1   1]
 [  0   0  25  79 740  54  14  88   0   0]
 [  0   0   3  87  51 757   6  96   0   0]
 [  0   1  20 159  89  39 682  10   0   0]
 [  1   0   1  19  42  54   0 883   0   0]
 [ 17  68  78 126   7   7  18   7 636  36]
 [  1  38   5 138   4   9   1  82   2 720]]
TRAINING class wise acc@11 :: {0: [0.7108], 1: [0.828], 2: [0.3188], 3: [0.6174], 4: [0.8], 5: [0.6688], 6: [0.8422], 7: [0.8418], 8: [0.8892], 9: [0.887]}
TRAIINING epoch@11 ::  acc@0.762  loss@0.700
[[1777    8  178   57   88    3    5   44  227  113]
 [  19 2070    9   42    0    3    6    5  113  233]
 [ 246    1  797  408  569   99  236   89   46    9]
 [  41    4  210 3087  246  909  261  158   36   48]
 [  81    0  199  221 4000  132  127  224    9    7]
 [   2    0   65 1015  223 3344   57  282    3    9]
 [  17    6  130  370  145   51 4211   10   52    8]
 [  39    0   50  223  232  187    7 4209    2   51]
 [ 223   69   99   54   10    2   34    6 4446   57]
 [ 128  149   21  109    6    6    5   70   71 4435]]
TESTING class wise acc@11 :: {0: [0.267], 1: [0.673], 2: [0.021], 3: [0.654], 4: [0.212], 5: [0.795], 6: [0.739], 7: [0.704], 8: [0.704], 9: [0.896]}
TESTING epoch@11 ::  acc@0.567  loss@1.583
Epoch@@12
[[267   3  80 299   1   3  28  18 165 136]
 [  1 673   0 119   0   7  21   0   7 172]
 [  8   0  21 552   6 164 215  20   9   5]
 [  0   0   1 654   1 286  44   8   0   6]
 [  0   0  21 345 212 202 100 114   2   4]
 [  0   0   0 177   1 795  12  15   0   0]
 [  0   0   0 139   0 122 739   0   0   0]
 [  1   0   0 129   0 139   2 704   0  25]
 [  0  13   0  85   0   3 159   2 704  34]
 [  0  14   0  71   0   1   4   1  13 896]]
TRAINING class wise acc@12 :: {0: [0.7372], 1: [0.8316], 2: [0.3948], 3: [0.6594], 4: [0.8058], 5: [0.7038], 6: [0.8552], 7: [0.8578], 8: [0.8924], 9: [0.8988]}
TRAIINING epoch@12 ::  acc@0.783  loss@0.665
[[1843    4  175   41   72    4    3   52  205  101]
 [  12 2079   13   43    0    2   10    2   87  252]
 [ 198    0  987  396  464   93  216   87   49   10]
 [  27   13  235 3297  213  780  238  142   20   35]
 [  69    1  272  178 4029  125  115  200    6    5]
 [  10    1   50  933  200 3519   56  224    1    6]
 [  11    4  169  300  134   55 4276    9   34    8]
 [  39    2   63  185  228  155    1 4289    0   38]
 [ 228   55   86   33    6    1   44    4 4462   81]
 [  92  158   28   79    7    2    8   57   75 4494]]
TESTING class wise acc@12 :: {0: [0.877], 1: [0.859], 2: [0.274], 3: [0.515], 4: [0.529], 5: [0.794], 6: [0.739], 7: [0.794], 8: [0.904], 9: [0.75]}
TESTING epoch@12 ::  acc@0.703  loss@0.987
Epoch@@13
[[877   9   5  12   1   2   1  10  72  11]
 [ 13 859   0   3   0   0   1   0 107  17]
 [350   2 274  93  53 115  33  58  20   2]
 [ 58   4  69 515  16 253  25  29  18  13]
 [140   0  86  36 529  68  21 115   5   0]
 [ 20   4  18 105  11 794   6  40   1   1]
 [ 21   2  88  47   7  30 739   5  61   0]
 [ 56   1  12  22   6  91   1 794   7  10]
 [ 85   3   2   1   0   0   1   1 904   3]
 [ 87  96   2   6   1   0   0   4  54 750]]
TRAINING class wise acc@13 :: {0: [0.7584], 1: [0.8592], 2: [0.4704], 3: [0.6692], 4: [0.8424], 5: [0.7336], 6: [0.8714], 7: [0.8734], 8: [0.896], 9: [0.909]}
TRAIINING epoch@13 ::  acc@0.805  loss@0.593
[[1896    5  184   33   65    3    2   51  189   72]
 [  10 2148    8   40    0    0   11    1   87  195]
 [ 190    0 1176  284  442  106  192   60   40   10]
 [  35    4  252 3346  178  797  197  131   15   45]
 [  49    0  229  133 4212  111   87  166   12    1]
 [   7    0   61  830  168 3668   55  201    2    8]
 [   5    8  201  227  103   57 4357    4   36    2]
 [  37    1   50  156  169  185    1 4367    1   33]
 [ 205   63  126   32    4    0   25    4 4480   61]
 [  81  147   44   70    6    5    2   50   50 4545]]
TESTING class wise acc@13 :: {0: [0.718], 1: [0.925], 2: [0.416], 3: [0.465], 4: [0.86], 5: [0.28], 6: [0.914], 7: [0.117], 8: [0.81], 9: [0.651]}
TESTING epoch@13 ::  acc@0.616  loss@1.340
Epoch@@14
[[718  17  62   8  49   0   9   0 125  12]
 [  3 925   2   5   6   0   7   0  44   8]
 [ 61  11 416  57 213   6 118   4 106   8]
 [ 12  29 126 465 146   6 187   2  21   6]
 [ 17   3  47  10 860   2  53   0   8   0]
 [  7   4  79 326 200 280  72   8  18   6]
 [  2   3  23  10  29   0 914   0  19   0]
 [ 13   4  48  65 724   5   9 117   3  12]
 [ 60  67  17   3  25   0   9   0 810   9]
 [ 29 219   9  42  19   0   4   3  24 651]]
TRAINING class wise acc@14 :: {0: [0.612], 1: [0.6884], 2: [0.2048], 3: [0.5292], 4: [0.688], 5: [0.6242], 6: [0.7968], 7: [0.6788], 8: [0.829], 9: [0.808]}
TRAIINING epoch@14 ::  acc@0.671  loss@0.960
[[1530   12  120   84  149   10   16   60  356  163]
 [  23 1721    6   92   10    4   37    7  172  428]
 [ 240    3  512  394  670  160  294  141   67   19]
 [  44   33  153 2646  344  958  383  253   51  135]
 [ 104    2  164  222 3440  214  230  572   23   29]
 [  16    4   40  954  284 3121  128  385   20   48]
 [  13   17  101  418  250  113 3984   28   45   31]
 [  62    2   51  331  621  428   31 3394    8   72]
 [ 286   89   78   84   39    9   60   18 4145  192]
 [ 131  240   20  243   19   23   19   90  175 4040]]
TESTING class wise acc@14 :: {0: [0.546], 1: [0.893], 2: [0.494], 3: [0.516], 4: [0.724], 5: [0.735], 6: [0.727], 7: [0.898], 8: [0.907], 9: [0.834]}
TESTING epoch@14 ::  acc@0.727  loss@0.847
Epoch@@15
[[546  15  99  18  40   1   1  32 217  31]
 [  2 893   5   6   0   1   0   0  46  47]
 [ 27   3 494  63 152  99  57  74  26   5]
 [  3  13  85 516  31 226  25  79   9  13]
 [  4   2  74  31 724  46  21  89   8   1]
 [  5   3  29 104  18 735   6  97   2   1]
 [  2  13  80  96  23  35 727   9  14   1]
 [  6   0  20  16  27  30   0 898   1   2]
 [ 18  19  21  15   2   0   0   3 907  15]
 [ 10  89   7  11   1   1   0  24  23 834]]
TRAINING class wise acc@15 :: {0: [0.7456], 1: [0.838], 2: [0.47], 3: [0.6708], 4: [0.807], 5: [0.7206], 6: [0.8568], 7: [0.857], 8: [0.8964], 9: [0.9024]}
TRAIINING epoch@15 ::  acc@0.793  loss@0.620
[[1864    3  187   48   56    3    2   47  194   96]
 [  13 2095   14   39    1    1   18    2   98  219]
 [ 199    1 1175  279  430   86  212   70   35   13]
 [  35   10  219 3354  185  739  234  159   18   47]
 [  64    0  260  142 4035  115  121  249   11    3]
 [   7    1   79  845  157 3603   65  229    7    7]
 [   5   18  178  301  109   62 4284    9   29    5]
 [  39    1   61  166  224  180    3 4285    3   38]
 [ 206   65  104   24    7    1   20    6 4482   85]
 [  88  123   30  115    3    0    5   49   75 4512]]
TESTING class wise acc@15 :: {0: [0.702], 1: [0.961], 2: [0.342], 3: [0.528], 4: [0.844], 5: [0.81], 6: [0.931], 7: [0.621], 8: [0.827], 9: [0.678]}
TESTING epoch@15 ::  acc@0.724  loss@0.937
Epoch@@16
[[702  38  68  30  42   1  10   3  88  18]
 [  3 961   1  13   0   0  10   0   6   6]
 [ 56   9 342 105 218  82 172   4   9   3]
 [  4  16  29 528  56 262 101   1   1   2]
 [  8   5  27  22 844  19  59   9   6   1]
 [  1   2  21  83  54 810  18   9   2   0]
 [  3   1   8  22  15  18 931   0   2   0]
 [ 13   1  26  92  94 140   2 621   3   8]
 [ 29 106   7   9   3   1  14   0 827   4]
 [ 13 248   4  36   0   0   6   0  15 678]]
TRAINING class wise acc@16 :: {0: [0.7904], 1: [0.8732], 2: [0.5576], 3: [0.719], 4: [0.8524], 5: [0.7528], 6: [0.8892], 7: [0.884], 8: [0.9206], 9: [0.9182]}
TRAIINING epoch@16 ::  acc@0.829  loss@0.517
[[1976    2  178   33   44    3    3   34  141   86]
 [  11 2183    7   41    0    0   10    2   66  180]
 [ 175    1 1394  270  322   74  168   54   31   11]
 [  25   10  245 3595  136  636  175  123   16   39]
 [  41    0  222  103 4262  108   94  158    8    4]
 [   6    0   60  778  148 3764   42  190    1   11]
 [   2   10  157  200  103   48 4446    4   27    3]
 [  39    0   60  140  155  155    0 4420    2   29]
 [ 141   67   97   28    6    1   17    2 4603   38]
 [  73  143   22   89    4    1    1   32   44 4591]]
TESTING class wise acc@16 :: {0: [0.656], 1: [0.768], 2: [0.553], 3: [0.587], 4: [0.827], 5: [0.713], 6: [0.819], 7: [0.923], 8: [0.906], 9: [0.857]}
TESTING epoch@16 ::  acc@0.761  loss@0.772
Epoch@@17
[[656   2  85  21  21   2   2  78 100  33]
 [  6 768   5  23   1   1   4   4  47 141]
 [ 42   0 553  89 147  43  39  68  15   4]
 [  6   2  62 587  61 148  33  89   7   5]
 [  5   0  41  30 827  16   9  69   3   0]
 [  3   0  19 128  46 713   8  82   1   0]
 [  2   0  63  55  45   5 819   5   6   0]
 [  4   0   4  13  27  28   0 923   0   1]
 [ 24   5  16  11   4   1   0   6 906  27]
 [ 10  17   6  22   1   1   0  63  23 857]]
TRAINING class wise acc@17 :: {0: [0.8132], 1: [0.8856], 2: [0.6296], 3: [0.7534], 4: [0.87], 5: [0.7826], 6: [0.9022], 7: [0.9092], 8: [0.9284], 9: [0.9314]}
TRAIINING epoch@17 ::  acc@0.852  loss@0.446
[[2033    5  178   16   32    2    1   27  134   72]
 [   8 2214    4   20    1    0    7    2   78  166]
 [ 148    1 1574  229  279   46  136   49   31    7]
 [  12    7  209 3767  106  604  145  103    8   39]
 [  41    0  227   73 4350  104   69  133    3    0]
 [   5    0   66  709  124 3913   29  151    0    3]
 [   2    7  156  180   66   46 4511    2   27    3]
 [  22    0   39  110  124  130    1 4546    1   27]
 [ 136   48   90   13    3    0   17    3 4642   48]
 [  71  109   15   71    0    0    2   40   35 4657]]
TESTING class wise acc@17 :: {0: [0.762], 1: [0.767], 2: [0.428], 3: [0.732], 4: [0.745], 5: [0.656], 6: [0.805], 7: [0.909], 8: [0.886], 9: [0.951]}
TESTING epoch@17 ::  acc@0.764  loss@0.778
Epoch@@18
[[762   1  41  27  14   1   1  24  63  66]
 [  4 767   1  15   0   0   3   0  23 187]
 [ 76   0 428 222  91  49  47  53  19  15]
 [  5   1  19 732  25  99  31  57   4  27]
 [ 14   0  34  58 745  20  16 108   4   1]
 [  4   0  12 204  16 656   5  97   1   5]
 [  4   0  31 111  11  16 805  12   7   3]
 [ 10   0   4  32  15   8   2 909   1  19]
 [ 40   3   9   6   3   1   2   1 886  49]
 [  8  14   2   7   0   0   0   4  14 951]]
TRAINING class wise acc@18 :: {0: [0.8484], 1: [0.908], 2: [0.6784], 3: [0.7674], 4: [0.8824], 5: [0.8094], 6: [0.9188], 7: [0.9234], 8: [0.9382], 9: [0.9428]}
TRAIINING epoch@18 ::  acc@0.871  loss@0.398
[[2121    2  155   14   20    1    2   35   98   52]
 [   3 2270    3   26    0    0    7    0   45  146]
 [ 127    1 1696  199  241   47  120   30   31    8]
 [  14    9  223 3837   83  568  142   90    8   26]
 [  35    0  200   60 4412   99   58  131    5    0]
 [   4    1   41  611  117 4047   40  139    0    0]
 [   1    4  168  132   50   31 4594    0   20    0]
 [  23    0   25  102  111  100    0 4617    0   22]
 [ 116   49   70   12    2    0   14    1 4691   45]
 [  54  100   11   49    1    1    3   20   47 4714]]
TESTING class wise acc@18 :: {0: [0.758], 1: [0.899], 2: [0.303], 3: [0.845], 4: [0.813], 5: [0.672], 6: [0.717], 7: [0.789], 8: [0.826], 9: [0.865]}
TESTING epoch@18 ::  acc@0.749  loss@0.860
Epoch@@19
[[758  10  25  47  21   3   1  45  35  55]
 [  6 899   1  21   0   1   1   0   8  63]
 [ 72   3 303 379 107  44  47  33   4   8]
 [  7   5   9 845  30  64  12  22   2   4]
 [  7   0  14  98 813  30  13  25   0   0]
 [  2   1   6 268  23 672   1  27   0   0]
 [  3   5   5 236  21  11 717   1   1   0]
 [  7   0   0  89  54  57   0 789   0   4]
 [ 52  43  10  33   4   1   0   5 826  26]
 [  8  58   2  43   2   2   0  14   6 865]]
TRAINING class wise acc@19 :: {0: [0.8392], 1: [0.9012], 2: [0.6732], 3: [0.7762], 4: [0.8806], 5: [0.8056], 6: [0.897], 7: [0.9194], 8: [0.935], 9: [0.934]}
TRAIINING epoch@19 ::  acc@0.865  loss@0.409
[[2098    2  128   18   31    1    0   44  107   71]
 [   4 2253    3   25    0    0   13    2   57  143]
 [ 132    2 1683  208  219   65  128   30   26    7]
 [  18    9  201 3881   93  516  135   99   12   36]
 [  33    0  209   59 4403  114   57  115    9    1]
 [   3    0   65  588  117 4028   36  160    0    3]
 [   0    7  162  197   66   51 4485    1   26    5]
 [  27    0   36   98  110  110    0 4597    0   22]
 [ 124   35   59   23    3    0   25    1 4675   55]
 [  63   96   13   76    0    1    3   36   42 4670]]
TESTING class wise acc@19 :: {0: [0.856], 1: [0.641], 2: [0.428], 3: [0.432], 4: [0.913], 5: [0.76], 6: [0.849], 7: [0.792], 8: [0.93], 9: [0.889]}
TESTING epoch@19 ::  acc@0.749  loss@0.907
Epoch@@20
[[856   0  21   6  21   1   0   6  80   9]
 [ 25 641   2   8   0   1   3   0 111 209]
 [117   0 428  36 254  55  55  15  35   5]
 [ 29   1  61 432 121 217  55  47  20  17]
 [ 25   0  12   6 913  15  12   8   9   0]
 [ 11   0  20  65  97 760  13  29   3   2]
 [  8   0  23  18  70  15 849   3  14   0]
 [ 34   0  12  15 110  26   2 792   4   5]
 [ 50   2   4   2   2   0   0   2 930   8]
 [ 48   4   3   7   3   1   1   4  40 889]]
TRAINING class wise acc@20 :: {0: [0.8664], 1: [0.916], 2: [0.7348], 3: [0.8058], 4: [0.907], 5: [0.8338], 6: [0.927], 7: [0.938], 8: [0.9486], 9: [0.9492]}
TRAIINING epoch@20 ::  acc@0.890  loss@0.339
[[2166    0  125   10   24    0    1   29   91   54]
 [   0 2290    1   16    0    0    6    0   59  128]
 [ 114    0 1837  178  177   33  106   26   25    4]
 [  10    9  171 4029   61  502  101   73    8   36]
 [  27    0  177   37 4535   80   50   92    2    0]
 [   3    0   36  563   87 4169   31  107    1    3]
 [   1    5  131  133   47   32 4635    0   16    0]
 [  23    0   26   78   97   71    0 4690    0   15]
 [  98   40   59    9    1    0   17    1 4743   32]
 [  51   88    8   53    1    1    2   15   35 4746]]
TESTING class wise acc@20 :: {0: [0.792], 1: [0.906], 2: [0.709], 3: [0.643], 4: [0.638], 5: [0.676], 6: [0.809], 7: [0.852], 8: [0.861], 9: [0.821]}
TESTING epoch@20 ::  acc@0.771  loss@0.844
Epoch@@21
[[792  16  55  19   1   0   1   5  72  39]
 [  5 906   6  27   0   2   2   3   5  44]
 [ 73   5 709  76  27  23  30  25  22  10]
 [ 16   8 115 643  23 108  36  30   6  15]
 [ 26   0 180  45 638  29  20  55   7   0]
 [  7   3  59 186  10 676   6  50   2   1]
 [  3   9  87  56   7  18 809   4   6   1]
 [ 25   0  22  36  21  31   0 852   1  12]
 [ 27  66  10  11   1   1   0   1 861  22]
 [ 19 105   7  21   2   1   0  12  12 821]]
TRAINING class wise acc@21 :: {0: [0.828], 1: [0.874], 2: [0.6624], 3: [0.7702], 4: [0.8646], 5: [0.8064], 6: [0.909], 7: [0.9138], 8: [0.925], 9: [0.928]}
TRAIINING epoch@21 ::  acc@0.859  loss@0.440
[[2070    1  117   19   42    3    0   53  115   80]
 [   4 2185    6   57    4    2   17    5   72  148]
 [ 127    2 1656  183  291   33  131   35   35    7]
 [  14    8  183 3851  102  541  140   87   15   59]
 [  48    0  218   98 4323  101   64  129   12    7]
 [   3    0   50  599  122 4032   37  152    1    4]
 [   4   12  137  171   68   42 4545    1   19    1]
 [  29    1   33   87  129  114    0 4569    0   38]
 [ 135   48  100   19    4    1   22    3 4625   43]
 [  60   92   13   74    4    7    2   51   57 4640]]
TESTING class wise acc@21 :: {0: [0.757], 1: [0.812], 2: [0.421], 3: [0.467], 4: [0.871], 5: [0.85], 6: [0.766], 7: [0.793], 8: [0.831], 9: [0.931]}
TESTING epoch@21 ::  acc@0.750  loss@0.942
Epoch@@22
[[757   8  31  21  20   7   1   8  41 106]
 [  6 812   3   4   0   1   1   0  14 159]
 [ 91   2 421  70 206 130  37  22   8  13]
 [  8   7  28 467  83 318  39  24   4  22]
 [ 21   1  28  15 871  35   7  16   0   6]
 [  3   3  11  47  47 850  10  26   0   3]
 [  3  12  40  35 100  34 766   1   5   4]
 [ 12   0   8  19  71  79   2 793   1  15]
 [ 57  21  10  11   6   2   1   1 831  60]
 [ 15  22   3   8   0   3   1  10   7 931]]
TRAINING class wise acc@22 :: {0: [0.86], 1: [0.896], 2: [0.694], 3: [0.7704], 4: [0.8588], 5: [0.8272], 6: [0.9174], 7: [0.9224], 8: [0.9402], 9: [0.9338]}
TRAIINING epoch@22 ::  acc@0.870  loss@0.409
[[2150    1  111   17   35    1    1   25   85   74]
 [   6 2240    6   28    0    0   16    0   61  143]
 [ 108    0 1735  202  274   23   94   24   31    9]
 [  17    5  175 3852  134  545  133   81    8   50]
 [  86    1  228  101 4294   92   53  134    5    6]
 [   3    0   27  549  118 4136   27  137    0    3]
 [   0    9  129  166   60   29 4587    1   17    2]
 [  26    0   18   94  126  102    1 4612    0   21]
 [  88   50   65   15    8    0   18    2 4701   53]
 [  83   79   13   83    6    1    2   22   42 4669]]
TESTING class wise acc@22 :: {0: [0.653], 1: [0.839], 2: [0.704], 3: [0.679], 4: [0.717], 5: [0.726], 6: [0.88], 7: [0.566], 8: [0.855], 9: [0.913]}
TESTING epoch@22 ::  acc@0.753  loss@0.972
Epoch@@23
[[653   4 114  37  38   1   9   5  53  86]
 [  2 839   1  20   0   1   5   0   7 125]
 [ 30   1 704  83  51  36  85   0   5   5]
 [  3   6  72 679  35 110  80   2   4   9]
 [  2   0  85  47 717  63  82   1   0   3]
 [  1   4  40 167  27 726  24   6   0   5]
 [  0   0  38  63   6  12 880   0   1   0]
 [  8   1  43 110  70 185   5 566   1  11]
 [ 21  20  19  23   1   0  24   0 855  37]
 [  5  27   6  33   0   1   3   2  10 913]]
TRAINING class wise acc@23 :: {0: [0.8464], 1: [0.884], 2: [0.7164], 3: [0.7862], 4: [0.889], 5: [0.8196], 6: [0.9088], 7: [0.9138], 8: [0.919], 9: [0.9356]}
TRAIINING epoch@23 ::  acc@0.870  loss@0.413
[[2116    0  123   17   33    1    1   33  111   65]
 [   4 2210    4   30    0    2   42    1   55  152]
 [ 123    0 1791  174  203   31  113   24   27   14]
 [  20   13  159 3931   79  483  146  109   18   42]
 [  36    0  179   54 4445  106   53  118    8    1]
 [   4    0   31  561  115 4098   33  151    3    4]
 [   5   17  139  160   60   39 4544    6   28    2]
 [  34    0   26  112  104  128    3 4569    0   24]
 [ 133   41   87   15    8    0   62    0 4595   59]
 [  49   87   10   69    1    1    0   46   59 4678]]
TESTING class wise acc@23 :: {0: [0.716], 1: [0.78], 2: [0.674], 3: [0.647], 4: [0.775], 5: [0.779], 6: [0.739], 7: [0.828], 8: [0.766], 9: [0.923]}
TESTING epoch@23 ::  acc@0.763  loss@0.910
Epoch@@24
[[716   3  87  47   9   2   1  37  23  75]
 [  5 780   6  44   0   1   5   2  15 142]
 [ 46   0 674  91  95  42  17  29   4   2]
 [ 10   2  77 647  38 159  24  40   0   3]
 [  5   0  52  43 775  47  15  63   0   0]
 [  2   0  29 130  19 779   8  32   0   1]
 [  2   0  98  85  47  22 739   5   2   0]
 [ 14   0  18  42  23  72   1 828   0   2]
 [ 68  11  33  41   2   3   2   5 766  69]
 [  3  20   5  35   0   2   0   9   3 923]]
TRAINING class wise acc@24 :: {0: [0.8648], 1: [0.9036], 2: [0.734], 3: [0.8124], 4: [0.9076], 5: [0.8308], 6: [0.929], 7: [0.9246], 8: [0.9346], 9: [0.9454]}
TRAIINING epoch@24 ::  acc@0.887  loss@0.361
[[2162    3  107   18   22    0    1   44  100   43]
 [   0 2259    1   27    0    1    5    0   57  150]
 [ 117    0 1835  170  184   34   93   27   36    4]
 [  22   13  160 4062   60  461   96   75   13   38]
 [  31    0  143   45 4538   91   38  108    5    1]
 [   3    1   40  522  109 4154   25  139    3    4]
 [   1    8  108  139   51   30 4645    1   16    1]
 [  42    0   28   81  110   90    1 4623    2   23]
 [  98   36   53   35    1    3   16    4 4673   81]
 [  40   86    5   60    1    5    0   26   50 4727]]
TESTING class wise acc@24 :: {0: [0.859], 1: [0.889], 2: [0.546], 3: [0.759], 4: [0.71], 5: [0.622], 6: [0.913], 7: [0.858], 8: [0.82], 9: [0.897]}
TESTING epoch@24 ::  acc@0.787  loss@0.864
Epoch@@25
[[859   7  26  23   4   0   5  10  24  42]
 [  6 889   1  15   0   0  11   0  12  66]
 [ 93   0 546 165  36  20 106  19  12   3]
 [ 16   4  34 759  14  62  72  28   3   8]
 [ 28   0  62  67 710  28  62  39   2   2]
 [  8   0  18 243  18 622  41  45   2   3]
 [  3   0  19  45   3   7 913   5   5   0]
 [ 13   1   4  59  25  27   3 858   2   8]
 [ 78  33   1   9   1   0  14   1 820  43]
 [ 20  46   1  23   0   0   2   6   5 897]]
TRAINING class wise acc@25 :: {0: [0.9072], 1: [0.9256], 2: [0.802], 3: [0.8556], 4: [0.9346], 5: [0.8828], 6: [0.9482], 7: [0.9614], 8: [0.9578], 9: [0.959]}
TRAIINING epoch@25 ::  acc@0.920  loss@0.248
[[2268    0   87    4   10    0    2   24   64   41]
 [   3 2314    0   27    0    1   33    0   40   82]
 [  76    0 2005  132  144    8   95   12   23    5]
 [   5    8  103 4278   45  390   90   54    9   18]
 [  18    0  133   26 4673   48   32   67    1    2]
 [   0    0   19  415   67 4414   12   71    1    1]
 [   1    6   83  108   28   20 4741    0   11    2]
 [  16    0   14   44   56   49    0 4807    0   14]
 [  76   29   32   13    1    0   23    0 4789   37]
 [  46   54    5   45    0    1    0   26   28 4795]]
TESTING class wise acc@25 :: {0: [0.772], 1: [0.796], 2: [0.598], 3: [0.638], 4: [0.672], 5: [0.478], 6: [0.877], 7: [0.743], 8: [0.957], 9: [0.923]}
TESTING epoch@25 ::  acc@0.745  loss@1.168
Epoch@@26
[[772   3  19  10   2   0   2   4 140  48]
 [  3 796   0   2   0   0   5   0  98  96]
 [140   0 598  58  29   5  64   8  85  13]
 [ 31   4  68 638  14  18  99   9  48  71]
 [ 37   0 162  34 672   3  43  21  21   7]
 [ 17   1  67 313  22 478  39  20  18  25]
 [  4   0  61  19   5   2 877   1  29   2]
 [ 48   0  24  64  24   8   5 743  11  73]
 [ 18   4   5   0   2   0   2   0 957  12]
 [  6  29   1   2   0   0   2   1  36 923]]
TRAINING class wise acc@26 :: {0: [0.92], 1: [0.956], 2: [0.8456], 3: [0.8854], 4: [0.9438], 5: [0.9058], 6: [0.9596], 7: [0.9712], 8: [0.97], 9: [0.9738]}
TRAIINING epoch@26 ::  acc@0.938  loss@0.204
[[2300    0   85    3    9    0    0   15   56   32]
 [   0 2390    2    9    0    0    6    1   37   55]
 [  89    0 2114  100  106    8   50   12   17    4]
 [   4    8   91 4427   31  292   75   44    5   23]
 [  12    0  117   20 4719   49   23   56    4    0]
 [   0    0   10  322   62 4529   15   61    0    1]
 [   0    6   89   60   18   17 4798    0   11    1]
 [  12    0    5   29   39   49    0 4856    0   10]
 [  53   21   37   10    0    0    5    0 4850   24]
 [  29   35    8   34    0    0    0    8   17 4869]]
TESTING class wise acc@26 :: {0: [0.762], 1: [0.962], 2: [0.535], 3: [0.592], 4: [0.842], 5: [0.823], 6: [0.897], 7: [0.8], 8: [0.904], 9: [0.82]}
TESTING epoch@26 ::  acc@0.794  loss@0.979
Epoch@@27
[[762  22  34  32  11   3   3   5  96  32]
 [  2 962   0   8   0   0   2   1   9  16]
 [ 72   3 535 107 112  62  76  11  21   1]
 [  6   4  22 592  57 231  59  12  10   7]
 [ 10   2  30  24 842  27  46  17   1   1]
 [  1   3  17  71  51 823  19  15   0   0]
 [  4   6  15  32  17  22 897   2   5   0]
 [ 18   2   5  48  59  60   3 800   2   3]
 [ 17  47   8   9   2   1   5   1 904   6]
 [  9 134   2  15   1   1   1   3  14 820]]
TRAINING class wise acc@27 :: {0: [0.93], 1: [0.9492], 2: [0.8568], 3: [0.8914], 4: [0.9538], 5: [0.9088], 6: [0.9588], 7: [0.9666], 8: [0.9668], 9: [0.9694]}
TRAIINING epoch@27 ::  acc@0.939  loss@0.188
[[2325    1   75    3    5    0    0   16   50   25]
 [   1 2373    0   16    0    0    6    0   30   74]
 [  76    1 2142   93   99    3   56    6   20    4]
 [   6    8   79 4457   31  274   78   37    5   25]
 [  13    0   86   19 4769   44   18   50    1    0]
 [   1    0    6  305   53 4544   18   73    0    0]
 [   0    2   72   74   26   16 4794    0   16    0]
 [  14    0    8   32   50   51    0 4833    1   11]
 [  49   25   36    7    0    0   39    0 4834   10]
 [  24   53    4   40    1    0    1   16   14 4847]]
TESTING class wise acc@27 :: {0: [0.803], 1: [0.817], 2: [0.62], 3: [0.782], 4: [0.848], 5: [0.678], 6: [0.725], 7: [0.862], 8: [0.881], 9: [0.905]}
TESTING epoch@27 ::  acc@0.792  loss@0.864
Epoch@@28
[[803   4  37  39  21   1   1  17  48  29]
 [  5 817   1  20   0   0   1   1  24 131]
 [ 65   0 620 125  89  31  26  27  15   2]
 [ 13   1  31 782  33  83  16  32   2   7]
 [ 12   0  32  50 848  17   8  30   3   0]
 [  3   0  20 209  39 678   3  45   1   2]
 [  5   1  28 180  27  20 725   8   6   0]
 [ 18   0   6  41  40  30   1 862   0   2]
 [ 47   5  14  16   2   0   2   2 881  31]
 [ 19  19   2  23   1   0   1  10  20 905]]
TRAINING class wise acc@28 :: {0: [0.9412], 1: [0.9628], 2: [0.882], 3: [0.9064], 4: [0.9576], 5: [0.9278], 6: [0.9662], 7: [0.9738], 8: [0.9744], 9: [0.9748]}
TRAIINING epoch@28 ::  acc@0.950  loss@0.163
[[2353    1   50    5   11    0    0   10   38   32]
 [   2 2407    1   10    0    0    4    0   22   54]
 [  59    0 2205   82   75    0   52    9   17    1]
 [   2    2   80 4532   27  235   71   26    4   21]
 [   9    0   81   18 4788   39   21   43    1    0]
 [   0    0    5  250   49 4639    9   48    0    0]
 [   0    2   54   67   22   11 4831    0   12    1]
 [  13    0    4   24   35   44    0 4869    1   10]
 [  42   15   32    6    0    0    8    1 4872   24]
 [  29   39    3   18    0    1    0   20   16 4874]]
TESTING class wise acc@28 :: {0: [0.859], 1: [0.805], 2: [0.635], 3: [0.832], 4: [0.792], 5: [0.507], 6: [0.714], 7: [0.743], 8: [0.912], 9: [0.919]}
TESTING epoch@28 ::  acc@0.772  loss@1.067
Epoch@@29
[[859   3  26  21   4   0   0   1  61  25]
 [  6 805   0  18   0   0   2   0  35 134]
 [123   0 635 130  69   8  14   4  13   4]
 [ 23   3  37 832  28  21  17  12  11  16]
 [ 36   0  67  62 792  10   6  20   6   1]
 [ 15   1  33 380  30 507   5  20   4   5]
 [  4   1  75 165  26   0 714   0  14   1]
 [ 35   0  17 105  46  29   1 743   8  16]
 [ 46   9   6   8   1   0   0   0 912  18]
 [ 26  17   4  11   0   0   1   1  21 919]]
TRAINING class wise acc@29 :: {0: [0.9268], 1: [0.9376], 2: [0.8616], 3: [0.901], 4: [0.9516], 5: [0.9162], 6: [0.9558], 7: [0.9684], 8: [0.9576], 9: [0.966]}
TRAIINING epoch@29 ::  acc@0.939  loss@0.207
[[2317    0   53    3   13    0    0   14   76   24]
 [   0 2344    0   39    0    0    1    0   38   78]
 [  63    0 2154   87  111    2   55    7   16    5]
 [   6    8   63 4505   31  260   62   32   12   21]
 [  15    0   93   17 4758   39   27   51    0    0]
 [   0    0    4  270   54 4581   18   71    1    1]
 [   0    4   70   90   23   22 4779    0   12    0]
 [  18    0   10   27   34   50    0 4842    1   18]
 [  81   31   41   15    3    0   11    0 4788   30]
 [  29   49    1   38    1    1    0   20   31 4830]]
TESTING class wise acc@29 :: {0: [0.765], 1: [0.65], 2: [0.56], 3: [0.792], 4: [0.792], 5: [0.739], 6: [0.815], 7: [0.782], 8: [0.738], 9: [0.959]}
TESTING epoch@29 ::  acc@0.759  loss@1.083
Epoch@@30
[[765   4  40  62   8   3   0  20  24  74]
 [  9 650   1  45   0   3   5   2  11 274]
 [ 69   0 560 146 105  55  47  14   1   3]
 [  7   0  36 792  26 101  24   8   2   4]
 [ 19   0  36  77 792  32  18  22   3   1]
 [  4   1  19 183  26 739   8  17   0   3]
 [  3   0  27 106  26  18 815   1   3   1]
 [ 14   0   9  75  57  53   1 782   2   7]
 [149   9  15  29   0   1   5   1 738  53]
 [  5   4   3  19   1   1   1   3   4 959]]
TRAINING class wise acc@30 :: {0: [0.85], 1: [0.8256], 2: [0.7724], 3: [0.8444], 4: [0.9144], 5: [0.8714], 6: [0.926], 7: [0.9404], 8: [0.929], 9: [0.9314]}
TRAIINING epoch@30 ::  acc@0.892  loss@0.350
[[2125   12   86   27   22    1    2   35  128   62]
 [  37 2064   10   47    1    1   15    1  114  210]
 [  90    1 1931  159  172   15   79   22   24    7]
 [  17   14  107 4222   70  343  117   75    8   27]
 [  28    0  124   56 4572   78   42   95    4    1]
 [   4    1    8  402   97 4357   21  109    0    1]
 [   2   13   89  137   76   30 4630    3   20    0]
 [  32    0   16   90   72   67    2 4702    0   19]
 [ 152   73   44   14    6    2   19    1 4645   44]
 [  53  168    5   52    2    1    2   27   33 4657]]
TESTING class wise acc@30 :: {0: [0.836], 1: [0.847], 2: [0.519], 3: [0.741], 4: [0.854], 5: [0.644], 6: [0.811], 7: [0.883], 8: [0.874], 9: [0.925]}
TESTING epoch@30 ::  acc@0.793  loss@0.866
Epoch@@31
[[836   8  31  19  22   0   1   8  32  43]
 [ 13 847   0   6   0   0   3   0  16 115]
 [ 78   3 519 134 133  29  54  31  10   9]
 [ 16   6  23 741  43  71  32  42   2  24]
 [  8   0  16  40 854  21  21  35   2   3]
 [ 13   1  12 193  48 644   5  77   1   6]
 [  4   7  13 111  18  23 811   8   5   0]
 [ 14   2   4  28  45  12   0 883   1  11]
 [ 44  27   3   9   5   0   1   1 874  36]
 [ 21  27   2   5   2   0   3   4  11 925]]
TRAINING class wise acc@31 :: {0: [0.9008], 1: [0.9192], 2: [0.7984], 3: [0.8462], 4: [0.9222], 5: [0.8644], 6: [0.9102], 7: [0.9472], 8: [0.9566], 9: [0.9538]}
TRAIINING epoch@31 ::  acc@0.907  loss@0.297
[[2252    0   78   12   12    0    1   23   74   48]
 [   1 2298    0   23    0    0   12    0   57  109]
 [  78    0 1996  138  112   47   74   27   26    2]
 [   8    7  115 4231   47  356  131   57   13   35]
 [  20    0  126   27 4611   86   55   71    4    0]
 [   1    0   30  409   88 4322   45  102    0    3]
 [   0    8   68  187   46  121 4551    1   16    2]
 [  25    0   19   64   75   68    1 4736    0   12]
 [  69   43   33   16    3    0   13    2 4783   38]
 [  47   70    4   45    0    1    3   23   38 4769]]
TESTING class wise acc@31 :: {0: [0.763], 1: [0.768], 2: [0.695], 3: [0.627], 4: [0.748], 5: [0.797], 6: [0.876], 7: [0.816], 8: [0.802], 9: [0.971]}
TESTING epoch@31 ::  acc@0.786  loss@0.932
Epoch@@32
[[763   3  75  14   5   3   0   7  24 106]
 [  1 768   1   4   0   2   2   0   7 215]
 [ 52   2 695  64  60  57  43   8   5  14]
 [ 10   3  65 627  26 160  58  16   3  32]
 [ 11   0 110  39 748  25  34  25   3   5]
 [  1   1  34 101  26 797  20  14   0   6]
 [  1   4  49  37   6  15 876   2   7   3]
 [ 13   0  15  37  30  54   3 816   0  32]
 [ 57   8  11   6   4   0   2   1 802 109]
 [  2  11   4   6   0   0   2   1   3 971]]
TRAINING class wise acc@32 :: {0: [0.9408], 1: [0.9432], 2: [0.8936], 3: [0.9132], 4: [0.9612], 5: [0.9326], 6: [0.9592], 7: [0.97], 8: [0.963], 9: [0.97]}
TRAIINING epoch@32 ::  acc@0.948  loss@0.168
[[2352    0   45    3    9    0    0   13   59   19]
 [   3 2358    0    9    0    0    9    0   35   86]
 [  46    0 2234   73   86    2   41    2   15    1]
 [   2    4   64 4566   22  215   68   35    5   19]
 [  11    0   67    8 4806   40   17   49    2    0]
 [   0    0    7  218   45 4663   23   43    0    1]
 [   0   12   67   68   21   26 4796    0    9    1]
 [  14    0    4   37   41   44    0 4850    1    9]
 [  73   22   23    9    3    0   15    0 4815   40]
 [  21   47    1   32    0    1    0   10   38 4850]]
TESTING class wise acc@32 :: {0: [0.74], 1: [0.775], 2: [0.651], 3: [0.546], 4: [0.816], 5: [0.839], 6: [0.691], 7: [0.893], 8: [0.911], 9: [0.83]}
TESTING epoch@32 ::  acc@0.769  loss@1.012
Epoch@@33
[[740   1 116  17  37   8   0  16  57   8]
 [ 20 775   5  39   2   3  11   3  75  67]
 [ 24   0 651  55 138  81  15  31   5   0]
 [  6   1  35 546  51 318   8  32   3   0]
 [  6   0  38  17 816  56   5  59   2   1]
 [  3   0  20  50  40 839   1  47   0   0]
 [  2   0  54  75  52 110 691  10   6   0]
 [ 12   0   5  18  24  47   0 893   0   1]
 [ 34   2  22   6   6   2   3   3 911  11]
 [ 36  31   9  32   3   5   2  23  29 830]]
TRAINING class wise acc@33 :: {0: [0.9424], 1: [0.9624], 2: [0.888], 3: [0.9038], 4: [0.9594], 5: [0.9282], 6: [0.9528], 7: [0.9766], 8: [0.974], 9: [0.9762]}
TRAIINING epoch@33 ::  acc@0.949  loss@0.166
[[2356    0   44    5   10    0    1   17   42   25]
 [   1 2406    0   13    0    0    2    0   32   46]
 [  59    0 2220   74   69   17   38    9   14    0]
 [   5    7   52 4519   25  278   67   23    4   20]
 [  11    0   68    7 4797   54   28   34    1    0]
 [   2    0    9  225   58 4641   22   43    0    0]
 [   0    5   40   76   22   74 4764    1   18    0]
 [  13    0    7   21   26   43    0 4883    0    7]
 [  38   21   28    4    2    1   15    0 4870   21]
 [  24   32    3   31    0    2    0   15   12 4881]]
TESTING class wise acc@33 :: {0: [0.781], 1: [0.825], 2: [0.627], 3: [0.834], 4: [0.696], 5: [0.661], 6: [0.798], 7: [0.85], 8: [0.899], 9: [0.747]}
TESTING epoch@33 ::  acc@0.772  loss@1.188
Epoch@@34
[[781   3  39  61  10   6   2  23  59  16]
 [  7 825   2  37   0   2  10   0  83  34]
 [ 70   1 627 174  31  40  23  21  13   0]
 [  3   1  44 834  14  53  31  13   5   2]
 [ 13   0  88  86 696  45  23  44   4   1]
 [  2   0  18 276  12 661   6  25   0   0]
 [  2   1  40 112   5  15 798   4  23   0]
 [  3   0  12  66  15  48   3 850   1   2]
 [ 49   2   5  27   0   1   7   2 899   8]
 [ 19  58   9  86   0   7  11  27  36 747]]
TRAINING class wise acc@34 :: {0: [0.8664], 1: [0.884], 2: [0.7828], 3: [0.8288], 4: [0.9098], 5: [0.8632], 6: [0.9196], 7: [0.9324], 8: [0.9386], 9: [0.9264]}
TRAIINING epoch@34 ::  acc@0.892  loss@0.345
[[2166    2   70   18   45    4    3   28  110   54]
 [   2 2210    4   19    1    1   30    0   59  174]
 [  77    2 1957  153  141   32   77   19   31   11]
 [  12    5  112 4144   84  368  130   76    9   60]
 [  46    0  108   63 4549   83   44   95    4    8]
 [   2    1   18  406   84 4316   53  103    2   15]
 [   5   23   72  151   60   45 4598    0   35   11]
 [  16    0   18   69  117   96    1 4662    2   19]
 [ 109   42   40    9    3    2   31    2 4693   69]
 [  51  137    6   67    8   13   17   29   40 4632]]
TESTING class wise acc@34 :: {0: [0.849], 1: [0.899], 2: [0.659], 3: [0.704], 4: [0.883], 5: [0.686], 6: [0.876], 7: [0.812], 8: [0.876], 9: [0.825]}
TESTING epoch@34 ::  acc@0.807  loss@0.792
Epoch@@35
[[849   4  56  15  23   2   8   4  27  12]
 [ 15 899   2  11   1   0   7   0  27  38]
 [ 55   1 659  85 107  23  49  17   4   0]
 [ 11   6  60 704  60  76  50  25   3   5]
 [  8   1  32  28 883  15  23   8   2   0]
 [  7   1  35 177  54 686  15  24   0   1]
 [  2   0  35  45  25   8 876   6   3   0]
 [ 20   0  17  43  79  26   3 812   0   0]
 [ 65  11  18  11   2   0   7   1 876   9]
 [ 39  67   7  14   6   0   4  12  26 825]]
TRAINING class wise acc@35 :: {0: [0.9632], 1: [0.966], 2: [0.9184], 3: [0.938], 4: [0.9764], 5: [0.9522], 6: [0.9766], 7: [0.9824], 8: [0.982], 9: [0.9762]}
TRAIINING epoch@35 ::  acc@0.966  loss@0.105
[[2408    0   28    3    5    0    0    6   36   14]
 [   0 2415    0    7    0    0    7    0   20   51]
 [  36    0 2296   69   57    3   25    4   10    0]
 [   2    5   46 4690    8  161   46   19    0   23]
 [   6    0   46    8 4882   24    8   24    2    0]
 [   0    0    2  158   33 4761   12   34    0    0]
 [   0    9   35   40   13   12 4883    0    8    0]
 [   8    0    2   14   29   29    0 4912    0    6]
 [  36   14   15    5    1    0    5    0 4910   14]
 [  20   42    1   31    1    1    1    7   15 4881]]
TESTING class wise acc@35 :: {0: [0.668], 1: [0.883], 2: [0.618], 3: [0.732], 4: [0.805], 5: [0.713], 6: [0.892], 7: [0.861], 8: [0.918], 9: [0.936]}
TESTING epoch@35 ::  acc@0.803  loss@1.002
Epoch@@36
[[668  14  80  24  12   4   5   6 107  80]
 [  1 883   1   4   0   0   0   1  17  93]
 [ 34   2 618  96  98  32  73  25  18   4]
 [  4   4  43 732  39  84  51  20   6  17]
 [  6   2  46  36 805  29  27  37   4   8]
 [  1   2  23 171  28 713  17  36   1   8]
 [  2   6  11  45  18  12 892   6   8   0]
 [  8   3   8  36  32  31   1 861   3  17]
 [ 12  22   5   5   3   0   4   1 918  30]
 [  3  35   2  10   0   0   2   1  11 936]]
TRAINING class wise acc@36 :: {0: [0.966], 1: [0.9744], 2: [0.9352], 3: [0.9512], 4: [0.9774], 5: [0.9652], 6: [0.9832], 7: [0.9856], 8: [0.985], 9: [0.9844]}
TRAIINING epoch@36 ::  acc@0.973  loss@0.081
[[2415    0   21    3    6    0    0    6   31   18]
 [   0 2436    0   12    0    0    2    0   17   33]
 [  37    0 2338   45   47    1   24    0    7    1]
 [   2    4   46 4756    5  118   35   22    2   10]
 [  11    0   30    9 4887   18   15   27    2    1]
 [   0    0    1  113   30 4826    7   23    0    0]
 [   0    4   17   34    9   14 4916    0    5    1]
 [   6    0    1   12   28   23    0 4928    0    2]
 [  28   11   11    4    1    0    7    0 4925   13]
 [  14   22    0   23    0    1    0    8   10 4922]]
TESTING class wise acc@36 :: {0: [0.831], 1: [0.935], 2: [0.614], 3: [0.739], 4: [0.78], 5: [0.723], 6: [0.907], 7: [0.781], 8: [0.916], 9: [0.849]}
TESTING epoch@36 ::  acc@0.808  loss@1.042
Epoch@@37
[[831  10  31  23   7   4   5   3  59  27]
 [  5 935   0   8   0   0   1   0  20  31]
 [ 80   2 614 118  66  31  63   9  16   1]
 [ 15   4  44 739  31  94  53   6   8   6]
 [ 21   0  56  43 780  29  33  30   6   2]
 [  6   0  34 174  23 723  19  18   1   2]
 [  4   2  15  39  16   7 907   3   7   0]
 [ 31   0  14  72  37  48   8 781   2   7]
 [ 34  17   6   9   1   0   5   1 916  11]
 [ 16  90   2  19   0   1   3   4  16 849]]
TRAINING class wise acc@37 :: {0: [0.9712], 1: [0.9764], 2: [0.9368], 3: [0.9594], 4: [0.9806], 5: [0.9694], 6: [0.9836], 7: [0.9872], 8: [0.9864], 9: [0.985]}
TRAIINING epoch@37 ::  acc@0.976  loss@0.074
[[2428    0   24    0    3    0    0    7   27   11]
 [   0 2441    0    6    0    0   10    0   16   27]
 [  33    0 2342   51   43    3   21    1    6    0]
 [   1    1   43 4797    7   98   28   11    4   10]
 [   3    0   39    2 4903   30    7   16    0    0]
 [   0    0    3   98   18 4847    9   24    0    1]
 [   0    8   21   29   10    8 4918    0    6    0]
 [   5    0    2   17   19   15    0 4936    0    6]
 [  20   21   10    1    1    0    6    0 4932    9]
 [  14   22    1   16    0    1    0   11   10 4925]]
TESTING class wise acc@37 :: {0: [0.768], 1: [0.847], 2: [0.52], 3: [0.613], 4: [0.855], 5: [0.847], 6: [0.805], 7: [0.852], 8: [0.907], 9: [0.923]}
TESTING epoch@37 ::  acc@0.794  loss@1.152
Epoch@@38
[[768   5  36  32  23   5   1  15  69  46]
 [  5 847   0   8   1   2   2   1  40  94]
 [ 56   1 520 128 126  80  42  15  21  11]
 [ 12   1  23 613  46 240  16  30   6  13]
 [ 12   0  16  37 855  33  11  29   5   2]
 [  1   1  13  66  37 847   4  27   1   3]
 [  3   4  10  60  54  39 805   5  13   7]
 [  9   0   5  28  45  50   1 852   3   7]
 [ 29   6   8  14   4   2   5   0 907  25]
 [ 10  26   1   8   1   3   2   4  22 923]]
TRAINING class wise acc@38 :: {0: [0.972], 1: [0.9776], 2: [0.942], 3: [0.9576], 4: [0.9796], 5: [0.9694], 6: [0.9822], 7: [0.9858], 8: [0.9826], 9: [0.9872]}
TRAIINING epoch@38 ::  acc@0.975  loss@0.080
[[2430    0   20    3    2    0    0    9   24   12]
 [   0 2444    0    4    0    0    4    0   20   28]
 [  32    0 2355   38   35    0   22    2   16    0]
 [   1    2   34 4788    8  109   29   18    2    9]
 [   8    0   37    9 4898   22    9   17    0    0]
 [   0    0    1   94   22 4847   10   25    0    1]
 [   0    8   23   27   15    9 4911    0    7    0]
 [   9    0    2   13   22   21    0 4929    0    4]
 [  24   17   21    5    2    0    9    0 4913    9]
 [  18   18    1   15    0    2    0    3    7 4936]]
TESTING class wise acc@38 :: {0: [0.767], 1: [0.958], 2: [0.661], 3: [0.562], 4: [0.776], 5: [0.839], 6: [0.905], 7: [0.759], 8: [0.881], 9: [0.718]}
TESTING epoch@38 ::  acc@0.783  loss@1.323
Epoch@@39
[[767  24  65  25  17   3   7   5  64  23]
 [  3 958   1   9   1   1   2   0   9  16]
 [ 54   3 661  53  82  55  69  13   9   1]
 [  8   7  53 562  40 235  77  15   3   0]
 [ 11   2  79  28 776  42  41  17   2   2]
 [  4   1  27  53  36 839  26  13   0   1]
 [  3   3  23  16  13  29 905   4   4   0]
 [ 18   2  23  40  49  87   7 759   4  11]
 [ 32  48  15   4   3   2  10   1 881   4]
 [ 16 219   2  20   1   3   7   2  12 718]]
TRAINING class wise acc@39 :: {0: [0.9248], 1: [0.9336], 2: [0.8904], 3: [0.9146], 4: [0.9556], 5: [0.9298], 6: [0.9634], 7: [0.9672], 8: [0.9628], 9: [0.9542]}
TRAIINING epoch@39 ::  acc@0.944  loss@0.181
[[2312    3   49    7   14    1    0   19   56   39]
 [   2 2334    0   11    0    0   14    0   37  102]
 [  49    0 2226   78   74    6   44    9   12    2]
 [  14    8   55 4573   30  181   72   34    7   26]
 [  17    0   54   15 4778   42   27   62    4    1]
 [   1    0    4  212   68 4649   17   48    0    1]
 [   1    6   44   63   27   22 4817    0   20    0]
 [  12    0    7   44   52   32    0 4836    0   17]
 [  56   41   21   12    4    0   11    0 4814   41]
 [  36  101    1   29    4    0    3   14   41 4771]]
TESTING class wise acc@39 :: {0: [0.702], 1: [0.843], 2: [0.674], 3: [0.771], 4: [0.815], 5: [0.63], 6: [0.882], 7: [0.874], 8: [0.929], 9: [0.92]}
TESTING epoch@39 ::  acc@0.804  loss@0.949
Epoch@@40
[[702   5 111  37  31   1   3  13  63  34]
 [  9 843   0  11   1   0   8   1  42  85]
 [ 28   1 674 106  74  21  64  20  10   2]
 [  5   1  47 771  42  50  35  30   8  11]
 [  4   0  52  51 815  14  20  37   5   2]
 [  2   0  26 245  31 630  16  45   2   3]
 [  1   0  18  57  19  11 882   4   7   1]
 [ 12   0  10  40  39  16   2 874   2   5]
 [ 18   7   5  10   2   0   7   1 929  21]
 [ 11  27   4   9   1   2   1   5  20 920]]
TRAINING class wise acc@40 :: {0: [0.972], 1: [0.9804], 2: [0.9472], 3: [0.9664], 4: [0.9804], 5: [0.9684], 6: [0.983], 7: [0.9866], 8: [0.988], 9: [0.989]}
TRAIINING epoch@40 ::  acc@0.978  loss@0.077
[[2430    0   26    1    6    0    0    3   23   11]
 [   0 2451    0    7    0    0    4    0   11   27]
 [  31    0 2368   41   34    2   16    4    4    0]
 [   1    1   32 4832    5   81   22   12    4   10]
 [   4    0   30    2 4902   29   11   20    2    0]
 [   0    0    1   86   36 4842   10   25    0    0]
 [   0    4   22   29   12   10 4915    0    7    1]
 [   7    0    1   12   24   18    0 4933    0    5]
 [  20    7    8    3    1    0   10    0 4940   11]
 [  11   18    0   12    0    0    0    4   10 4945]]
TESTING class wise acc@40 :: {0: [0.523], 1: [0.803], 2: [0.559], 3: [0.713], 4: [0.826], 5: [0.757], 6: [0.915], 7: [0.832], 8: [0.875], 9: [0.937]}
TESTING epoch@40 ::  acc@0.774  loss@1.283
Epoch@@41
[[523   7 137  76  27   7  15  30  79  99]
 [  1 803   0  21   0   3  15   2  14 141]
 [ 12   0 559 109  89  44 162  20   3   2]
 [  2   3  33 713  31 113  72  19   2  12]
 [  2   1  33  43 826  21  50  18   3   3]
 [  0   0  15 146  31 757  26  24   0   1]
 [  0   1  13  37  14  14 915   4   1   1]
 [  5   0   9  55  48  37   7 832   0   7]
 [  9  16  12  11   2   0  32   1 875  42]
 [  0  24   5  15   1   2   3   4   9 937]]
TRAINING class wise acc@41 :: {0: [0.95], 1: [0.9592], 2: [0.9072], 3: [0.9318], 4: [0.9638], 5: [0.9484], 6: [0.959], 7: [0.9732], 8: [0.9698], 9: [0.9754]}
TRAIINING epoch@41 ::  acc@0.956  loss@0.141
[[2375    0   38    2   14    0    1   15   37   18]
 [   0 2398    0    9    0    1   15    1   21   55]
 [  43    0 2268   54   55    3   62    3   12    0]
 [   2    1   54 4659   13  148   71   25    8   19]
 [  13    0   48   14 4819   35   31   37    2    1]
 [   0    0    2  142   46 4742   17   45    1    5]
 [   0   11   63   64   33   21 4795    0   12    1]
 [  11    0    6   19   45   45    1 4866    0    7]
 [  53   15   16    5    1    0   45    0 4849   16]
 [  19   34    0   28    4    7    2   12   17 4877]]
TESTING class wise acc@41 :: {0: [0.788], 1: [0.92], 2: [0.597], 3: [0.596], 4: [0.797], 5: [0.803], 6: [0.872], 7: [0.907], 8: [0.896], 9: [0.885]}
TESTING epoch@41 ::  acc@0.806  loss@1.044
Epoch@@42
[[788  15  37  16  15   6   5  14  75  29]
 [  4 920   0   2   0   2   4   1  18  49]
 [ 55   5 597  83  77  49  66  45  17   6]
 [ 11   4  28 596  46 203  38  55   7  12]
 [ 13   0  42  24 797  29  32  55   4   4]
 [  3   1  15  76  32 803   6  57   3   4]
 [  4   2  17  33  13  39 872  10   6   4]
 [ 10   1   3  15  28  32   1 907   0   3]
 [ 31  26   2   4   4   0   9   4 896  24]
 [ 16  58   1   6   2   2   4  14  12 885]]
TRAINING class wise acc@42 :: {0: [0.9668], 1: [0.9664], 2: [0.9436], 3: [0.9498], 4: [0.9792], 5: [0.9678], 6: [0.982], 7: [0.9828], 8: [0.9812], 9: [0.9804]}
TRAIINING epoch@42 ::  acc@0.972  loss@0.104
[[2417    0   26    3    6    0    0    6   29   13]
 [   0 2416    1    8    0    0    4    0   19   52]
 [  33    0 2359   42   36    2   18    2    8    0]
 [   5    2   32 4749   12  120   38   27    2   13]
 [   6    1   28    8 4896   23   10   27    0    1]
 [   0    0    2  103   20 4839    8   24    0    4]
 [   0    2   24   37    9   10 4910    0    8    0]
 [   9    0    2   12   34   27    0 4914    0    2]
 [  44   15   12    6    2    0    5    0 4906   10]
 [   9   49    0   24    1    2    0    2   11 4902]]
TESTING class wise acc@42 :: {0: [0.839], 1: [0.758], 2: [0.673], 3: [0.675], 4: [0.797], 5: [0.798], 6: [0.763], 7: [0.837], 8: [0.87], 9: [0.808]}
TESTING epoch@42 ::  acc@0.782  loss@1.165
Epoch@@43
[[839   0  81  11  22   3   1   7  32   4]
 [ 43 758   6  34   3   3   4   1  96  52]
 [ 47   0 673  88  94  48  24  24   2   0]
 [ 10   1  76 675  44 152  12  26   3   1]
 [ 10   0  63  37 797  42  12  38   1   0]
 [  4   0  24 121  28 798   2  23   0   0]
 [  8   0  56  79  37  45 763  10   2   0]
 [ 20   0  16  48  26  51   2 837   0   0]
 [ 73   1  31  10   3   3   2   2 870   5]
 [ 66  31  14  34   1   3   0  16  27 808]]
TRAINING class wise acc@43 :: {0: [0.9356], 1: [0.9496], 2: [0.88], 3: [0.9092], 4: [0.9544], 5: [0.9324], 6: [0.9574], 7: [0.9642], 8: [0.9698], 9: [0.9622]}
TRAIINING epoch@43 ::  acc@0.945  loss@0.183
[[2339    2   44   13   10    0    0   16   47   29]
 [  17 2374    1   15    2    1    6    0   29   55]
 [  43    0 2200  102   81    7   46    8   13    0]
 [  21    4   70 4546   42  185   47   54    5   26]
 [   8    0   52   35 4772   40   31   57    4    1]
 [   2    0    3  179   62 4662   30   58    0    4]
 [   2    4   43   63   65   28 4787    1    7    0]
 [  18    1    4   41   45   60    1 4821    0    9]
 [  61   31   21   10    3    0   10    0 4849   15]
 [  62   45    0   38    3    2    0   23   16 4811]]
TESTING class wise acc@43 :: {0: [0.757], 1: [0.848], 2: [0.602], 3: [0.709], 4: [0.825], 5: [0.687], 6: [0.874], 7: [0.902], 8: [0.878], 9: [0.925]}
TESTING epoch@43 ::  acc@0.801  loss@0.979
Epoch@@44
[[757   8  49  48  21   1   4  29  38  45]
 [  6 848   1  21   1   1   2   0   9 111]
 [ 38   2 602 153  93  21  50  36   5   0]
 [  4   2  41 709  49  92  40  54   2   7]
 [  9   0  40  35 825  23  29  38   1   0]
 [  1   0  17 174  38 687  13  70   0   0]
 [  2   2  18  72  11  14 874   5   2   0]
 [  4   0   4  35  35  17   2 902   0   1]
 [ 42  20  11  16   7   0   3   2 878  21]
 [  2  23   5  22   1   1   0  11  10 925]]
TRAINING class wise acc@44 :: {0: [0.9744], 1: [0.9844], 2: [0.9616], 3: [0.9714], 4: [0.9842], 5: [0.978], 6: [0.9878], 7: [0.9908], 8: [0.9896], 9: [0.9896]}
TRAIINING epoch@44 ::  acc@0.983  loss@0.054
[[2436    0   19    7    4    0    0    6   17   11]
 [   0 2461    0    7    0    0    3    0    7   22]
 [  16    0 2404   29   32    1   14    0    4    0]
 [   6    5   19 4857    6   66   19   12    3    7]
 [   8    0   21    3 4921   13   12   22    0    0]
 [   0    0    0   73   16 4890    6   15    0    0]
 [   0    2   19   18    8    6 4939    0    8    0]
 [   6    0    1    5   17   11    0 4954    0    6]
 [  25    5    7    2    1    0    7    0 4948    5]
 [  10   17    0   15    1    1    0    4    4 4948]]
TESTING class wise acc@44 :: {0: [0.761], 1: [0.835], 2: [0.552], 3: [0.772], 4: [0.777], 5: [0.774], 6: [0.836], 7: [0.894], 8: [0.875], 9: [0.925]}
TESTING epoch@44 ::  acc@0.800  loss@1.085
Epoch@@45
[[761   3  37  46  23   3   3  40  43  41]
 [  7 835   0  14   1   3   3   2  21 114]
 [ 57   0 552 153 103  47  43  38   2   5]
 [  5   2  22 772  37  91  28  34   2   7]
 [  4   0  39  53 777  29  20  75   1   2]
 [  1   0  14 139  21 774   8  43   0   0]
 [  3   0  23  91  20  17 836   5   5   0]
 [  6   0  10  37  21  26   1 894   0   5]
 [ 34   9  17  16   5   0   5   3 875  36]
 [  8  20   2  17   1   2   2  15   8 925]]
TRAINING class wise acc@45 :: {0: [0.9688], 1: [0.9816], 2: [0.9512], 3: [0.9648], 4: [0.9818], 5: [0.9774], 6: [0.9786], 7: [0.988], 8: [0.9872], 9: [0.9876]}
TRAIINING epoch@45 ::  acc@0.978  loss@0.079
[[2422    0   22    4   11    0    1    8   24    8]
 [   0 2454    0    9    0    0    1    0    8   28]
 [  28    0 2378   46   25    0   17    3    3    0]
 [   1    4   35 4824    7   63   41   12    6    7]
 [   9    0   25    5 4909   21    9   21    1    0]
 [   0    0    0   66   17 4887    7   22    0    1]
 [   1    3   22   49   12   10 4893    0   10    0]
 [   9    0    1   10   17   18    0 4940    0    5]
 [  23    9    9    4    2    0    7    0 4936   10]
 [  17   27    0    7    0    0    1    3    7 4938]]
TESTING class wise acc@45 :: {0: [0.711], 1: [0.797], 2: [0.541], 3: [0.732], 4: [0.791], 5: [0.792], 6: [0.941], 7: [0.834], 8: [0.877], 9: [0.863]}
TESTING epoch@45 ::  acc@0.788  loss@1.208
Epoch@@46
[[711   4  37  56  61   8  13  22  56  32]
 [  5 797   0  34   2   6  29   1  22 104]
 [ 28   0 541  88 130  69 124  15   3   2]
 [  2   1  20 732  20 126  82  12   2   3]
 [  1   0  23  43 791  57  46  39   0   0]
 [  0   0   8 131  30 792  21  18   0   0]
 [  1   0   8  23   9  15 941   2   1   0]
 [  5   0  10  55  29  60   6 834   1   0]
 [ 29   5   4  19  12   3  27   2 877  22]
 [ 10  24   1  38   6  11  12  23  12 863]]
TRAINING class wise acc@46 :: {0: [0.9684], 1: [0.9572], 2: [0.9456], 3: [0.954], 4: [0.9792], 5: [0.9694], 6: [0.984], 7: [0.9838], 8: [0.9858], 9: [0.9846]}
TRAIINING epoch@46 ::  acc@0.974  loss@0.083
[[2421    1   20    4    8    0    0    9   21   16]
 [   0 2393    0   42    0    0    2    0   25   38]
 [  23    0 2364   34   48    1   23    4    3    0]
 [   5   19   34 4770    6  103   29   20    3   11]
 [   6    0   30    7 4896   23    6   30    2    0]
 [   0    0    2   96   22 4847    9   22    0    2]
 [   0    3   21   27   13   10 4920    0    6    0]
 [  14    0    4   21   21   18    0 4919    0    3]
 [  26    9    7    6    9    0    7    0 4929    7]
 [  14   21    0   20    0    0    0   13    9 4923]]
TESTING class wise acc@46 :: {0: [0.81], 1: [0.919], 2: [0.557], 3: [0.755], 4: [0.744], 5: [0.675], 6: [0.918], 7: [0.88], 8: [0.825], 9: [0.904]}
TESTING epoch@46 ::  acc@0.799  loss@1.220
Epoch@@47
[[810  19  22  34  17   3   3  13  28  51]
 [  3 919   0   6   0   1   2   0  11  58]
 [ 67   6 557 127  71  28  88  44   8   4]
 [ 11   5  22 755  24  70  71  29   4   9]
 [  8   1  26  42 744  50  66  53   3   7]
 [  6   2  21 191  17 675  41  46   1   0]
 [  4   0  12  49   3  10 918   2   2   0]
 [  5   0   5  58  15  26   4 880   0   7]
 [ 40  62   5  13   4   0   9   3 825  39]
 [  6  68   1   9   0   2   2   1   7 904]]
TRAINING class wise acc@47 :: {0: [0.9784], 1: [0.986], 2: [0.9592], 3: [0.974], 4: [0.9876], 5: [0.9818], 6: [0.986], 7: [0.9916], 8: [0.9916], 9: [0.9894]}
TRAIINING epoch@47 ::  acc@0.984  loss@0.051
[[2446    0   20    5    5    0    0    1   15    8]
 [   0 2465    0    5    0    0    3    0   11   16]
 [  26    0 2398   24   21    1   18    3    9    0]
 [   4    2   24 4870    1   51   25    9    1   13]
 [   3    0   16    4 4938   13    9   17    0    0]
 [   0    0    2   53   17 4909    6   13    0    0]
 [   0    2   17   24   12    9 4930    0    6    0]
 [   4    0    1   12   12    8    0 4958    0    5]
 [  13    8    5    4    0    0    5    0 4958    7]
 [   8   12    2   16    1    2    0    5    7 4947]]
TESTING class wise acc@47 :: {0: [0.889], 1: [0.86], 2: [0.606], 3: [0.68], 4: [0.863], 5: [0.644], 6: [0.88], 7: [0.874], 8: [0.909], 9: [0.896]}
TESTING epoch@47 ::  acc@0.810  loss@1.067
Epoch@@48
[[889   5  18  12  14   0   0   9  38  15]
 [ 18 860   0   5   0   1   1   0  33  82]
 [108   1 606  92  97  21  33  23  16   3]
 [ 31   1  52 680  73  53  49  44  10   7]
 [ 22   1  32  18 863   8  24  26   5   1]
 [ 13   0  28 165  62 644  28  58   0   2]
 [ 10   0  24  36  30   4 880   3  13   0]
 [ 18   0   9  25  49  15   3 874   1   6]
 [ 57   3   2   3   4   1   1   1 909  19]
 [ 24  42   2   7   4   0   1   4  20 896]]
TRAINING class wise acc@48 :: {0: [0.9784], 1: [0.9852], 2: [0.9624], 3: [0.9754], 4: [0.9858], 5: [0.98], 6: [0.9892], 7: [0.9914], 8: [0.991], 9: [0.9924]}
TRAIINING epoch@48 ::  acc@0.984  loss@0.067
[[2446    0   18    2    3    0    0    7   16    8]
 [   0 2463    0    2    0    0    6    0   12   17]
 [  17    0 2406   27   33    1   11    2    3    0]
 [   3    3   21 4877    8   53   17    8    0   10]
 [   3    0   26    4 4929   16    5   16    0    1]
 [   0    0    2   66   14 4900    4   14    0    0]
 [   0    2   16   18    7    4 4946    0    7    0]
 [   4    0    2    9   12   10    0 4957    0    6]
 [  18   10    3    2    1    0    6    0 4955    5]
 [   7   11    0    8    1    0    0    6    5 4962]]
TESTING class wise acc@48 :: {0: [0.847], 1: [0.853], 2: [0.502], 3: [0.618], 4: [0.93], 5: [0.672], 6: [0.82], 7: [0.858], 8: [0.866], 9: [0.869]}
TESTING epoch@48 ::  acc@0.783  loss@1.183
Epoch@@49
[[847   6  27  13  41   4   1  13  28  20]
 [ 12 853   0  11   2   0   0   1  27  94]
 [ 82   0 502  62 224  45  39  33  13   0]
 [ 21   1  40 618 123  95  36  55   5   6]
 [  9   1  13  13 930   7   9  15   3   0]
 [  9   1  13 122 103 672  18  59   1   2]
 [  7   3  18  45  80  15 820   3   6   3]
 [ 16   0   3  29  79  12   1 858   1   1]
 [ 96   7   3   5   7   0   0   3 866  13]
 [ 39  39   0   8   8   0   1  24  12 869]]
TRAINING class wise acc@49 :: {0: [0.942], 1: [0.9652], 2: [0.9284], 3: [0.9404], 4: [0.966], 5: [0.9562], 6: [0.9704], 7: [0.9754], 8: [0.9712], 9: [0.9666]}
TRAIINING epoch@49 ::  acc@0.960  loss@0.125
[[2355    0   29    4   17    1    0   13   49   32]
 [   0 2413    0   12    1    0   11    0   14   49]
 [  31    0 2321   52   53    1   32    5    5    0]
 [   5    9   43 4702   10  128   43   41    2   17]
 [  13    0   48   18 4830   31   19   32    5    4]
 [   0    1    3  121   36 4781   18   36    0    4]
 [   1    4   28   53   32   21 4852    0    9    0]
 [   7    0    3   34   35   27    1 4877    1   15]
 [  73   19   11    4    6    0   12    0 4856   19]
 [  32   38    0   27    4    1    0   47   18 4833]]
TESTING class wise acc@49 :: {0: [0.852], 1: [0.887], 2: [0.508], 3: [0.6], 4: [0.831], 5: [0.857], 6: [0.892], 7: [0.812], 8: [0.884], 9: [0.899]}
TESTING epoch@49 ::  acc@0.802  loss@1.121
[[852   3   9  28  13   7   5  14  36  33]
 [  7 887   0  14   0   6   4   1  17  64]
 [ 98   1 508 100  92  99  74  14  10   4]
 [  8   2  20 600  43 251  46  17   6   7]
 [ 21   0  23  26 831  52  20  26   1   0]
 [  2   0   8  75  33 857  13  10   0   2]
 [  5   1   6  18  31  37 892   3   6   1]
 [ 17   0   5  28  29 103   2 812   0   4]
 [ 56  13   1   9   7   4   6   1 884  19]
 [ 13  44   2   8   2  10   3   7  12 899]]
