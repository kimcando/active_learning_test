

Fri Jan 15 17:39:31 2021
imbal_test_imbal12345_rand_cifar10_vgg_Adam's set level: 10
imbal_test_imbal12345_rand_cifar10_vgg_Adam logger is ready
imbal_test with imbalance_ratio: [0.5 0.5 0.5 0.5 0.5 1.  1.  1.  1.  1. ]
parallel mode is False
initial count: cifar10: [0 1 2 3 4 5 6 7 8 9] class, each has [5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]
cifar10: [0 1 2 3 4 5 6 7 8 9] class, each has [2500 2500 2500 2500 2500 5000 5000 5000 5000 5000]
Epoch@@0
Files already downloaded and verified
TRAINING class wise acc@0 :: {0: [0.028], 1: [0.0048], 2: [0.0032], 3: [0.0016], 4: [0.0], 5: [0.0632], 6: [0.3418], 7: [0.2178], 8: [0.2312], 9: [0.1498]}
TRAIINING epoch@0 ::  acc@0.136  loss@2.466
[[  70    3    8   10    3  197  818  478  588  325]
 [   7   12    2    2    0  147  847  522  587  374]
 [  20    3    8    4    1  144  833  514  576  397]
 [  25    5    6    4    2  173  803  552  563  367]
 [   8    7    0    0    0  128  836  567  581  373]
 [  47    7    8    7    2  316 1624 1070 1186  733]
 [  19    4    4    5    0  270 1709 1046 1133  810]
 [  24    7    3    3    0  269 1673 1089 1169  763]
 [  15   18    5    4    1  300 1713 1065 1156  723]
 [   8    9    3    1    0  304 1668 1112 1146  749]]
TESTING class wise acc@0 :: {0: [0.066], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.013], 6: [0.0], 7: [0.987], 8: [0.0], 9: [0.0]}
TESTING epoch@0 ::  acc@0.107  loss@2.350
Epoch@@1
[[ 66   0   0   0   0  16   0 918   0   0]
 [  6   0   0   0   0   8   0 986   0   0]
 [ 15   0   0   0   0   5   0 980   0   0]
 [ 29   0   0   0   0   3   0 968   0   0]
 [  3   0   0   0   0   3   0 994   0   0]
 [ 20   0   0   0   0  13   0 967   0   0]
 [ 10   0   0   0   0   3   0 987   0   0]
 [  9   0   0   0   0   4   0 987   0   0]
 [  8   0   0   0   0   4   0 988   0   0]
 [ 10   0   0   0   0   4   0 986   0   0]]
TRAINING class wise acc@1 :: {0: [0.0368], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.2546], 6: [0.203], 7: [0.242], 8: [0.082], 9: [0.2078]}
TRAIINING epoch@1 ::  acc@0.134  loss@2.245
[[  92    0    1    0    0  706  428  566  213  494]
 [   7    0    0    0    0  650  490  616  208  529]
 [  25    0    0    0    0  633  471  614  224  533]
 [  24    0    1    0    0  631  497  572  213  562]
 [   8    0    0    0    0  625  499  602  223  543]
 [  52    0    0    0    0 1273  936 1166  442 1131]
 [  27    0    0    0    0 1201 1015 1234  500 1023]
 [  25    0    0    0    0 1310  977 1210  446 1032]
 [  24    0    0    0    0 1305  982 1184  410 1095]
 [   9    0    0    0    0 1293  964 1240  455 1039]]
TESTING class wise acc@1 :: {0: [0.057], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.044], 6: [0.0], 7: [0.0], 8: [0.014], 9: [0.973]}
TESTING epoch@1 ::  acc@0.109  loss@2.369
Epoch@@2
[[ 57   0   0   0   0  67   0   0  19 857]
 [  2   0   0   0   0  24   0   0   8 966]
 [ 11   0   0   0   0  24   0   0  12 953]
 [ 14   0   0   0   0  32   0   0  12 942]
 [  1   0   0   0   0  10   0   0   0 989]
 [  9   0   0   0   0  44   0   0  15 932]
 [  3   0   0   0   0  14   0   0   5 978]
 [  7   0   0   0   0  16   0   0   3 974]
 [  6   0   0   0   0  25   0   0  14 955]
 [  5   0   0   0   0  18   0   0   4 973]]
TRAINING class wise acc@2 :: {0: [0.0772], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.2398], 6: [0.1946], 7: [0.27], 8: [0.2922], 9: [0.2384]}
TRAIINING epoch@2 ::  acc@0.170  loss@2.178
[[ 193    0    0    0    0  424  233  435  703  512]
 [   9    0    0    0    0  598  326  562  393  612]
 [  58    0    0    0    0  588  448  632  246  528]
 [  19    0    0    0    0  663  437  645  153  583]
 [  14    0    0    0    0  618  477  608  193  590]
 [  35    0    1    0    0 1199  968 1280  357 1160]
 [  19    0    0    0    0 1294  973 1327  267 1120]
 [  13    0    0    0    0 1241  942 1350  281 1173]
 [ 111    0    0    0    0  970  499  841 1461 1118]
 [  28    0    0    0    0 1161  736 1203  680 1192]]
TESTING class wise acc@2 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.0], 6: [0.0], 7: [0.75], 8: [0.763], 9: [0.299]}
TESTING epoch@2 ::  acc@0.181  loss@2.118
Epoch@@3
[[  0   0   0   0   0   0   0 133 708 159]
 [  0   0   0   0   0   0   0 338 401 261]
 [  0   0   0   0   0   0   0 756 121 123]
 [  0   0   0   0   0   0   0 810  61 129]
 [  0   0   0   0   0   0   0 846  67  87]
 [  0   0   0   0   0   0   0 815  77 108]
 [  0   0   0   0   0   0   0 945  12  43]
 [  0   0   0   0   0   0   0 750  62 188]
 [  0   0   0   0   0   0   0  94 763 143]
 [  0   0   0   0   0   0   0 343 358 299]]
TRAINING class wise acc@3 :: {0: [0.002], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.004], 5: [0.2476], 6: [0.449], 7: [0.3984], 8: [0.6772], 9: [0.5558]}
TRAIINING epoch@3 ::  acc@0.311  loss@1.829
[[   5    0    0    0    0   56   60  266 1232  881]
 [   5    0    0    0    0   26   31  235  959 1244]
 [   1    0    0    0    3  503  826  616  119  432]
 [   0    0    0    0    6  581  878  673   32  330]
 [   0    0    0    0   10  575  992  540   76  307]
 [   0    0    0    0   16 1238 1984 1260   34  468]
 [   1    0    0    0   12 1264 2245 1110   26  342]
 [   0    0    0    0    7  838 1229 1992   73  861]
 [   4    0    0    0    0   35   53  230 3386 1292]
 [   3    0    0    0    0   72   96  702 1348 2779]]
TESTING class wise acc@3 :: {0: [0.0], 1: [0.0], 2: [0.0], 3: [0.0], 4: [0.0], 5: [0.005], 6: [0.943], 7: [0.213], 8: [0.385], 9: [0.547]}
TESTING epoch@3 ::  acc@0.209  loss@2.118
Epoch@@4
[[  0   0   0   0   0   7 210 193 145 445]
 [  0   0   0   0   0   6 115 282  52 545]
 [  0   0   0   0   0   1 865  89   1  44]
 [  0   0   0   0   0   5 893  78   1  23]
 [  0   0   0   0   0   2 905  58   1  34]
 [  0   0   0   0   0   5 923  60   0  12]
 [  0   0   0   0   0   0 943  46   0  11]
 [  0   0   0   0   0  10 735 213   0  42]
 [  0   0   0   0   0   0  75  92 385 448]
 [  0   0   0   0   0   5 104 310  34 547]]
TRAINING class wise acc@4 :: {0: [0.0256], 1: [0.0428], 2: [0.042], 3: [0.0008], 4: [0.0016], 5: [0.2976], 6: [0.6054], 7: [0.5592], 8: [0.7428], 9: [0.6868]}
TRAIINING epoch@4 ::  acc@0.393  loss@1.667
[[  64   16   66    0    2   93   60  213 1249  737]
 [  13  107   10    0    0   22   17  174  620 1537]
 [  39    1  105    2    7  453 1023  490  120  260]
 [  20    0   44    2    2  682  856  667   40  187]
 [  12    0   55    0    4  469 1150  543   37  230]
 [  22    1   73    2    0 1488 1624 1484   38  268]
 [  18    0   41    2    7  908 3027  776   30  191]
 [  14    0   28    0    2  811  635 2796   54  660]
 [  70   40   45    0    2   73   41  130 3714  885]
 [  16   83   23    0    3   78   38  609  716 3434]]
TESTING class wise acc@4 :: {0: [0.283], 1: [0.079], 2: [0.064], 3: [0.0], 4: [0.0], 5: [0.272], 6: [0.764], 7: [0.768], 8: [0.493], 9: [0.716]}
TESTING epoch@4 ::  acc@0.344  loss@1.756
Epoch@@5
[[283   6 123   0   0  89  36 139 165 159]
 [ 13  79   7   0   0  27   6  81  28 759]
 [ 27   0  64   0   0 242 417 205   7  38]
 [  6   0  20   0   0 268 226 403   3  74]
 [ 11   0  23   0   0 206 525 206   1  28]
 [  8   1   6   0   0 272 137 542   0  34]
 [  5   0   5   0   0 140 764  81   0   5]
 [  1   0   4   0   0 120  64 768   0  43]
 [196  32  30   0   0  23  16  47 493 163]
 [ 12  19   6   0   0  22   7 199  19 716]]
TRAINING class wise acc@5 :: {0: [0.1236], 1: [0.1004], 2: [0.124], 3: [0.01], 4: [0.0128], 5: [0.6258], 6: [0.7278], 7: [0.6626], 8: [0.8044], 9: [0.7766]}
TRAIINING epoch@5 ::  acc@0.504  loss@1.371
[[ 309   21  177   12   20  140   73   73 1389  286]
 [  41  251   13    1    5   45   11   78  295 1760]
 [ 134    0  310   25   42  771  791  170  205   52]
 [  56    0  132   25   24 1277  502  369   58   57]
 [  59    0  142   27   32  982  692  434   67   65]
 [  44    0  149   23   44 3129  428 1066   47   70]
 [  64    0  210   25   23  804 3639  129   60   46]
 [  31    1   61    6    7 1138  103 3313   44  296]
 [ 277   64  109    6    9   68   47   41 4022  357]
 [  55  211   28    1    8  102    8  366  338 3883]]
TESTING class wise acc@5 :: {0: [0.131], 1: [0.074], 2: [0.294], 3: [0.0], 4: [0.068], 5: [0.786], 6: [0.244], 7: [0.708], 8: [0.908], 9: [0.815]}
TESTING epoch@5 ::  acc@0.403  loss@1.527
Epoch@@6
[[131   3 102   0   9  39   0  20 601  95]
 [ 24  74   7   0   4   9   0  30 155 697]
 [ 89   0 294   0  73 402  22  43  64  13]
 [ 53   0 116   0  53 609  15  95  27  32]
 [ 51   0 160   0  68 502  16 163  20  20]
 [ 19   0  49   0  30 786   3 100   5   8]
 [ 38   0 173   0  38 438 244  40  17  12]
 [ 17   0  19   0   6 202   0 708  12  36]
 [ 33   7  15   0   2   8   0   4 908  23]
 [ 12  26   9   0   0   5   0  52  81 815]]
TRAINING class wise acc@6 :: {0: [0.2352], 1: [0.2784], 2: [0.1864], 3: [0.0172], 4: [0.092], 5: [0.7332], 6: [0.7806], 7: [0.7332], 8: [0.7998], 9: [0.807]}
TRAIINING epoch@6 ::  acc@0.568  loss@1.197
[[ 588   16  220   11   88   61   62   83 1110  261]
 [  56  696   15    2    8   11   12   23  269 1408]
 [ 217    0  466   42  214  637  603  162  120   39]
 [  62    0  180   43  123 1322  455  237   29   49]
 [ 124    0  302   56  230  842  453  400   28   65]
 [  55    0  137   46  164 3666  274  611    9   38]
 [  94    0  225   46   82  545 3903   43   36   26]
 [  50    0   47   14   74  919   41 3666   14  175]
 [ 428  102   99    2   38   31   41   32 3999  228]
 [ 105  297   26    4   39   29   11  268  186 4035]]
TESTING class wise acc@6 :: {0: [0.294], 1: [0.491], 2: [0.004], 3: [0.0], 4: [0.284], 5: [0.794], 6: [0.369], 7: [0.796], 8: [0.855], 9: [0.389]}
TESTING epoch@6 ::  acc@0.428  loss@1.571
Epoch@@7
[[294  25   0   0  57  26   0  48 524  26]
 [ 56 491   0   0  17  10   0  44 238 144]
 [214   1   4   1 275 310   8 141  39   7]
 [ 70   0   2   0 160 641  10 109   6   2]
 [ 83   0   0   0 284 326   7 286  12   2]
 [ 15   0   0   0  53 794   0 135   2   1]
 [ 61   0  17   3 233 296 369  17   4   0]
 [ 11   0   0   0  14 162   1 796   1  15]
 [ 92   7   0   0  20  10   0  11 855   5]
 [ 44 324   0   0  16  13   0 130  84 389]]
TRAINING class wise acc@7 :: {0: [0.3588], 1: [0.5752], 2: [0.2188], 3: [0.0308], 4: [0.2304], 5: [0.7444], 6: [0.8122], 7: [0.7892], 8: [0.8104], 9: [0.8294]}
TRAIINING epoch@7 ::  acc@0.626  loss@1.057
[[ 897   18  229    3  152   40   26   65  831  239]
 [  57 1438   11    0   15    5    9   12  215  738]
 [ 311    2  547   51  483  415  451  146   64   30]
 [  92    3  179   77  232 1291  409  173   11   33]
 [ 142    1  442   73  576  455  308  439   20   44]
 [  43    0  111   72  234 3722  228  561    4   25]
 [  83    1  221   63  158  339 4061   23   35   16]
 [  36    0   63    9  176  607   21 3946    4  138]
 [ 437  138   81    3   66   12   27   18 4052  166]
 [ 133  301   23    2   45   16   10  214  109 4147]]
TESTING class wise acc@7 :: {0: [0.582], 1: [0.638], 2: [0.43], 3: [0.005], 4: [0.397], 5: [0.76], 6: [0.721], 7: [0.735], 8: [0.6], 9: [0.912]}
TESTING epoch@7 ::  acc@0.578  loss@1.159
Epoch@@8
[[582   5 191   0  71   3   1   8  27 112]
 [ 48 638   3   0   4   1   1   3  10 292]
 [ 81   0 430   5 232 149  57  32   3  11]
 [ 34   0 105   5 197 495  71  61   2  30]
 [ 50   0 253   3 397 138  28 112   1  18]
 [ 10   0  25   0  88 760  22  83   0  12]
 [ 22   0 150   4  45  49 721   5   2   2]
 [  7   0   8   0  78 116   3 735   0  53]
 [286  33  16   0  13   4   0   1 600  47]
 [ 29  33   1   0  11   0   0   9   5 912]]
TRAINING class wise acc@8 :: {0: [0.4756], 1: [0.6824], 2: [0.2604], 3: [0.0324], 4: [0.264], 5: [0.7728], 6: [0.8294], 7: [0.8256], 8: [0.8368], 9: [0.859]}
TRAIINING epoch@8 ::  acc@0.664  loss@0.963
[[1189   19  266    2  165   25   15   66  537  216]
 [  85 1706   10    0    9    5    7    7  162  509]
 [ 341    0  651   70  497  345  378  131   55   32]
 [  62    0  201   81  245 1337  368  171   14   21]
 [ 181    1  530   68  660  360  231  430    9   30]
 [  35    0  123   77  221 3864  170  494    3   13]
 [  87    1  255   88  105  259 4147   18   34    6]
 [  34    0   46   12  173  487   19 4128    4   97]
 [ 411  107   75    0   45   13   41   10 4184  114]
 [ 164  225   38    2   39   13    4  134   86 4295]]
TESTING class wise acc@8 :: {0: [0.798], 1: [0.885], 2: [0.342], 3: [0.014], 4: [0.112], 5: [0.799], 6: [0.705], 7: [0.706], 8: [0.74], 9: [0.84]}
TESTING epoch@8 ::  acc@0.594  loss@1.204
Epoch@@9
[[798  20  32   0  11   8   2   4  51  74]
 [ 29 885   1   0   0   1   0   1  11  72]
 [314   1 342  17  42 173  40  48   5  18]
 [131   3 123  14  42 562  47  45   2  31]
 [214   2 318   9 112 113  11 190   3  28]
 [ 43   1  44   2  28 799   3  67   0  13]
 [ 65   1 106  23   2  89 705   1   5   3]
 [ 56   2  12   0  17  98   0 706   0 109]
 [178  54   9   0   4   2   1   0 740  12]
 [ 44 105   1   0   0   0   0   2   8 840]]
TRAINING class wise acc@9 :: {0: [0.5668], 1: [0.7568], 2: [0.2804], 3: [0.0764], 4: [0.3584], 5: [0.7854], 6: [0.8544], 7: [0.8466], 8: [0.8468], 9: [0.879]}
TRAIINING epoch@9 ::  acc@0.698  loss@0.872
[[1417   18  189    4  179   14   10   63  428  178]
 [  86 1892    7    1    7    2    4    8  103  390]
 [ 353    0  701   99  553  265  354  121   39   15]
 [  66    0  190  191  284 1257  325  159    6   22]
 [ 174    0  488  114  896  246  159  393    8   22]
 [  25    0   97  150  218 3927  143  423    3   14]
 [  50    0  239   94   80  202 4272   17   39    7]
 [  35    0   38   17  205  378   12 4233    2   80]
 [ 397  106   86    5   26    2   49   13 4234   82]
 [ 166  197   17    1   55    8    3   99   59 4395]]
TESTING class wise acc@9 :: {0: [0.747], 1: [0.851], 2: [0.377], 3: [0.068], 4: [0.423], 5: [0.374], 6: [0.673], 7: [0.694], 8: [0.881], 9: [0.848]}
TESTING epoch@9 ::  acc@0.594  loss@1.214
Epoch@@10
[[747  27  20   0   9   0   0   2 143  52]
 [ 28 851   4   0   3   1   0   0  42  71]
 [362   0 377   9 188   5  25   4  22   8]
 [110   1 298  68 329  89  63  10  14  18]
 [242   0 262   1 423   1   4  46   6  15]
 [ 33   0 106 111 319 374   9  39   5   4]
 [ 40   0 245   6  13   1 673   0  22   0]
 [ 53   1  18   3 155   9   1 694   0  66]
 [ 70  32   5   0   1   0   1   0 881  10]
 [ 49  85   3   0   2   0   0   0  13 848]]
TRAINING class wise acc@10 :: {0: [0.6192], 1: [0.796], 2: [0.3052], 3: [0.096], 4: [0.424], 5: [0.8072], 6: [0.8628], 7: [0.8678], 8: [0.8614], 9: [0.891]}
TRAIINING epoch@10 ::  acc@0.721  loss@0.804
[[1548   26  194    7  189    6   15   38  322  155]
 [  82 1990    8    0    7    3    6    6   91  307]
 [ 300    1  763  119  592  245  318  111   35   16]
 [  48    0  207  240  290 1259  286  145    7   18]
 [ 138    1  506   99 1060  195  117  364    9   11]
 [  14    0  102  180  236 4036  102  324    0    6]
 [  46    1  253  111   69  159 4314   14   27    6]
 [  31    0   26   28  217  299    4 4339    3   53]
 [ 401   96   62    7   28    5   38    7 4307   49]
 [ 170  182   10    3   44    7    2   82   45 4455]]
TESTING class wise acc@10 :: {0: [0.573], 1: [0.513], 2: [0.48], 3: [0.001], 4: [0.319], 5: [0.545], 6: [0.918], 7: [0.859], 8: [0.876], 9: [0.85]}
TESTING epoch@10 ::  acc@0.593  loss@1.197
Epoch@@11
[[573   1 110   0  42   0  13  21 207  33]
 [ 91 513   5   0   6   0   5   2  27 351]
 [ 91   0 480   1 135  35 186  38  29   5]
 [ 28   2 260   1 162 200 272  60  11   4]
 [ 45   0 401   0 319   9  93 119  12   2]
 [ 12   0  98   6 100 545 127 105   4   3]
 [  6   0  51   0  13   7 918   2   3   0]
 [  6   0  24   0  76  18  11 859   3   3]
 [ 62   3  14   0   6   1  20   2 876  16]
 [ 75   2   9   0  12   0   3  40   9 850]]
TRAINING class wise acc@11 :: {0: [0.6468], 1: [0.814], 2: [0.3336], 3: [0.1172], 4: [0.43], 5: [0.8102], 6: [0.8682], 7: [0.8768], 8: [0.877], 9: [0.8948]}
TRAIINING epoch@11 ::  acc@0.733  loss@0.770
[[1617   18  182    7  180   10    7   29  282  168]
 [  78 2035   10    1   13    1    3    3   77  279]
 [ 293    0  834  138  649  193  281   70   29   13]
 [  39    0  207  293  290 1241  279  125    9   17]
 [ 158    0  534  128 1075  192   92  303    4   14]
 [  20    0  103  205  213 4051   91  305    3    9]
 [  44    0  243  118   79  128 4341    9   34    4]
 [  21    0   31   20  219  270    2 4384    1   52]
 [ 336   89   64    5   19    4   37    8 4385   53]
 [ 187  164   12    0   39    5    4   73   42 4474]]
TESTING class wise acc@11 :: {0: [0.799], 1: [0.884], 2: [0.32], 3: [0.159], 4: [0.472], 5: [0.796], 6: [0.705], 7: [0.785], 8: [0.848], 9: [0.869]}
TESTING epoch@11 ::  acc@0.664  loss@1.043
Epoch@@12
[[799  17  37   2  35   1   3   5  62  39]
 [ 25 884   1   0   4   1   1   0  14  70]
 [187   0 320 109 184 104  43  23  12  18]
 [ 42   3 101 159 137 460  33  30   5  30]
 [ 97   2 136  73 472  95  11 100   4  10]
 [ 11   3  14  65  53 796   4  39   1  14]
 [ 26   2  96  95  19  37 705   4  12   4]
 [ 18   0   4   8  58  69   0 785   1  57]
 [ 99  28   7   2   3   1   2   2 848   8]
 [ 41  69   4   0   3   0   0   4  10 869]]
TRAINING class wise acc@12 :: {0: [0.7032], 1: [0.82], 2: [0.3448], 3: [0.1288], 4: [0.5152], 5: [0.8178], 6: [0.8818], 7: [0.8868], 8: [0.8874], 9: [0.9018]}
TRAIINING epoch@12 ::  acc@0.751  loss@0.725
[[1758   21  140   12  178    3   10   20  236  122]
 [  76 2050    5    2    8    1    2    1   65  290]
 [ 299    0  862  177  625  152  281   68   24   12]
 [  32    0  219  322  301 1206  266  136    5   13]
 [ 125    0  424  109 1288  159   76  298    5   16]
 [  19    0   97  244  198 4089   76  270    1    6]
 [  42    1  215  120   58  107 4409    6   38    4]
 [  15    0   26   18  218  256    3 4434    1   29]
 [ 299   79   61    1   21    2   47    3 4437   50]
 [ 177  165   11    2   46    6    1   56   27 4509]]
TESTING class wise acc@12 :: {0: [0.697], 1: [0.41], 2: [0.192], 3: [0.145], 4: [0.517], 5: [0.767], 6: [0.873], 7: [0.908], 8: [0.805], 9: [0.883]}
TESTING epoch@12 ::  acc@0.620  loss@1.083
Epoch@@13
[[697   0  53   3 133   0  13  33  23  45]
 [ 57 410   2   0  10   0   2   6  18 495]
 [ 64   0 192  85 339  86 141  85   6   2]
 [ 13   0  41 145 131 403 161  99   4   3]
 [ 14   0  35  36 517  39  21 337   1   0]
 [  5   0  10  74  31 767  27  86   0   0]
 [  2   0  31  52  22   7 873  10   3   0]
 [  1   0   2   9  31  42   3 908   0   4]
 [126   5  12   1   9   0  13   5 805  24]
 [ 27   0   5   2  18   1   3  54   7 883]]
TRAINING class wise acc@13 :: {0: [0.7104], 1: [0.854], 2: [0.4152], 3: [0.176], 4: [0.5672], 5: [0.822], 6: [0.894], 7: [0.8956], 8: [0.9008], 9: [0.9164]}
TRAIINING epoch@13 ::  acc@0.772  loss@0.670
[[1776   14  186    3  158    5    8   11  218  121]
 [  71 2135    4    1    7    2    1    5   54  220]
 [ 266    2 1038  181  573  126  229   58   19    8]
 [  34    0  217  440  289 1212  207   88    3   10]
 [ 117    1  410  106 1418  118   60  247    4   19]
 [   9    0   79  283  203 4110   74  239    2    1]
 [  35    0  200  130   50   89 4470    1   22    3]
 [  20    0   16   20  216  190    2 4478    3   55]
 [ 287   57   51    3   23    2   38    3 4504   32]
 [ 140  141    7    2   56    2    0   56   14 4582]]
TESTING class wise acc@13 :: {0: [0.388], 1: [0.599], 2: [0.35], 3: [0.0], 4: [0.257], 5: [0.928], 6: [0.69], 7: [0.903], 8: [0.584], 9: [0.735]}
TESTING epoch@13 ::  acc@0.543  loss@1.548
Epoch@@14
[[388   3 184   0 213 107   3  62  10  30]
 [ 52 599   6   0  28  16   1  45   6 247]
 [  9   0 350   0 115 404  30  91   0   1]
 [  1   0  12   0  15 887  15  69   0   1]
 [  1   0  43   0 257 389   8 301   0   1]
 [  0   0   4   0   5 928   0  63   0   0]
 [  0   0  36   0   6 260 690   8   0   0]
 [  0   0   0   0   6  91   0 903   0   0]
 [133   7 129   0  49  34  30  25 584   9]
 [ 13  11   4   0  10  18   0 207   2 735]]
TRAINING class wise acc@14 :: {0: [0.7468], 1: [0.8596], 2: [0.4616], 3: [0.2144], 4: [0.5948], 5: [0.825], 6: [0.8938], 7: [0.9036], 8: [0.8998], 9: [0.9212]}
TRAIINING epoch@14 ::  acc@0.784  loss@0.640
[[1867   19  171    4  139    6    9   10  174  101]
 [  60 2149    6    0    8    3    2    0   61  211]
 [ 248    0 1154  181  523  112  230   28   17    7]
 [  26    0  243  536  298 1115  182   88    3    9]
 [ 104    0  351  133 1487  122   34  258    1   10]
 [  15    1   80  298  196 4125   68  213    1    3]
 [  26    1  215  134   51   77 4469    4   21    2]
 [   8    0   13   22  231  170    1 4518    1   36]
 [ 292   60   65    4   17    3   33    1 4499   26]
 [ 149  129   13    1   36    1    1   51   13 4606]]
TESTING class wise acc@14 :: {0: [0.653], 1: [0.968], 2: [0.451], 3: [0.168], 4: [0.541], 5: [0.742], 6: [0.814], 7: [0.703], 8: [0.828], 9: [0.802]}
TESTING epoch@14 ::  acc@0.667  loss@1.206
Epoch@@15
[[653 120  43   2  26   1   0   4  69  82]
 [  6 968   0   1   2   0   0   0   3  20]
 [137   9 451  65 135  49  96   7  25  26]
 [ 43  14 145 168 136 334  98  23   5  34]
 [ 84   7 198  42 541  32  23  43   7  23]
 [ 10   4  33  73  61 742  20  40   0  17]
 [ 29  10  78  17  16   3 814   1  25   7]
 [ 14   4   9   9  86  64   2 703   0 109]
 [ 35 113   5   2   1   0   2   0 828  14]
 [ 13 173   1   0   2   0   0   1   8 802]]
TRAINING class wise acc@15 :: {0: [0.7492], 1: [0.876], 2: [0.5148], 3: [0.268], 4: [0.6508], 5: [0.8334], 6: [0.896], 7: [0.9074], 8: [0.9062], 9: [0.9256]}
TRAIINING epoch@15 ::  acc@0.800  loss@0.606
[[1873   15  158    5  157    3    5   15  178   91]
 [  50 2190    4    2    8    0    0    1   56  189]
 [ 193    1 1287  216  459   74  208   34   22    6]
 [  27    0  230  670  258 1045  184   69    7   10]
 [  88    0  302  103 1627  108   31  229    3    9]
 [   7    0   66  333  171 4167   47  204    1    4]
 [  18    1  221  146   45   51 4480    3   34    1]
 [   6    0   16   25  218  159    1 4537    0   38]
 [ 264   73   60    0   10    2   34    1 4531   25]
 [ 129  135    8    2   44    1    2   38   13 4628]]
TESTING class wise acc@15 :: {0: [0.731], 1: [0.918], 2: [0.313], 3: [0.13], 4: [0.521], 5: [0.83], 6: [0.958], 7: [0.797], 8: [0.843], 9: [0.898]}
TESTING epoch@15 ::  acc@0.694  loss@1.053
Epoch@@16
[[731  27  43   5  39   4  13   4  55  79]
 [ 14 918   1   0   1   1   1   0   9  55]
 [120   0 313  85 111  77 260  15  13   6]
 [ 29   1  24 130  96 496 185  21   1  17]
 [ 68   0 108  84 521  54  89  65   1  10]
 [  9   0  11  48  37 830  32  28   0   5]
 [  4   0   5  10   5  12 958   2   4   0]
 [  9   0   7   4  65  73  15 797   1  29]
 [ 71  44   4   4   4   1  10   0 843  19]
 [ 15  70   1   2   3   1   4   4   2 898]]
TRAINING class wise acc@16 :: {0: [0.7732], 1: [0.8908], 2: [0.5536], 3: [0.2876], 4: [0.6984], 5: [0.8334], 6: [0.9028], 7: [0.9162], 8: [0.9178], 9: [0.9366]}
TRAIINING epoch@16 ::  acc@0.814  loss@0.554
[[1933    5  150    7  142    5    2    7  162   87]
 [  42 2227    6    1    5    0    0    0   53  166]
 [ 186    0 1384  194  443   67  173   21   23    9]
 [  21    0  206  719  244 1075  172   55    3    5]
 [  84    0  240   99 1746   98   14  205    2   12]
 [   5    0   52  396  162 4167   51  160    2    5]
 [  14    0  207  159   33   50 4514    2   21    0]
 [   5    0   11   21  232  122    0 4581    0   28]
 [ 240   62   60    2   10    0   17    1 4589   19]
 [ 128  101    3    2   34    0    1   40    8 4683]]
TESTING class wise acc@16 :: {0: [0.834], 1: [0.94], 2: [0.564], 3: [0.341], 4: [0.472], 5: [0.581], 6: [0.85], 7: [0.771], 8: [0.902], 9: [0.779]}
TESTING epoch@16 ::  acc@0.703  loss@1.042
Epoch@@17
[[834  23  28   1  10   0   1   0  82  21]
 [ 18 940   0   1   2   0   1   0  19  19]
 [234   1 564  32  75  12  49  10  17   6]
 [ 51   4 249 341 121 115  87  18   6   8]
 [186   1 257  15 472   1  26  34   4   4]
 [ 10   1  89 177  80 581  16  42   1   3]
 [ 14   1  90  15   7   0 850   2  21   0]
 [ 21   0  15  12 134  15   2 771   0  30]
 [ 49  28  10   0   2   1   2   0 902   6]
 [ 45 151   6   1   4   0   1   2  11 779]]
TRAINING class wise acc@17 :: {0: [0.7968], 1: [0.9016], 2: [0.6316], 3: [0.3788], 4: [0.75], 5: [0.8424], 6: [0.902], 7: [0.9268], 8: [0.9258], 9: [0.9392]}
TRAIINING epoch@17 ::  acc@0.835  loss@0.506
[[1992   13  147    3  116    3    2    7  135   82]
 [  45 2254    1    0    7    0    1    0   42  150]
 [ 180    0 1579  193  311   52  142   15   23    5]
 [  14    0  195  947  213  902  164   57    1    7]
 [  67    0  192  119 1875   70   18  153    2    4]
 [   2    0   49  425  142 4212   32  138    0    0]
 [  10    0  226  163   26   45 4510    2   18    0]
 [   1    0    8   18  188  124    0 4634    0   27]
 [ 200   58   79    5   11    0   10    0 4629    8]
 [ 108  111    5    0   45    1    1   20   13 4696]]
TESTING class wise acc@17 :: {0: [0.591], 1: [0.826], 2: [0.32], 3: [0.211], 4: [0.69], 5: [0.697], 6: [0.709], 7: [0.846], 8: [0.944], 9: [0.936]}
TESTING epoch@17 ::  acc@0.677  loss@1.285
Epoch@@18
[[591  17  11   2  66   0   0   3 206 104]
 [ 10 826   0   0   0   1   0   0  25 138]
 [101   3 320  47 330  36  29  34  55  45]
 [ 24   3  65 211 220 288  25  83  27  54]
 [ 31   3  36  36 690  24   3 130  17  30]
 [  6   1  12  52  87 697   3 114   7  21]
 [ 16   4  81  64  52  20 709   6  34  14]
 [  3   0   0   1  51  14   1 846   4  80]
 [  9  19   2   0   6   0   1   1 944  18]
 [ 15  30   1   0   2   0   0   0  16 936]]
TRAINING class wise acc@18 :: {0: [0.8136], 1: [0.9036], 2: [0.6244], 3: [0.4192], 4: [0.7688], 5: [0.8396], 6: [0.9132], 7: [0.9256], 8: [0.9314], 9: [0.9428]}
TRAIINING epoch@18 ::  acc@0.842  loss@0.480
[[2034    7  142    5  105    2    2    6  130   67]
 [  45 2259    1    0    4    0    0    1   32  158]
 [ 160    0 1561  197  333   49  152   17   25    6]
 [  17    0  173 1048  218  865  133   43    2    1]
 [  65    0  175  111 1922   60   12  149    0    6]
 [   4    0   45  444  131 4198   40  137    0    1]
 [   6    0  192  159   22   31 4566    1   21    2]
 [   3    0    6   18  205  115    1 4628    0   24]
 [ 189   48   73    2    7    0   10    2 4657   12]
 [  95  120    2    0   40    0    0   25    4 4714]]
TESTING class wise acc@18 :: {0: [0.847], 1: [0.881], 2: [0.505], 3: [0.244], 4: [0.478], 5: [0.83], 6: [0.812], 7: [0.805], 8: [0.887], 9: [0.889]}
TESTING epoch@18 ::  acc@0.718  loss@1.032
Epoch@@19
[[847  15  14   0  15   3   2   3  50  51]
 [  8 881   0   0   0   2   0   0  11  98]
 [171   0 505  89  90  64  44  20  11   6]
 [ 46   4  84 244  77 448  51  25  11  10]
 [ 68   1  91  92 478 128  14 116   4   8]
 [ 10   0  22  62  35 830  12  24   4   1]
 [ 13   0  53  59  11  38 812   2  10   2]
 [ 10   0  10  10  40 103   0 805   1  21]
 [ 65  29   6   1   1   0   1   2 887   8]
 [ 26  49   3   0   6   2   0  10  15 889]]
TRAINING class wise acc@19 :: {0: [0.8304], 1: [0.9176], 2: [0.6852], 3: [0.472], 4: [0.7904], 5: [0.8484], 6: [0.919], 7: [0.9334], 8: [0.9378], 9: [0.9472]}
TRAIINING epoch@19 ::  acc@0.858  loss@0.436
[[2076    9  113    4  105    0    1    4  112   76]
 [  37 2294    2    0    3    0    0    0   29  135]
 [ 150    0 1713  188  283   21  107   15   20    3]
 [  10    0  163 1180  205  800  114   22    2    4]
 [  57    0  148  126 1976   46    6  128    2   11]
 [   2    0   36  453  103 4242   26  136    0    2]
 [   6    0  182  155   15   31 4595    2   13    1]
 [   3    0    3   16  177  108    0 4667    0   26]
 [ 162   52   69    2    4    0   14    0 4689    8]
 [  98   96    3    0   33    0    0   28    6 4736]]
TESTING class wise acc@19 :: {0: [0.589], 1: [0.794], 2: [0.643], 3: [0.401], 4: [0.656], 5: [0.777], 6: [0.913], 7: [0.825], 8: [0.872], 9: [0.874]}
TESTING epoch@19 ::  acc@0.734  loss@1.081
Epoch@@20
[[589   5 203  18  53   3  11  11  64  43]
 [ 33 794  16   7   6   2   3   0  19 120]
 [ 26   0 643  76  95  47  93  18   1   1]
 [  6   1  75 401  76 266 154  19   0   2]
 [ 11   0 160  68 656  16  33  53   2   1]
 [  1   0  25 124  19 777  35  19   0   0]
 [  0   0  44  32   4   2 913   3   2   0]
 [  2   0  10  33  55  64  10 825   0   1]
 [ 39  10  41   6   8   2  11   3 872   8]
 [ 18  32  11   7  20   4   6  21   7 874]]
TRAINING class wise acc@20 :: {0: [0.844], 1: [0.9128], 2: [0.7128], 3: [0.5152], 4: [0.8188], 5: [0.8526], 6: [0.9224], 7: [0.935], 8: [0.9458], 9: [0.952]}
TRAIINING epoch@20 ::  acc@0.868  loss@0.416
[[2110   12  111    7   78    1    2    5  116   58]
 [  44 2282    2    0    3    0    1    0   29  139]
 [ 132    0 1782  189  236   21  107   12   18    3]
 [  11    0  148 1288  185  717  114   33    0    4]
 [  48    0  132  110 2047   38    3  116    0    6]
 [   4    0   31  459  102 4263   25  112    0    4]
 [   9    1  174  142   17   25 4612    0   17    3]
 [   1    0    4   17  176  104    0 4675    1   22]
 [ 158   33   49    2    6    1   14    0 4729    8]
 [  91   86    1    2   42    0    0   14    4 4760]]
TESTING class wise acc@20 :: {0: [0.819], 1: [0.933], 2: [0.435], 3: [0.578], 4: [0.496], 5: [0.779], 6: [0.857], 7: [0.817], 8: [0.838], 9: [0.835]}
TESTING epoch@20 ::  acc@0.739  loss@1.074
Epoch@@21
[[819  34  34  10  28   1   2   0  38  34]
 [ 12 933   1   3   5   0   1   0   4  41]
 [106   2 435 191  81  46  99  16  20   4]
 [ 19   1  30 578  69 230  41  17   5  10]
 [ 46   1  68 209 496  34  42  97   3   4]
 [  7   0   4 150  28 779   4  26   1   1]
 [  4   2  20  90   5  15 857   0   6   1]
 [ 11   0   5  43  53  52   2 817   0  17]
 [ 65  60  12   3   6   1   2   1 838  12]
 [ 24 105   6   5  13   0   2   3   7 835]]
TRAINING class wise acc@21 :: {0: [0.8628], 1: [0.9208], 2: [0.734], 3: [0.554], 4: [0.8184], 5: [0.854], 6: [0.9282], 7: [0.9488], 8: [0.943], 9: [0.9552]}
TRAIINING epoch@21 ::  acc@0.877  loss@0.392
[[2157   11   96    3   69    1    1    4   91   67]
 [  44 2302    0    0    1    0    0    0   31  122]
 [ 111    0 1835  183  232   24   85    6   19    5]
 [   8    0  152 1385  187  642   87   33    2    4]
 [  51    0  128  110 2046   33    9  118    1    4]
 [   0    0   18  473   98 4270   23  117    0    1]
 [   5    1  156  147   12   17 4641    1   15    5]
 [   2    0    2   18  123   90    0 4744    0   21]
 [ 164   42   52    2    4    0   14    0 4715    7]
 [  75   86    3    0   39    0    1   19    1 4776]]
TESTING class wise acc@21 :: {0: [0.601], 1: [0.726], 2: [0.61], 3: [0.606], 4: [0.634], 5: [0.651], 6: [0.915], 7: [0.657], 8: [0.931], 9: [0.773]}
TESTING epoch@21 ::  acc@0.710  loss@1.305
Epoch@@22
[[601   1 141  16  27   2   9   1 194   8]
 [ 73 726  11   2   3   0   2   0 127  56]
 [ 47   0 610 119  66  25 112   3  16   2]
 [  9   0  96 606  58 115  96   5  10   5]
 [  9   0 144 144 634   6  49   8   6   0]
 [  1   0  28 256  32 651  22  10   0   0]
 [  2   0  26  41   3   9 915   0   4   0]
 [ 10   0  22  58 187  50   6 657   3   7]
 [ 26   1  21   4   5   1   6   0 931   5]
 [101  29  17  12  29   0   8   0  31 773]]
TRAINING class wise acc@22 :: {0: [0.8636], 1: [0.9288], 2: [0.7584], 3: [0.5784], 4: [0.8492], 5: [0.8596], 6: [0.9358], 7: [0.9444], 8: [0.9472], 9: [0.9608]}
TRAIINING epoch@22 ::  acc@0.885  loss@0.356
[[2159   16  100    4   73    0    0    3   89   56]
 [  36 2322    0    0    1    0    0    0   35  106]
 [ 116    0 1896  174  179   14   87    5   25    4]
 [   8    0  129 1446  152  634   95   34    1    1]
 [  38    0   97   91 2123   35    4  103    0    9]
 [   1    0   12  475   88 4298   21  105    0    0]
 [   4    0  145  127   14   20 4679    0   11    0]
 [   2    0    3   11  144  102    0 4722    0   16]
 [ 148   36   65    0    3    0    8    0 4736    4]
 [  68   78    0    2   22    0    0   21    5 4804]]
TESTING class wise acc@22 :: {0: [0.391], 1: [0.669], 2: [0.528], 3: [0.355], 4: [0.759], 5: [0.857], 6: [0.866], 7: [0.875], 8: [0.819], 9: [0.884]}
TESTING epoch@22 ::  acc@0.700  loss@1.293
Epoch@@23
[[391   1 298  40  90  14  28  19  88  31]
 [ 48 669  16   1   8   7   5   2  32 212]
 [ 16   0 528 138 134  69  78  30   4   3]
 [  4   1  43 355  62 419  67  40   1   8]
 [  2   0  72  65 759  36  10  54   0   2]
 [  0   0   8  52  26 857  11  46   0   0]
 [  1   0  31  49  10  37 866   4   2   0]
 [  0   0   6   9  47  56   3 875   0   4]
 [ 26   2  83   4   8   3  42   5 819   8]
 [ 35  10   7   5  24   7   6  12  10 884]]
TRAINING class wise acc@23 :: {0: [0.8712], 1: [0.9336], 2: [0.772], 3: [0.6248], 4: [0.858], 5: [0.8722], 6: [0.929], 7: [0.9492], 8: [0.9532], 9: [0.9618]}
TRAIINING epoch@23 ::  acc@0.893  loss@0.342
[[2178    8  116    3   60    0    1    2   81   51]
 [  39 2334    2    0    2    0    0    0   24   99]
 [ 115    0 1930  155  177   12   84    2   22    3]
 [   4    0  118 1562  160  522  112   21    1    0]
 [  45    0   93   88 2145   31    5   88    2    3]
 [   0    0   14  437   77 4361   20   90    0    1]
 [   5    0  146  170    9   16 4645    1    7    1]
 [   1    0    1   15  119   92    0 4746    0   26]
 [ 131   37   51    1    0    0   11    0 4766    3]
 [  72   62    3    0   29    0    0   22    3 4809]]
TESTING class wise acc@23 :: {0: [0.698], 1: [0.714], 2: [0.364], 3: [0.124], 4: [0.623], 5: [0.96], 6: [0.803], 7: [0.756], 8: [0.824], 9: [0.79]}
TESTING epoch@23 ::  acc@0.666  loss@1.861
Epoch@@24
[[698   4  37  33 130  29   5   8  36  20]
 [ 41 714   6  10  33  30   5   2  28 131]
 [ 60   0 364 159 162 195  38  17   4   1]
 [  2   0   7 124  45 786  30   6   0   0]
 [  4   0  16  67 623 207  31  51   1   0]
 [  0   0   0  12  10 960   4  14   0   0]
 [  0   0  15  54   7 119 803   0   2   0]
 [  1   0   0   4  37 200   1 756   0   1]
 [ 65   5  37  15  13  12  15   3 824  11]
 [ 22  29   7  10  42  36   7  48   9 790]]
TRAINING class wise acc@24 :: {0: [0.8884], 1: [0.9344], 2: [0.7756], 3: [0.662], 4: [0.8588], 5: [0.875], 6: [0.9392], 7: [0.9526], 8: [0.9546], 9: [0.962]}
TRAIINING epoch@24 ::  acc@0.899  loss@0.321
[[2221   10   93    3   45    0    0    1   84   43]
 [  35 2336    0    0    2    0    0    0   31   96]
 [ 116    0 1939  156  162    7   83    8   24    5]
 [   6    0   97 1655  142  495   81   23    0    1]
 [  45    0   77  103 2147   27    3   85    0   13]
 [   1    0   10  440   61 4375   16   96    0    1]
 [   3    0  151  110   10   16 4696    1   11    2]
 [   0    0    1   14  120   87    0 4763    0   15]
 [ 125   37   49    1    1    0    7    1 4773    6]
 [  70   60    1    0   38    1    0   18    2 4810]]
TESTING class wise acc@24 :: {0: [0.71], 1: [0.773], 2: [0.58], 3: [0.44], 4: [0.724], 5: [0.803], 6: [0.843], 7: [0.898], 8: [0.869], 9: [0.944]}
TESTING epoch@24 ::  acc@0.758  loss@1.028
Epoch@@25
[[710   4  59   7  59   1   6   7  49  98]
 [ 10 773   1   2   2   1   2   0   9 200]
 [ 72   0 580  77 145  37  55  19   8   7]
 [  7   1  69 440 108 256  59  39   4  17]
 [ 11   0  52  54 724  30   9 109   4   7]
 [  2   0  18  88  39 803   2  46   0   2]
 [  3   0  44  56  31  15 843   5   1   2]
 [  2   0   2   8  40  42   0 898   0   8]
 [ 63  15  14   1   7   2   4   1 869  24]
 [ 11  18   6   3   8   0   0   4   6 944]]
TRAINING class wise acc@25 :: {0: [0.8904], 1: [0.936], 2: [0.7956], 3: [0.6728], 4: [0.8872], 5: [0.8806], 6: [0.938], 7: [0.9576], 8: [0.9568], 9: [0.968]}
TRAIINING epoch@25 ::  acc@0.906  loss@0.301
[[2226   12   79    2   52    0    0    3   73   53]
 [  36 2340    1    0    3    0    0    0   25   95]
 [ 112    0 1989  135  137   15   85    4   18    5]
 [   8    0  112 1682  109  491   77   18    0    3]
 [  34    0   70   71 2218   19    3   81    0    4]
 [   0    0    4  433   49 4403   21   90    0    0]
 [   2    1  129  142    7   15 4690    1   12    1]
 [   0    0    1   15  106   73    0 4788    0   17]
 [ 127   30   45    1    3    0    7    0 4784    3]
 [  66   60    1    0   14    1    0   16    2 4840]]
TESTING class wise acc@25 :: {0: [0.835], 1: [0.903], 2: [0.669], 3: [0.411], 4: [0.659], 5: [0.611], 6: [0.942], 7: [0.763], 8: [0.913], 9: [0.795]}
TESTING epoch@25 ::  acc@0.750  loss@1.260
Epoch@@26
[[835  16  31   1  13   1   9   1  76  17]
 [ 32 903   3   2   1   1   3   0  12  43]
 [108   2 669  30  53  15 105   6  10   2]
 [ 38   2 189 411  76  66 197  11   5   5]
 [ 63   1 145  43 659   7  50  26   4   2]
 [ 12   0  58 200  40 611  53  24   2   0]
 [  6   0  35   8   5   1 942   0   3   0]
 [ 17   0  16  26 115  37  11 763   1  14]
 [ 40  19  10   2   4   0   8   0 913   4]
 [ 42 115   6   5   7   0   8   4  18 795]]
TRAINING class wise acc@26 :: {0: [0.8936], 1: [0.9452], 2: [0.8268], 3: [0.6984], 4: [0.886], 5: [0.8904], 6: [0.9502], 7: [0.962], 8: [0.9576], 9: [0.9654]}
TRAIINING epoch@26 ::  acc@0.913  loss@0.281
[[2234    8   80    4   48    1    0    0   75   50]
 [  27 2363    0    0    4    0    0    0   19   87]
 [  78    1 2067  153  116    3   58    5   18    1]
 [   4    0  112 1746  131  424   67   15    1    0]
 [  32    0   81   79 2215   25    3   59    0    6]
 [   2    0    7  392   54 4452    8   85    0    0]
 [   3    0  108  108    5   14 4751    0   11    0]
 [   1    0    1   10   87   72    0 4810    0   19]
 [ 122   31   41    1    4    0   11    0 4788    2]
 [  57   64    3    1   28    0    0   20    0 4827]]
TESTING class wise acc@26 :: {0: [0.89], 1: [0.837], 2: [0.653], 3: [0.504], 4: [0.664], 5: [0.527], 6: [0.929], 7: [0.79], 8: [0.85], 9: [0.903]}
TESTING epoch@26 ::  acc@0.755  loss@1.348
Epoch@@27
[[890   5  41   0   8   1   3   0  23  29]
 [ 33 837   2   1   1   0   1   0  15 110]
 [ 98   0 653  47  62  10 111   9   5   5]
 [ 42   2 129 504  81  45 170  14   4   9]
 [ 41   0 140  51 664   4  64  30   3   3]
 [ 11   1  41 277  55 527  63  24   1   0]
 [  9   0  43  14   1   1 929   0   3   0]
 [ 10   0  21  27 103  20   9 790   0  20]
 [101  12  14   4   1   1   4   1 850  12]
 [ 39  32   7   2   7   0   3   1   6 903]]
TRAINING class wise acc@27 :: {0: [0.9064], 1: [0.9532], 2: [0.8372], 3: [0.7388], 4: [0.9008], 5: [0.9028], 6: [0.956], 7: [0.9658], 8: [0.962], 9: [0.9734]}
TRAIINING epoch@27 ::  acc@0.924  loss@0.246
[[2266   11   84    0   39    0    0    2   61   37]
 [  24 2383    2    1    0    0    0    0   25   65]
 [  85    0 2093  129  102    7   62    6   15    1]
 [   6    0   93 1847   87  400   52   13    1    1]
 [  18    0   72   57 2252   18    0   78    0    5]
 [   0    0    2  369   34 4514    8   72    0    1]
 [   0    1   98   98    5    7 4780    0   11    0]
 [   1    0    0    9   94   58    0 4829    0    9]
 [ 106   43   32    1    0    0    6    0 4810    2]
 [  57   47    1    0   18    0    0   10    0 4867]]
TESTING class wise acc@27 :: {0: [0.772], 1: [0.891], 2: [0.717], 3: [0.583], 4: [0.602], 5: [0.703], 6: [0.829], 7: [0.857], 8: [0.895], 9: [0.879]}
TESTING epoch@27 ::  acc@0.773  loss@1.066
Epoch@@28
[[772   8  66   9  38   2   5   3  45  52]
 [ 15 891   2   1   2   0   0   1   9  79]
 [ 70   0 717  72  57  17  31  24   8   4]
 [ 17   2 122 583  52 138  40  35   5   6]
 [ 25   1 130 123 602  23  18  72   2   4]
 [  5   0  36 180  22 703  10  42   0   2]
 [  8   1  63  61  13  16 829   3   4   2]
 [  8   0  11  22  43  43   1 857   0  15]
 [ 48  20  12   3   5   0   2   2 895  13]
 [ 28  55   3   3  12   1   1  12   6 879]]
TRAINING class wise acc@28 :: {0: [0.9036], 1: [0.9472], 2: [0.83], 3: [0.7436], 4: [0.9004], 5: [0.8996], 6: [0.9498], 7: [0.9594], 8: [0.9584], 9: [0.9678]}
TRAIINING epoch@28 ::  acc@0.920  loss@0.264
[[2259    6   73    3   46    1    0    2   64   46]
 [  20 2368    1    0    0    0    1    0   27   83]
 [  75    0 2075  146  110    7   67    6   13    1]
 [   3    1   82 1859   97  374   69   14    0    1]
 [  36    0   68   59 2251   17    1   63    2    3]
 [   1    0    5  356   32 4498   14   94    0    0]
 [   2    0  115  117    2    6 4749    0    7    2]
 [   4    0    1    5   95   83    0 4797    0   15]
 [ 110   40   45    1    2    0    3    1 4792    6]
 [  53   61    2    0   21    0    1   18    5 4839]]
TESTING class wise acc@28 :: {0: [0.641], 1: [0.852], 2: [0.597], 3: [0.549], 4: [0.537], 5: [0.809], 6: [0.747], 7: [0.904], 8: [0.935], 9: [0.848]}
TESTING epoch@28 ::  acc@0.742  loss@1.200
Epoch@@29
[[641  12 107  25  41   9   1   8 137  19]
 [ 15 852   1   4   0   1   0   3  59  65]
 [ 38   0 597 177  63  58  12  31  23   1]
 [  7   2  42 549  38 291  19  45   3   4]
 [  9   0  82 156 537  37  13 160   5   1]
 [  2   0  10 105  14 809   4  55   0   1]
 [  3   0  60 138   8  37 747   5   2   0]
 [  5   0   4  27  15  35   0 904   2   8]
 [ 18   5  21  11   1   0   1   4 935   4]
 [ 36  50   9   5  20   1   0   7  24 848]]
TRAINING class wise acc@29 :: {0: [0.92], 1: [0.9576], 2: [0.858], 3: [0.7688], 4: [0.91], 5: [0.918], 6: [0.957], 7: [0.97], 8: [0.9652], 9: [0.9718]}
TRAIINING epoch@29 ::  acc@0.932  loss@0.225
[[2300    8   50    2   35    1    1    3   60   40]
 [  17 2394    0    0    0    0    0    0   17   72]
 [  68    0 2145  128   91    5   44    3   15    1]
 [   1    0   78 1922  104  321   62   11    1    0]
 [  29    0   61   66 2275   16    1   49    0    3]
 [   0    0    4  303   31 4590   10   62    0    0]
 [   0    0  101   84    4   17 4785    1    8    0]
 [   2    0    0    9   77   55    0 4850    0    7]
 [ 102   19   37    0    1    0   11    1 4826    3]
 [  58   54    0    0   11    0    0   16    2 4859]]
TESTING class wise acc@29 :: {0: [0.764], 1: [0.822], 2: [0.588], 3: [0.612], 4: [0.695], 5: [0.843], 6: [0.832], 7: [0.842], 8: [0.85], 9: [0.902]}
TESTING epoch@29 ::  acc@0.775  loss@1.119
Epoch@@30
[[764   7  55  23  94  10   1   5  19  22]
 [ 26 822   6   0   9   2   0   2  15 118]
 [ 42   0 588 131 120  56  38  18   6   1]
 [  5   1  34 612  40 255  30  17   2   4]
 [  4   0  39 120 695  51  10  79   1   1]
 [  1   0  10  95  22 843   7  21   0   1]
 [  3   0  33  98  11  20 832   0   3   0]
 [  1   0   6  29  40  79   1 842   0   2]
 [ 85   9  17   8   8   6   5   1 850  11]
 [ 19  29   4  13  15   3   1   9   5 902]]
TRAINING class wise acc@30 :: {0: [0.9232], 1: [0.9652], 2: [0.8748], 3: [0.7992], 4: [0.9312], 5: [0.9206], 6: [0.9648], 7: [0.973], 8: [0.976], 9: [0.979]}
TRAIINING epoch@30 ::  acc@0.941  loss@0.197
[[2308    9   57    2   38    0    1    2   43   40]
 [  21 2413    0    0    0    0    0    0   10   56]
 [  68    0 2187  120   71    3   36    3   10    2]
 [   2    0   72 1998   78  287   53    7    1    2]
 [  20    0   39   44 2328    8    0   57    0    4]
 [   1    0    5  289   33 4603   10   59    0    0]
 [   2    0   70   81    4   12 4824    0    7    0]
 [   0    0    0    3   67   49    0 4865    0   16]
 [  70   16   27    0    0    0    3    0 4880    4]
 [  43   32    2    0   14    0    0   13    1 4895]]
TESTING class wise acc@30 :: {0: [0.819], 1: [0.889], 2: [0.609], 3: [0.585], 4: [0.721], 5: [0.662], 6: [0.897], 7: [0.738], 8: [0.954], 9: [0.761]}
TESTING epoch@30 ::  acc@0.763  loss@1.420
Epoch@@31
[[819   9  23   4  14   1   5   0 118   7]
 [ 26 889   1   1   0   0   2   0  52  29]
 [ 82   0 609 106  71  30  53   7  41   1]
 [ 37   3  84 585  81  90  78  14  24   4]
 [ 33   1  76 100 721  13  21  24  11   0]
 [ 11   1  20 213  40 662  26  15  12   0]
 [  2   0  30  48   7   2 897   1  13   0]
 [ 17   0  17  32 120  60   4 738   4   8]
 [ 21  13   3   3   2   0   2   1 954   1]
 [ 71  87   4   3  13   1   4   1  55 761]]
TRAINING class wise acc@31 :: {0: [0.9264], 1: [0.9564], 2: [0.882], 3: [0.7888], 4: [0.9268], 5: [0.9158], 6: [0.9618], 7: [0.972], 8: [0.9704], 9: [0.9736]}
TRAIINING epoch@31 ::  acc@0.938  loss@0.206
[[2316    6   63    3   35    0    0    1   43   33]
 [  21 2391    0    0    0    0    0    0   20   68]
 [  68    0 2205  103   60    8   43    0   11    2]
 [   5    0   88 1972   80  295   47   12    0    1]
 [  24    0   38   56 2317    8    1   52    0    4]
 [   0    0    4  296   24 4579   11   84    0    2]
 [   5    0   83   82    2    7 4809    0   10    2]
 [   0    0    0    6   63   59    0 4860    0   12]
 [  78   34   26    1    1    0    6    0 4852    2]
 [  45   56    1    0   18    0    1   11    0 4868]]
TESTING class wise acc@31 :: {0: [0.871], 1: [0.88], 2: [0.554], 3: [0.565], 4: [0.754], 5: [0.8], 6: [0.849], 7: [0.848], 8: [0.917], 9: [0.851]}
TESTING epoch@31 ::  acc@0.789  loss@1.124
Epoch@@32
[[871   3  19   6  24   2   1   3  55  16]
 [ 29 880   2   1   2   0   0   1  51  34]
 [124   0 554 103 105  49  35  17  12   1]
 [ 31   2  44 565  76 198  48  23   7   6]
 [ 34   0  35  67 754  36  19  52   3   0]
 [  8   1  12 112  36 800   5  25   0   1]
 [  6   0  49  55  15  18 849   1   6   1]
 [ 10   0   3  18  56  57   2 848   0   6]
 [ 55   7   4   5   4   2   2   0 917   4]
 [ 52  53   6   5   9   3   1   9  11 851]]
TRAINING class wise acc@32 :: {0: [0.9344], 1: [0.9604], 2: [0.8748], 3: [0.8204], 4: [0.9424], 5: [0.9344], 6: [0.9648], 7: [0.975], 8: [0.971], 9: [0.9816]}
TRAIINING epoch@32 ::  acc@0.946  loss@0.185
[[2336    5   51    2   31    0    0    0   49   26]
 [  14 2401    0    0    0    0    0    0   26   59]
 [  60    0 2187  111   71    2   48    4   15    2]
 [   3    0   75 2051   68  242   51   10    0    0]
 [  15    0   42   28 2356   11    1   45    0    2]
 [   0    0    5  235   16 4672    9   63    0    0]
 [   0    0   64   86    4   13 4824    1    8    0]
 [   1    0    2    6   48   56    0 4875    0   12]
 [  82   30   24    0    2    0    7    0 4855    0]
 [  36   34    0    0    9    0    0   11    2 4908]]
TESTING class wise acc@32 :: {0: [0.828], 1: [0.81], 2: [0.559], 3: [0.427], 4: [0.706], 5: [0.838], 6: [0.912], 7: [0.814], 8: [0.904], 9: [0.839]}
TESTING epoch@32 ::  acc@0.764  loss@1.385
Epoch@@33
[[828   3  53   9  23   3   6   3  59  13]
 [ 43 810   7   4   3   3   3   4  33  90]
 [ 85   0 559  81  86  81  78  13  17   0]
 [ 11   1  59 427  59 306 114  16   3   4]
 [ 25   0  73  71 706  46  28  45   5   1]
 [  4   0  13  82  17 838  27  19   0   0]
 [  3   0  32  28   5  15 912   0   5   0]
 [  8   0  13  28  55  76   3 814   1   2]
 [ 50   4  15   5   5   2   7   2 904   6]
 [ 59  38   8   6  19   3   5  11  12 839]]
TRAINING class wise acc@33 :: {0: [0.9316], 1: [0.968], 2: [0.89], 3: [0.8104], 4: [0.92], 5: [0.9256], 6: [0.962], 7: [0.9708], 8: [0.9746], 9: [0.9798]}
TRAIINING epoch@33 ::  acc@0.943  loss@0.191
[[2329    7   57    2   32    0    0    1   42   30]
 [  17 2420    1    0    0    0    0    0   14   48]
 [  51    0 2225   84   78    6   40    3    9    4]
 [   3    0   63 2026   73  260   63   12    0    0]
 [  26    0   48   49 2300   11    2   61    0    3]
 [   0    0    5  282   15 4628    7   63    0    0]
 [   1    0   82   84    7   12 4810    0    4    0]
 [   1    0    1    7   67   58    0 4854    0   12]
 [  77   23   20    0    1    0    3    0 4873    3]
 [  41   36    0    0   12    0    0   12    0 4899]]
TESTING class wise acc@33 :: {0: [0.721], 1: [0.781], 2: [0.648], 3: [0.553], 4: [0.72], 5: [0.739], 6: [0.913], 7: [0.849], 8: [0.858], 9: [0.924]}
TESTING epoch@33 ::  acc@0.771  loss@1.240
Epoch@@34
[[721   2 127  19  48   3   9   3  21  47]
 [ 30 781   4   1   5   1   1   2  16 159]
 [ 40   0 648  86  89  29  87  16   4   1]
 [  7   2  67 553  53 176 107  27   2   6]
 [  7   0  64  94 720  28  31  51   3   2]
 [  3   1  19 158  20 739  26  33   0   1]
 [  4   1  22  42   5   7 913   1   3   2]
 [  3   0  11  28  48  58   1 849   0   2]
 [ 77   7  26   4   4   0   9   1 858  14]
 [ 17  14   6   3  14   1   4  12   5 924]]
TRAINING class wise acc@34 :: {0: [0.9348], 1: [0.9676], 2: [0.8976], 3: [0.8268], 4: [0.9396], 5: [0.9374], 6: [0.9652], 7: [0.9772], 8: [0.979], 9: [0.9794]}
TRAIINING epoch@34 ::  acc@0.950  loss@0.173
[[2337   12   55    2   29    0    1    0   38   26]
 [  17 2419    0    0    0    0    0    0   14   50]
 [  58    0 2244   91   47    3   45    1    9    2]
 [   2    0   82 2067   71  232   42    3    1    0]
 [  15    0   27   46 2349   11    0   46    0    6]
 [   0    0    3  231   24 4687    8   47    0    0]
 [   2    0   76   78    3    7 4826    1    7    0]
 [   0    0    0    7   59   37    0 4886    0   11]
 [  61   13   23    1    0    0    6    0 4895    1]
 [  29   44    0    2   13    0    0   14    1 4897]]
TESTING class wise acc@34 :: {0: [0.807], 1: [0.908], 2: [0.609], 3: [0.612], 4: [0.739], 5: [0.791], 6: [0.898], 7: [0.831], 8: [0.858], 9: [0.914]}
TESTING epoch@34 ::  acc@0.797  loss@1.221
Epoch@@35
[[807  12  69  19  13   2   8   1  32  37]
 [ 14 908   1   1   2   1   0   0  12  61]
 [ 73   0 609 104  75  43  74  14   5   3]
 [ 10   2  52 612  56 152  85  15   2  14]
 [ 24   1  62 105 739  19  17  27   1   5]
 [  6   1  12 122  22 791  19  25   0   2]
 [  5   1  36  37  11   7 898   2   2   1]
 [  6   0   7  26  56  56   2 831   1  15]
 [ 66  23  20   4   4   2   5   0 858  18]
 [ 24  42   5   5   3   0   3   0   4 914]]
TRAINING class wise acc@35 :: {0: [0.9364], 1: [0.9644], 2: [0.9016], 3: [0.8524], 4: [0.9312], 5: [0.9446], 6: [0.969], 7: [0.974], 8: [0.978], 9: [0.9802]}
TRAIINING epoch@35 ::  acc@0.952  loss@0.168
[[2341    7   43    1   36    0    0    0   42   30]
 [  18 2411    0    0    0    0    0    0   19   52]
 [  43    0 2254   92   60    3   37    1    8    2]
 [   3    0   63 2131   55  190   47    9    0    2]
 [  27    0   37   41 2328    7    0   56    0    4]
 [   0    0    2  210   13 4723    5   47    0    0]
 [   4    1   61   66    3    9 4845    0    9    2]
 [   0    0    1    9   62   46    0 4870    0   12]
 [  54   25   22    0    0    0    8    0 4890    1]
 [  34   33    1    0   15    0    1   14    1 4901]]
TESTING class wise acc@35 :: {0: [0.748], 1: [0.882], 2: [0.636], 3: [0.608], 4: [0.695], 5: [0.81], 6: [0.88], 7: [0.858], 8: [0.874], 9: [0.882]}
TESTING epoch@35 ::  acc@0.787  loss@1.260
Epoch@@36
[[748  10 103  23  28   8   3   4  32  41]
 [ 15 882   3   2   3   5   0   0  14  76]
 [ 45   0 636 129  69  43  53  19   4   2]
 [  5   2  46 608  45 206  50  28   3   7]
 [ 10   0  76 109 695  35  27  45   1   2]
 [  3   0   8 122  18 810  11  26   1   1]
 [  2   0  35  55   8  15 880   2   2   1]
 [  3   0   6  26  35  63   1 858   0   8]
 [ 48  20  29  10   2   0   4   2 874  11]
 [ 17  48   9  11   9   1   4  12   7 882]]
TRAINING class wise acc@36 :: {0: [0.9368], 1: [0.964], 2: [0.9032], 3: [0.8396], 4: [0.9288], 5: [0.935], 6: [0.9646], 7: [0.9698], 8: [0.9764], 9: [0.9732]}
TRAIINING epoch@36 ::  acc@0.947  loss@0.182
[[2342    5   47    3   28    1    1    0   37   36]
 [  20 2410    0    0    1    0    0    1   12   56]
 [  49    0 2258   83   56    4   35    4   10    1]
 [   1    0   53 2099   76  212   47   12    0    0]
 [  17    0   44   55 2322    6    2   49    0    5]
 [   0    0    2  219   24 4675   13   67    0    0]
 [   0    1   75   78    2   15 4823    0    6    0]
 [   1    0    1    7   56   70    0 4849    0   16]
 [  68   22   16    0    1    0    7    0 4882    4]
 [  51   47    1    0   21    0    0   14    0 4866]]
TESTING class wise acc@36 :: {0: [0.807], 1: [0.883], 2: [0.678], 3: [0.631], 4: [0.638], 5: [0.715], 6: [0.903], 7: [0.811], 8: [0.925], 9: [0.877]}
TESTING epoch@36 ::  acc@0.787  loss@1.292
Epoch@@37
[[807   4  68   8  22   2   7   0  64  18]
 [ 29 883   2   2   2   0   0   0  22  60]
 [ 76   0 678  89  47  25  54  11  16   4]
 [ 14   3  91 631  31 134  68  14   8   6]
 [ 19   1 118 129 638  22  29  40   4   0]
 [  4   1  30 192  19 715  19  18   2   0]
 [  2   1  41  36   5   6 903   1   5   0]
 [  7   0   8  42  56  63   3 811   2   8]
 [ 39  13  10   2   1   0   3   0 925   7]
 [ 38  50   5   4   5   0   4   4  13 877]]
TRAINING class wise acc@37 :: {0: [0.9424], 1: [0.9732], 2: [0.9144], 3: [0.8688], 4: [0.9448], 5: [0.9452], 6: [0.9688], 7: [0.9788], 8: [0.978], 9: [0.9808]}
TRAIINING epoch@37 ::  acc@0.956  loss@0.159
[[2356    5   41    3   26    1    0    0   48   20]
 [  10 2433    0    0    0    0    0    0   15   42]
 [  48    0 2286   71   44    1   33    4   11    2]
 [   4    0   50 2172   44  178   47    3    0    2]
 [  20    0   21   36 2362    9    1   48    0    3]
 [   0    0    1  199   17 4726   15   41    1    0]
 [   3    0   65   71    2    9 4844    0    5    1]
 [   0    0    0    4   54   41    0 4894    0    7]
 [  62   20   23    0    0    0    2    0 4890    3]
 [  35   36    0    0   11    2    1   10    1 4904]]
TESTING class wise acc@37 :: {0: [0.784], 1: [0.848], 2: [0.762], 3: [0.581], 4: [0.717], 5: [0.716], 6: [0.862], 7: [0.867], 8: [0.852], 9: [0.921]}
TESTING epoch@37 ::  acc@0.791  loss@1.165
Epoch@@38
[[784   7  84  13  31   2   1   5  27  46]
 [ 17 848   2   2   0   2   0   0   9 120]
 [ 45   0 762  56  56  21  33  19   6   2]
 [  7   3 119 581  66 134  57  23   4   6]
 [ 13   0 111  86 717  15  19  36   3   0]
 [  3   0  30 185  32 716  12  21   0   1]
 [  3   2  77  41   6   2 862   2   5   0]
 [  3   0  13  21  46  47   1 867   0   2]
 [ 72  18  22   9   3   0   1   1 852  22]
 [ 11  28   9   6   9   0   1  10   5 921]]
TRAINING class wise acc@38 :: {0: [0.9596], 1: [0.9796], 2: [0.93], 3: [0.8912], 4: [0.9564], 5: [0.9572], 6: [0.9764], 7: [0.9806], 8: [0.9848], 9: [0.985]}
TRAIINING epoch@38 ::  acc@0.966  loss@0.117
[[2399    5   33    1   19    0    0    1   27   15]
 [   5 2449    0    0    0    0    0    0   12   34]
 [  46    0 2325   64   29    2   24    2    7    1]
 [   0    0   49 2228   40  136   41    6    0    0]
 [  11    0   15   26 2391    4    0   50    0    3]
 [   0    0    1  159    6 4786    8   40    0    0]
 [   0    0   47   58    0   11 4882    1    1    0]
 [   2    0    0    3   45   40    0 4903    0    7]
 [  41   14   18    0    0    0    1    1 4924    1]
 [  26   30    1    1    4    0    0   12    1 4925]]
TESTING class wise acc@38 :: {0: [0.795], 1: [0.828], 2: [0.617], 3: [0.596], 4: [0.625], 5: [0.75], 6: [0.853], 7: [0.838], 8: [0.92], 9: [0.927]}
TESTING epoch@38 ::  acc@0.775  loss@1.335
Epoch@@39
[[795   5  57  18  26   5   2   3  58  31]
 [ 16 828   3   1   3   3   1   2  12 131]
 [ 67   0 617 137  58  50  44  16   7   4]
 [  8   2  46 596  50 191  68  20   6  13]
 [ 19   0 113 142 625  33  24  37   4   3]
 [  7   0  12 158  26 750  14  28   0   5]
 [  2   0  44  69   5  17 853   2   7   1]
 [  7   0   2  38  45  56   1 838   0  13]
 [ 35   8   7   8   4   2   1   0 920  15]
 [ 23  25   2   3   8   3   1   1   7 927]]
TRAINING class wise acc@39 :: {0: [0.9504], 1: [0.978], 2: [0.922], 3: [0.8716], 4: [0.9492], 5: [0.9522], 6: [0.9698], 7: [0.9782], 8: [0.9818], 9: [0.9796]}
TRAIINING epoch@39 ::  acc@0.960  loss@0.145
[[2376    4   39    3   26    0    0    1   31   20]
 [  10 2445    0    0    0    0    0    0    9   36]
 [  42    0 2305   66   43    4   26    1   10    3]
 [   0    0   46 2179   53  172   44    4    1    1]
 [  15    0   22   46 2373    3    1   38    0    2]
 [   0    0    1  171   11 4761   10   46    0    0]
 [   3    0   62   67    6    7 4849    0    5    1]
 [   1    0    0    4   46   51    0 4891    0    7]
 [  50   12   24    0    0    0    4    0 4909    1]
 [  40   34    2    1   17    0    0    8    0 4898]]
TESTING class wise acc@39 :: {0: [0.822], 1: [0.87], 2: [0.564], 3: [0.614], 4: [0.745], 5: [0.708], 6: [0.911], 7: [0.885], 8: [0.902], 9: [0.892]}
TESTING epoch@39 ::  acc@0.791  loss@1.283
Epoch@@40
[[822   9  36  13  28   2   4   3  51  32]
 [ 17 870   0   5   1   1   1   0  28  77]
 [ 71   1 564 114 106  36  75  24   6   3]
 [ 12   3  36 614  74 138  82  28   3  10]
 [ 15   1  23 104 745  23  26  59   3   1]
 [  5   0  17 190  29 708  15  35   1   0]
 [  2   1  19  50   7   4 911   2   3   1]
 [  3   0   3  24  32  50   1 885   0   2]
 [ 45  16   5   3   6   0   4   3 902  16]
 [ 20  50   4   6   9   1   6   5   7 892]]
TRAINING class wise acc@40 :: {0: [0.9504], 1: [0.9788], 2: [0.9292], 3: [0.8848], 4: [0.96], 5: [0.9534], 6: [0.9728], 7: [0.9858], 8: [0.9824], 9: [0.9862]}
TRAIINING epoch@40 ::  acc@0.964  loss@0.127
[[2376    5   38    1   22    0    0    1   33   24]
 [  13 2447    0    0    0    0    0    0   10   30]
 [  40    0 2323   61   35    0   31    1    8    1]
 [   2    0   50 2212   33  161   36    6    0    0]
 [  14    0   17   33 2400    3    0   31    0    2]
 [   0    0    1  171    9 4767   11   41    0    0]
 [   4    1   55   55    2   13 4864    0    5    1]
 [   0    0    0    5   37   26    0 4929    0    3]
 [  49   14   17    2    1    0    4    0 4912    1]
 [  27   26    0    1   12    0    0    2    1 4931]]
TESTING class wise acc@40 :: {0: [0.618], 1: [0.77], 2: [0.448], 3: [0.381], 4: [0.539], 5: [0.817], 6: [0.909], 7: [0.936], 8: [0.934], 9: [0.911]}
TESTING epoch@40 ::  acc@0.726  loss@2.021
Epoch@@41
[[618   5  75  17  37   7   4  19 180  38]
 [ 15 770   5   2   4   4   4   9  51 136]
 [ 48   0 448  98  76 104 142  65  14   5]
 [  7   2  26 381  31 338 115  87   7   6]
 [  9   0  45  91 539  78  51 182   3   2]
 [  3   0   3  59  14 817  36  64   3   1]
 [  2   0   9  31   7  29 909  10   2   1]
 [  5   0   1  10  10  34   1 936   0   3]
 [ 16   2  12   3   1   7   5   4 934  16]
 [ 17  27   1   2   5   2   5  15  15 911]]
TRAINING class wise acc@41 :: {0: [0.9568], 1: [0.9708], 2: [0.9276], 3: [0.8908], 4: [0.9528], 5: [0.9606], 6: [0.975], 7: [0.9798], 8: [0.983], 9: [0.982]}
TRAIINING epoch@41 ::  acc@0.964  loss@0.127
[[2392    4   38    1   16    1    0    0   29   19]
 [  13 2427    0    0    0    0    0    0   18   42]
 [  34    0 2319   69   37    2   26    2   10    1]
 [   2    0   52 2227   50  126   37    4    0    2]
 [   9    0   28   39 2382    6    2   29    0    5]
 [   0    0    1  137    4 4803    9   46    0    0]
 [   0    0   56   57    0    7 4875    0    5    0]
 [   0    0    1    2   41   45    0 4899    0   12]
 [  45   15   21    0    0    0    4    0 4915    0]
 [  27   33    1    1   15    0    0   12    1 4910]]
TESTING class wise acc@41 :: {0: [0.74], 1: [0.89], 2: [0.491], 3: [0.516], 4: [0.643], 5: [0.86], 6: [0.888], 7: [0.882], 8: [0.889], 9: [0.912]}
TESTING epoch@41 ::  acc@0.771  loss@1.554
Epoch@@42
[[740  24  46  18  40   7   5   7  55  58]
 [ 14 890   3   2   4   2   2   3   7  73]
 [ 79   1 491 147  82 101  60  26   7   6]
 [ 10   2  21 516  35 321  51  26   4  14]
 [ 21   0  39 114 643  64  25  85   5   4]
 [  2   0   5  69  17 860  10  34   2   1]
 [  1   1  16  55  10  23 888   3   1   2]
 [  5   0   1  16  18  70   1 882   0   7]
 [ 33  39   7   4   4   1   5   1 889  17]
 [ 10  42   1   3   8   2   1  13   8 912]]
TRAINING class wise acc@42 :: {0: [0.9584], 1: [0.98], 2: [0.9316], 3: [0.8924], 4: [0.9572], 5: [0.9652], 6: [0.9798], 7: [0.9816], 8: [0.9836], 9: [0.9848]}
TRAIINING epoch@42 ::  acc@0.967  loss@0.114
[[2396    6   28    0   17    0    0    2   33   18]
 [   8 2450    0    0    1    0    0    0   14   27]
 [  35    0 2329   59   38    2   26    2    8    1]
 [   2    0   57 2231   33  133   40    4    0    0]
 [  12    0   23   31 2393    2    1   32    0    6]
 [   0    0    2  120    6 4826    6   40    0    0]
 [   0    0   40   48    0   10 4899    0    3    0]
 [   1    0    0    4   38   36    0 4908    0   13]
 [  47   18   14    0    0    0    2    0 4918    1]
 [  25   28    0    0    8    0    0   15    0 4924]]
TESTING class wise acc@42 :: {0: [0.744], 1: [0.851], 2: [0.719], 3: [0.592], 4: [0.738], 5: [0.782], 6: [0.796], 7: [0.863], 8: [0.92], 9: [0.929]}
TESTING epoch@42 ::  acc@0.793  loss@1.297
Epoch@@43
[[744   7  71  16  34   4   0   3  75  46]
 [ 24 851   5   3   2   0   0   0  21  94]
 [ 52   0 719  86  61  33  16  18   9   6]
 [  6   3  79 592  56 191  27  26  10  10]
 [ 10   1  74  79 738  23   6  62   5   2]
 [  2   0  24 128  21 782   8  29   5   1]
 [  4   0  81  72   9  18 796   2  15   3]
 [  9   0   4  24  39  52   0 863   2   7]
 [ 37   9  10   7   4   1   0   1 920  11]
 [ 19  26   5   1   3   1   0   4  12 929]]
TRAINING class wise acc@43 :: {0: [0.968], 1: [0.9816], 2: [0.9508], 3: [0.9076], 4: [0.9584], 5: [0.9694], 6: [0.9832], 7: [0.9864], 8: [0.9888], 9: [0.9882]}
TRAIINING epoch@43 ::  acc@0.973  loss@0.091
[[2420    3   23    0   17    0    0    0   20   17]
 [   5 2454    0    0    0    0    0    0    9   32]
 [  30    1 2377   38   16    2   24    2    8    2]
 [   0    0   30 2269   39  132   26    4    0    0]
 [  11    0   22   31 2396    3    2   32    1    2]
 [   0    0    1  111    4 4847    3   34    0    0]
 [   0    0   41   35    1    7 4916    0    0    0]
 [   0    0    0    2   32   25    0 4932    0    9]
 [  26   15   11    0    1    0    2    0 4944    1]
 [  16   24    0    0    8    0    0    9    2 4941]]
TESTING class wise acc@43 :: {0: [0.736], 1: [0.736], 2: [0.702], 3: [0.556], 4: [0.794], 5: [0.695], 6: [0.898], 7: [0.885], 8: [0.846], 9: [0.95]}
TESTING epoch@43 ::  acc@0.780  loss@1.478
Epoch@@44
[[736   4 101  12  39   3   8   4  40  53]
 [ 19 736   4   1   3   1   2   2   8 224]
 [ 35   0 702  56  96  26  44  31   4   6]
 [ 12   2  68 556  81 130 104  29   3  15]
 [  8   0  65  56 794   6  20  48   0   3]
 [  3   0  24 151  43 695  35  48   0   1]
 [  2   0  45  24  16   8 898   4   2   1]
 [  7   0   6  20  43  28   1 885   0  10]
 [ 61  15  25   5   5   1   6   5 846  31]
 [ 12  11   4   2   6   1   3   5   6 950]]
TRAINING class wise acc@44 :: {0: [0.9576], 1: [0.9784], 2: [0.9292], 3: [0.9004], 4: [0.9512], 5: [0.9628], 6: [0.9786], 7: [0.9806], 8: [0.9834], 9: [0.9862]}
TRAIINING epoch@44 ::  acc@0.967  loss@0.117
[[2394    3   35    0   23    0    0    1   30   14]
 [   8 2446    0    0    0    0    0    0   10   36]
 [  37    0 2323   56   31    6   31    3   11    2]
 [   1    0   49 2251   40  124   27    7    1    0]
 [  15    0   23   38 2378    1    1   39    0    5]
 [   0    0    3  137    6 4814    5   35    0    0]
 [   0    0   47   48    1    9 4893    0    2    0]
 [   0    0    1    4   55   30    0 4903    0    7]
 [  45   11   19    2    1    0    2    0 4917    3]
 [  19   26    0    0   11    0    0   13    0 4931]]
TESTING class wise acc@44 :: {0: [0.725], 1: [0.857], 2: [0.554], 3: [0.702], 4: [0.69], 5: [0.72], 6: [0.876], 7: [0.863], 8: [0.887], 9: [0.853]}
TESTING epoch@44 ::  acc@0.773  loss@1.527
Epoch@@45
[[725   9  55  41  45   8   4   5  91  17]
 [ 16 857   4   5   2   3   2   2  16  93]
 [ 50   0 554 158  91  46  57  29  13   2]
 [  9   1  27 702  42 147  46  18   3   5]
 [ 15   0  52 145 690  16  23  56   1   2]
 [  3   0   8 201  23 720  14  30   1   0]
 [  2   0  18  83   5  10 876   3   3   0]
 [  6   0   2  40  37  47   2 863   0   3]
 [ 35  17  28  10   3   2   5   6 887   7]
 [ 37  41   5  19  13   1   4  15  12 853]]
TRAINING class wise acc@45 :: {0: [0.9472], 1: [0.9748], 2: [0.9328], 3: [0.89], 4: [0.9548], 5: [0.9554], 6: [0.975], 7: [0.9818], 8: [0.981], 9: [0.9838]}
TRAIINING epoch@45 ::  acc@0.964  loss@0.129
[[2368    5   34    0   23    0    0    1   43   26]
 [   9 2437    0    0    0    0    0    0   12   42]
 [  37    0 2332   62   30    4   22    1   11    1]
 [   0    0   52 2225   37  144   35    6    0    1]
 [  11    0   23   36 2387    5    1   36    0    1]
 [   0    0    2  156   10 4777   13   42    0    0]
 [   0    0   47   63    2   10 4875    0    2    1]
 [   0    0    1    5   43   32    0 4909    0   10]
 [  52   18   19    2    0    0    3    0 4905    1]
 [  35   28    0    0    4    0    0   12    2 4919]]
TESTING class wise acc@45 :: {0: [0.737], 1: [0.636], 2: [0.542], 3: [0.509], 4: [0.756], 5: [0.783], 6: [0.868], 7: [0.881], 8: [0.826], 9: [0.949]}
TESTING epoch@45 ::  acc@0.749  loss@1.554
Epoch@@46
[[737   3  40  13  58   5   4  13  35  92]
 [ 24 636   4   3   2   2   3   0   7 319]
 [ 75   0 542 111 126  47  57  30   4   8]
 [ 11   2  33 509 104 235  48  37   4  17]
 [ 18   0  35  75 756  28  13  71   1   3]
 [  4   0  14  93  37 783  14  50   1   4]
 [  3   0  22  67   8  21 868   4   6   1]
 [  4   0   3  12  40  47   2 881   1  10]
 [107   6  12   3   6   1   6   2 826  31]
 [ 19   8   3   1   9   1   1   5   4 949]]
TRAINING class wise acc@46 :: {0: [0.958], 1: [0.9728], 2: [0.9432], 3: [0.9108], 4: [0.9608], 5: [0.9674], 6: [0.9818], 7: [0.985], 8: [0.9838], 9: [0.9856]}
TRAIINING epoch@46 ::  acc@0.970  loss@0.111
[[2395    2   26    0   25    0    1    2   31   18]
 [  13 2432    0    0    0    0    1    0   15   39]
 [  28    0 2358   51   25    1   30    0    7    0]
 [   1    0   38 2277   25  125   27    6    1    0]
 [  14    0   15   26 2402    8    1   31    0    3]
 [   0    0    2  113   14 4837    6   28    0    0]
 [   1    0   40   38    1    4 4909    0    7    0]
 [   0    0    0    6   30   26    0 4925    0   13]
 [  43   15   15    0    0    0    5    0 4919    3]
 [  21   30    0    0   11    0    0   10    0 4928]]
TESTING class wise acc@46 :: {0: [0.863], 1: [0.854], 2: [0.721], 3: [0.521], 4: [0.72], 5: [0.705], 6: [0.861], 7: [0.8], 8: [0.885], 9: [0.892]}
TESTING epoch@46 ::  acc@0.782  loss@1.499
Epoch@@47
[[863   7  47   3  13   0   1   1  44  21]
 [ 30 854   3   1   0   0   2   1  11  98]
 [ 97   0 721  50  67  22  26   9   5   3]
 [ 43   1 123 521  86 136  52  18   7  13]
 [ 41   0 100  60 720  14  17  41   3   4]
 [ 19   0  41 136  45 705  12  35   2   5]
 [  7   1  76  29  12   3 861   4   6   1]
 [ 17   0  20  19  83  33   0 800   2  26]
 [ 71  16  10   3   2   1   2   1 885   9]
 [ 35  46   8   2   3   0   1   1  12 892]]
TRAINING class wise acc@47 :: {0: [0.9496], 1: [0.9752], 2: [0.9364], 3: [0.8948], 4: [0.9588], 5: [0.967], 6: [0.9752], 7: [0.984], 8: [0.9816], 9: [0.9828]}
TRAIINING epoch@47 ::  acc@0.966  loss@0.125
[[2374    5   39    1   23    0    0    0   31   27]
 [  11 2438    1    0    0    0    0    0   12   38]
 [  34    0 2341   53   29    3   27    2    9    2]
 [   4    0   41 2237   52  119   39    6    1    1]
 [  17    0   16   39 2397    3    1   22    0    5]
 [   0    0    1  109   10 4835    6   38    0    1]
 [   2    0   47   58    3   10 4876    0    4    0]
 [   1    0    1    4   33   35    0 4920    0    6]
 [  47   22   20    1    0    0    1    0 4908    1]
 [  39   25    2    0   12    0    0    7    1 4914]]
TESTING class wise acc@47 :: {0: [0.817], 1: [0.846], 2: [0.657], 3: [0.649], 4: [0.671], 5: [0.768], 6: [0.879], 7: [0.87], 8: [0.898], 9: [0.92]}
TESTING epoch@47 ::  acc@0.797  loss@1.330
Epoch@@48
[[817   3  56  11  20   2   3   3  58  27]
 [ 27 846   4   3   2   1   2   1  20  94]
 [ 90   0 657  97  57  36  35  18   7   3]
 [ 16   1  63 649  31 152  56  15   5  12]
 [ 19   1  86 121 671  21  18  57   3   3]
 [  5   0  23 148  20 768  11  24   1   0]
 [  4   0  48  47   4  14 879   2   2   0]
 [  5   0  11  28  33  42   2 870   0   9]
 [ 51  12  11   6   2   1   4   2 898  13]
 [ 12  29   7   3   5   0   1  13  10 920]]
TRAINING class wise acc@48 :: {0: [0.9648], 1: [0.9788], 2: [0.9484], 3: [0.9156], 4: [0.9656], 5: [0.9676], 6: [0.981], 7: [0.9852], 8: [0.9842], 9: [0.9848]}
TRAIINING epoch@48 ::  acc@0.972  loss@0.104
[[2412    4   26    1   10    1    0    2   23   21]
 [   4 2447    0    0    0    0    0    0   16   33]
 [  32    0 2371   48   25    2   16    2    4    0]
 [   1    0   43 2289   35  101   27    4    0    0]
 [   9    0   12   32 2414    2    2   29    0    0]
 [   1    0    3  111    8 4838    7   32    0    0]
 [   0    0   33   45    2   13 4905    0    2    0]
 [   0    1    0    2   33   29    0 4926    0    9]
 [  42   19   14    0    0    0    3    0 4921    1]
 [  31   26    0    1   10    0    0    8    0 4924]]
TESTING class wise acc@48 :: {0: [0.849], 1: [0.777], 2: [0.663], 3: [0.55], 4: [0.708], 5: [0.733], 6: [0.902], 7: [0.863], 8: [0.925], 9: [0.899]}
TESTING epoch@48 ::  acc@0.787  loss@1.440
Epoch@@49
[[849   3  40   3  16   0   4   0  62  23]
 [ 44 777   6   2   1   1   1   0  26 142]
 [ 86   0 663  83  73  28  43  13   8   3]
 [ 27   2 102 550  76 118  83  26   7   9]
 [ 24   1  84  82 708  14  25  57   3   2]
 [  8   0  35 132  33 733  23  32   3   1]
 [  6   0  44  25   8   4 902   1   9   1]
 [ 15   0   9  24  47  29   3 863   3   7]
 [ 43   6   7   1   4   0   4   1 925   9]
 [ 39  27   8   1   3   1   1   4  17 899]]
TRAINING class wise acc@49 :: {0: [0.9704], 1: [0.9892], 2: [0.96], 3: [0.926], 4: [0.972], 5: [0.9732], 6: [0.985], 7: [0.9894], 8: [0.99], 9: [0.9914]}
TRAIINING epoch@49 ::  acc@0.978  loss@0.076
[[2426    2   17    0   21    0    0    0   25    9]
 [   3 2473    0    0    0    0    0    0    3   21]
 [  26    0 2400   35   20    0   15    0    4    0]
 [   0    0   33 2315   27  100   20    5    0    0]
 [   8    0   12   19 2430    2    0   29    0    0]
 [   0    0    0   93    5 4866   11   25    0    0]
 [   0    0   20   38    2   13 4925    0    2    0]
 [   0    0    0    3   22   23    0 4947    0    5]
 [  29    9   12    0    0    0    0    0 4950    0]
 [  16   15    0    0    7    0    0    5    0 4957]]
TESTING class wise acc@49 :: {0: [0.835], 1: [0.848], 2: [0.663], 3: [0.561], 4: [0.704], 5: [0.77], 6: [0.805], 7: [0.834], 8: [0.943], 9: [0.868]}
TESTING epoch@49 ::  acc@0.783  loss@1.530
[[835   5  35  11  10   2   1   1  87  13]
 [ 31 848   2   2   0   1   0   0  46  70]
 [107   0 663  87  64  26  21  15  14   3]
 [ 24   2  85 561  68 168  37  29  16  10]
 [ 44   1  70  92 704  15   8  60   6   0]
 [  9   0  25 133  30 770   4  21   8   0]
 [  6   0  74  60  16  22 805   2  15   0]
 [ 13   0  14  20  52  58   0 834   4   5]
 [ 35   4   4   4   2   1   1   0 943   6]
 [ 55  37   5   3   4   0   1   9  18 868]]
